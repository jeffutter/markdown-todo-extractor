{"id":"markdown-todo-extractor-29z","title":"Remove unused CapabilityRegistry accessor methods","description":"## Problem\n\nIn `src/capabilities/mod.rs`, the `CapabilityRegistry` has accessor methods marked with `#[allow(dead_code)]`:\n\n```rust\n#[allow(dead_code)]\npub fn base_path(\u0026self) -\u003e \u0026PathBuf {\n    \u0026self.base_path\n}\n\n/// Get the config\n#[allow(dead_code)]\npub fn config(\u0026self) -\u003e \u0026Arc\u003cConfig\u003e {\n    \u0026self.config\n}\n```\n\nThese methods:\n1. Are never called in the current codebase\n2. Required `#[allow(dead_code)]` to suppress compiler warnings\n3. Represent speculative design (added for potential future use)\n\n## Proposed Solution\n\nRemove these methods if they're not part of a planned public API:\n\n```rust\n// DELETE these methods from CapabilityRegistry impl\n```\n\nIf there's a planned use case, document it with a TODO comment instead.\n\n## Locations\n\n- `src/capabilities/mod.rs` lines ~91-100\n\n## Estimated Impact\n\n- ~10 lines of unused code removed\n- Eliminates `#[allow(dead_code)]` attributes (code smell)\n- Cleaner public API surface","status":"closed","priority":4,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:48:19.246400463-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T11:03:52.931182869-06:00","closed_at":"2026-01-21T11:03:52.931186485-06:00","labels":["simplification-opportunity"]}
{"id":"markdown-todo-extractor-2ko","title":"Consolidate CLI path handling across operations","description":"## Problem\n\n1. **Naming inconsistency**: CLI path fields have different names across request types:\n   - `SearchTasksRequest::path` (tasks.rs line 33)\n   - `ListTagsRequest::cli_path` (tags.rs line 59)\n   - `SearchByTagsRequest::cli_path` (tags.rs line 106)\n   - `ListFilesRequest::cli_path` (files.rs line 27)\n\n2. **Duplicated path handling logic**: Each operation's `execute_from_args` has nearly identical path resolution code (~20 lines each):\n\n```rust\nlet response = if let Some(ref path) = request.cli_path {\n    let config = Arc::new(Config::load_from_base_path(path.as_path()));\n    let capability = TagCapability::new(path.clone(), config);\n    let mut req_without_path = request;\n    req_without_path.cli_path = None;\n    capability.list_tags(req_without_path).await?\n} else {\n    self.capability.list_tags(request).await?\n};\n```\n\nThis pattern is repeated in all 6 operations with only the capability type changing.\n\n## Proposed Solution\n\n1. **Standardize naming**: Use `cli_path: Option\u003cPathBuf\u003e` consistently across all request types\n\n2. **Extract helper function**:\n```rust\n/// Resolves CLI path to create a temporary capability if provided,\n/// otherwise uses the registry's default capability.\npub fn with_cli_path\u003cC, F, R\u003e(\n    cli_path: Option\u003cPathBuf\u003e,\n    default_capability: Arc\u003cC\u003e,\n    capability_factory: impl FnOnce(PathBuf, Arc\u003cConfig\u003e) -\u003e C,\n    operation: F,\n) -\u003e CapabilityResult\u003cR\u003e\nwhere\n    F: FnOnce(\u0026C) -\u003e CapabilityResult\u003cR\u003e,\n{\n    match cli_path {\n        Some(path) =\u003e {\n            let config = Arc::new(Config::load_from_base_path(\u0026path));\n            let temp_capability = capability_factory(path, config);\n            operation(\u0026temp_capability)\n        }\n        None =\u003e operation(\u0026*default_capability),\n    }\n}\n```\n\n## Locations to Update\n\n- `src/capabilities/tasks.rs` - SearchTasksRequest, SearchTasksOperation\n- `src/capabilities/tags.rs` - ExtractTagsRequest, ListTagsRequest, SearchByTagsRequest, and their operations\n- `src/capabilities/files.rs` - ListFilesRequest, ReadFileRequest, and their operations\n\n## Estimated Impact\n\n- ~60 lines of duplicated path handling eliminated\n- Consistent naming across all request types\n- Single point of change for CLI path resolution logic","status":"closed","priority":2,"issue_type":"task","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:47:14.580831534-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T05:52:05.546891839-06:00","closed_at":"2026-01-21T05:52:05.546891839-06:00","close_reason":"Closed","labels":["simplification-opportunity"]}
{"id":"markdown-todo-extractor-2m1","title":"Add test coverage for filter.rs","description":"## Problem\n\nThe filtering module (`src/filter.rs` - 104 lines) has **no tests**. This module handles:\n\n- Status filtering (complete, incomplete)\n- Date range filtering (due dates)\n- Tag filtering\n- Path filtering\n\nComplex date comparison and tag matching logic is untested.\n\n## Key Functions That Need Tests\n\n1. **`filter_tasks()`** - Main filtering function\n   - Test status filter: only incomplete tasks\n   - Test status filter: only complete tasks\n   - Test no status filter: all tasks\n\n2. **Date filtering**\n   - Test due_after: tasks due after a specific date\n   - Test due_before: tasks due before a specific date\n   - Test date range: tasks between two dates\n   - Test tasks without due dates (should be excluded or included?)\n\n3. **Tag filtering**\n   - Test single tag filter\n   - Test multiple tag filter (AND vs OR logic?)\n   - Test tag not present in task\n\n4. **Path filtering**\n   - Test exact path match\n   - Test partial path match\n   - Test path prefix\n\n5. **Edge cases**\n   - Empty task list\n   - No filters applied (all tasks returned)\n   - Multiple filters combined\n\n## Locations\n\n- `src/filter.rs` - `FilterOptions` struct and `filter_tasks()` function\n\n## Estimated Impact\n\n- ~100-150 lines of test code to add\n- Documents filter behavior (especially edge cases)\n- Enables confident changes to filter logic","status":"closed","priority":3,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:47:47.423303908-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T11:02:16.393822736-06:00","closed_at":"2026-01-21T11:02:16.393822736-06:00","close_reason":"Closed","labels":["simplification-opportunity","testing"]}
{"id":"markdown-todo-extractor-35e","title":"Time-based queries capability","description":"Query notes based on file system timestamps (created, modified). Methods: recently_modified(limit, days), created_in_range(start, end), activity_timeline().","design":"# Implementation Plan: Time-Based Queries Capability\n\n## Overview\n\nAdd a new `TimeQueryCapability` to query markdown files by filesystem timestamps (modified, created). Expose three operations via HTTP, CLI, and MCP:\n1. `recently-modified` - Get files modified in last N days\n2. `created-in-range` - Get files created between two dates\n3. `activity-timeline` - Get aggregated daily activity counts\n\n## Architecture\n\nFollowing the established capability-based architecture:\n- Single capability struct with three operation methods\n- Three operation wrappers implementing the `Operation` trait\n- Automatic registration for HTTP, CLI, and MCP interfaces\n- Uses `chrono` for date/time handling\n- Follows patterns from `TagCapability` (3 operations) and `FileCapability` (traversal/security)\n\n## Key Design Decisions\n\n### 1. Three Separate Operations (not a unified query operation)\n- **Rationale**: Follows existing patterns (tags has 3 operations, files has 2)\n- Each operation has distinct use cases with minimal parameter overlap\n- Simpler, more discoverable APIs\n\n### 2. Date/Time Library: chrono\n- Industry standard for Rust date/time handling\n- Excellent YYYY-MM-DD parsing support\n- RFC 3339 output for JSON serialization\n- Already in dependency tree\n\n### 3. Creation Time Handling\n- **Challenge**: Creation time not available on all filesystems (notably Linux ext4)\n- **Solution**: Gracefully handle with `created: Option\u003cString\u003e`\n- Track `unavailable_count` in responses where relevant\n- Document limitation in API descriptions\n\n### 4. Sorting Strategy\n- Default: Most recent first (intuitive for \"recent\" queries)\n- `created_in_range` supports sort parameter: \"newest\" or \"oldest\"\n\n## Implementation Steps\n\n### Step 1: Add Dependencies\n**File**: `Cargo.toml`\n```toml\n[dependencies]\nchrono = { version = \"0.4\", features = [\"serde\"] }\n```\n\n### Step 2: Create Capability Module\n**File**: `src/capabilities/time_queries.rs` (NEW, ~600-700 lines)\n\nStructure:\n1. **Metadata modules** (3 modules with DESCRIPTION, CLI_NAME, HTTP_PATH constants)\n2. **Request/Response structs** (6 structs total, all with `#[derive(Parser, Serialize, Deserialize, JsonSchema)]`)\n3. **TimeQueryCapability struct** with three async methods\n4. **Helper functions** (`collect_file_metadata`, `extract_file_metadata`, `system_time_to_string`, `parse_date`)\n5. **Three Operation structs** implementing `Operation` trait\n\nKey data structures:\n- FileMetadata: path, name, size_bytes, modified (RFC 3339), created (Optional RFC 3339)\n- RecentlyModifiedRequest: path (CLI only), days (default 7), limit (default 50)\n- CreatedInRangeRequest: path (CLI only), start_date, end_date, limit, sort\n- ActivityTimelineRequest: path (CLI only), days (default 30)\n- DayActivity: date (YYYY-MM-DD), modified_count, created_count\n\n### Step 3: File Scanning Implementation\n\n**Pattern**: Reuse `FileCapability` traversal logic adapted for metadata collection\n- Check exclusions via config.should_exclude()\n- Skip hidden files (starting with '.')\n- Only process .md files\n- Recurse into directories\n- Extract metadata via std::fs::metadata()\n\n**Security**: Follow `FileCapability` patterns:\n- Canonicalize paths before use\n- Validate paths stay within base directory\n- Respect config exclusions\n- Skip hidden files\n\n### Step 4: Operation Implementations\n\n#### Recently Modified Algorithm\n1. Collect all `.md` file metadata\n2. Calculate cutoff: `now - days * 24h` using chrono\n3. Filter where `modified \u003e= cutoff`\n4. Sort by modified (most recent first)\n5. Apply limit, track truncation\n\n#### Created in Range Algorithm\n1. Parse start/end dates (YYYY-MM-DD → `NaiveDate`)\n2. Validate range (start \u003c= end)\n3. Collect all file metadata\n4. Filter by creation date, track `unavailable_count` for files without creation time\n5. Sort by created time (respecting sort parameter)\n6. Apply limit\n\n#### Activity Timeline Algorithm\n1. Calculate date range: `now - days` to `now`\n2. Initialize HashMap with all dates (zero counts)\n3. Collect all file metadata\n4. Bucket files by date for both modified and created\n5. Convert to sorted vector (most recent first)\n\n### Step 5: Register in Capability Registry\n**File**: `src/capabilities/mod.rs`\n\nChanges:\n1. Add `pub mod time_queries;`\n2. Import: `use self::time_queries::TimeQueryCapability;`\n3. Add field: `time_query_capability: Arc\u003cTimeQueryCapability\u003e`\n4. Initialize in `new()`: `time_query_capability: Arc::new(TimeQueryCapability::new(...))`\n5. Add getter: `pub fn time_queries(\u0026self) -\u003e Arc\u003cTimeQueryCapability\u003e`\n6. Add 3 operations to `create_operations()`\n\n### Step 6: Build and Test\n1. `cargo build` - verify compilation\n2. Run unit tests (if implemented)\n3. Manual CLI testing\n4. Manual HTTP testing (if running server)\n5. Manual MCP testing (via inspector)\n\n## Critical Files\n\n### Files to Create\n- `src/capabilities/time_queries.rs` (NEW, ~600-700 lines)\n\n### Files to Modify\n- `Cargo.toml` - Add chrono dependency\n- `src/capabilities/mod.rs` - Register capability and operations\n\n### Reference Files (read-only)\n- `src/capabilities/files.rs` - Security patterns, traversal logic\n- `src/capabilities/tags.rs` - Multi-operation capability pattern\n- `src/operation.rs` - Operation trait definition\n- `src/http_router.rs` - `execute_json_operation` helper\n- `src/error.rs` - Error handling (`internal_error`, `invalid_params`)\n\n## Error Handling\n\nFollow existing patterns:\n- `invalid_params()` for invalid dates, invalid date ranges\n- `internal_error()` for I/O errors, path resolution failures\n- Continue on individual file errors (don't fail entire scan)\n- Document filesystem limitations in responses\n\n## Testing Strategy\n\n### Manual CLI Testing\n```bash\ncargo run -- recently-modified /path/to/vault --days 7 --limit 10\ncargo run -- created-in-range /path/to/vault --start-date 2025-01-01 --end-date 2025-01-20 --sort newest --limit 20\ncargo run -- activity-timeline /path/to/vault --days 30\n```\n\n### Manual HTTP Testing\n```bash\ncurl \"http://localhost:3000/api/time-queries/recently-modified?days=7\u0026limit=10\"\ncurl -X POST http://localhost:3000/api/time-queries/created-in-range -H \"Content-Type: application/json\" -d '{\"start_date\": \"2025-01-01\", \"end_date\": \"2025-01-20\"}'\ncurl \"http://localhost:3000/api/time-queries/activity-timeline?days=30\"\n```\n\n### Verification Checklist\n- [ ] `cargo build` succeeds\n- [ ] CLI commands return valid JSON\n- [ ] HTTP endpoints accessible and return correct data\n- [ ] Respects config exclusions\n- [ ] Skips hidden files\n- [ ] Only processes `.md` files\n- [ ] Handles missing creation time gracefully\n- [ ] Date parsing validates YYYY-MM-DD format\n- [ ] Invalid date ranges return errors\n- [ ] Results sorted correctly\n- [ ] Limits applied correctly\n\n## Performance Considerations\n\n**Current approach**: Single-threaded file traversal\n- Sufficient for most vaults (\u003c10k files)\n- Profile if performance issues arise\n- Consider parallelization with `rayon` if needed\n\n## Success Criteria\n\n1. Three new CLI commands work and output valid JSON\n2. Three new HTTP endpoints accessible and functional\n3. Three new MCP tools exposed (automatic via capability registry)\n4. Follows established architecture patterns exactly\n5. Handles edge cases gracefully (missing creation time, invalid dates)\n6. Respects configuration (exclusions, hidden files)","status":"open","priority":2,"issue_type":"feature","owner":"jeff@jeffutter.com","created_at":"2026-01-21T20:52:03.622063164-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-22T06:38:59.999114838-06:00","labels":["planned"]}
{"id":"markdown-todo-extractor-40j","title":"Remove unused markdown dependency","description":"## Problem\n\nThe `markdown = \"1.0.0\"` dependency is listed in `Cargo.toml` but does not appear to be used anywhere in the source code.\n\n## Investigation Required\n\nSearch for:\n- `use markdown` - No results expected\n- `markdown::` - No results expected\n- Any actual markdown parsing that might justify the dependency\n\n## Possible Explanations\n\n1. **Vestigial**: Was used in earlier design and forgotten during cleanup\n2. **Indirect**: Used implicitly through another crate (unlikely)\n3. **Future feature**: Intended for planned functionality (but speculative design)\n\n## Proposed Solution\n\nIf confirmed unused:\n```toml\n# Remove from Cargo.toml\nmarkdown = \"1.0.0\"  # DELETE THIS LINE\n```\n\n## Estimated Impact\n\n- Reduces compile time\n- Reduces binary size\n- Cleaner dependency list","status":"closed","priority":4,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:48:18.57043359-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T06:12:39.566188467-06:00","closed_at":"2026-01-21T06:12:39.566188467-06:00","close_reason":"Removed unused markdown dependency from Cargo.toml. Verified with grep search that it was not used anywhere in the codebase. Build successful after removal.","labels":["dependencies","simplification-opportunity"]}
{"id":"markdown-todo-extractor-4ia","title":"Optimize tokio features to reduce binary size","description":"## Problem\n\nIn `Cargo.toml`, tokio is included with `features = [\"full\"]`:\n\n```toml\ntokio = { version = \"1.44.2\", features = [\"full\"] }\n```\n\nThis includes many features not needed by this CLI/server application:\n- `signal` - Unix signal handling\n- `process` - Child process spawning\n- `fs` - File system operations (we use std::fs)\n- `tracing` - Tracing integration\n- Various test utilities\n\n## Proposed Solution\n\nReplace `full` with only the features actually needed:\n\n```toml\ntokio = { version = \"1.44.2\", features = [\"rt-multi-thread\", \"macros\", \"net\", \"io-util\", \"sync\"] }\n```\n\nFeatures breakdown:\n- `rt-multi-thread` - Multi-threaded runtime for async execution\n- `macros` - `#[tokio::main]` attribute macro\n- `net` - TCP/UDP for HTTP server\n- `io-util` - Async I/O utilities\n- `sync` - Synchronization primitives (channels, mutexes)\n\n## Investigation Required\n\n1. Check which tokio features are actually used in the code\n2. May need `time` feature if any timeouts are used\n3. May need `signal` if graceful shutdown is implemented\n\n## Estimated Impact\n\n- Reduced binary size (~1-3 MB smaller)\n- Faster compile times\n- Clearer declaration of actual dependencies","status":"closed","priority":4,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:48:18.912714052-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T11:07:41.021864695-06:00","closed_at":"2026-01-21T11:07:41.021868932-06:00","labels":["dependencies","simplification-opportunity"]}
{"id":"markdown-todo-extractor-5i0","title":"Implement CLI automatic registration for files operations","description":"Apply CLI automatic registration pattern to 2 file operations (list_files, read_file). Follow the same pattern used for tasks: add Parser derives to request structs, implement CliOperation for operation structs, update create_cli_operations() in capabilities/mod.rs.","status":"closed","priority":2,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T19:35:13.508211743-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T21:11:31.744212654-06:00","closed_at":"2026-01-20T21:11:31.744212654-06:00","close_reason":"Closed","dependencies":[{"issue_id":"markdown-todo-extractor-5i0","depends_on_id":"markdown-todo-extractor-b68","type":"blocks","created_at":"2026-01-20T19:35:29.746867336-06:00","created_by":"Jeffery Utter"}],"comments":[{"id":2,"issue_id":"markdown-todo-extractor-5i0","author":"Jeffery Utter","text":"Implementation Guide - Files Operations\n\nFollow the CLI automatic registration pattern documented in CLAUDE.md section \"Adding CLI Automatic Registration for an Operation\".\n\nReference: src/capabilities/tasks.rs SearchTasksOperation for working example\n\nDepends on: markdown-todo-extractor-b68 (tags operations must be converted first)\n\nOperations to convert (2 total):\n1. ListFilesOperation (list_files) - CLI name: \"list-files\"\n2. ReadFileOperation (read_file) - CLI name: \"read-file\"\n\nFor EACH operation:\n1. Add Parser derive to request struct (ListFilesRequest, ReadFileRequest)\n   - Import: use clap::{CommandFactory, FromArgMatches, Parser};\n   - Add Parser to #[derive(...)]\n   - Add #[command(name = \"...\", about = \"...\")]\n   - For ReadFileRequest: may need special handling since it takes vault_path + file_path\n   - Add #[arg(...)] to all existing fields\n\n2. Implement CliOperation trait for operation struct\n   - command_name() - use existing CLI_NAME constant\n   - get_command() - call RequestStruct::command()\n   - execute_from_args() - parse args, handle path, call capability method, return JSON\n\n3. Register in create_cli_operations() in src/capabilities/mod.rs\n   - Uncomment the Arc::new(...) lines for file operations\n\n4. Update src/cli.rs - remove manual routing\n   - Remove ListFiles/ReadFile from Commands enum\n   - Remove command structs\n   - Remove match arms from run_cli()\n   - Update the check at top of run_cli() to include these commands\n\n5. Once all operations converted, simplify run_cli()\n   - Remove the conditional check for specific commands\n   - Make ALL commands use automatic routing\n   - Remove the old match statement entirely\n   - Keep only the automatic routing path\n\nTesting:\ncargo run -- list-files /tmp/test-vault --max-depth 2\ncargo run -- read-file /tmp/test-vault test.md","created_at":"2026-01-21T01:43:17Z"}]}
{"id":"markdown-todo-extractor-6oq","title":"Remove duplicate merge_from_env_var implementations in config.rs","description":"## Problem\n\nIn `src/config.rs`, the `merge_from_env_var` function is defined twice with `#[cfg(test)]` and `#[cfg(not(test))]` guards (lines 45-72), but **both implementations are identical**:\n\n```rust\n#[cfg(test)]\nfn merge_from_env_var(\u0026mut self, var_name: \u0026str) {\n    if let Ok(paths) = std::env::var(var_name) {\n        let env_paths: Vec\u003cString\u003e = paths\n            .split(',')\n            .map(|s| s.trim().to_string())\n            .filter(|s| !s.is_empty())\n            .collect();\n        self.exclude_paths.extend(env_paths);\n    }\n}\n\n#[cfg(not(test))]\nfn merge_from_env_var(\u0026mut self, var_name: \u0026str) {\n    if let Ok(paths) = std::env::var(var_name) {\n        let env_paths: Vec\u003cString\u003e = paths\n            .split(',')\n            .map(|s| s.trim().to_string())\n            .filter(|s| !s.is_empty())\n            .collect();\n        self.exclude_paths.extend(env_paths);\n    }\n}\n```\n\nThis is unnecessary duplication. The only difference should be at the call site (whether to call the function or not during tests).\n\n## Proposed Solution\n\nRemove the cfg guards from the function definition. If tests need to avoid merging env vars, handle it at the call site in `load_from_base_path()`:\n\n```rust\nfn merge_from_env_var(\u0026mut self, var_name: \u0026str) {\n    if let Ok(paths) = std::env::var(var_name) {\n        let env_paths: Vec\u003cString\u003e = paths\n            .split(',')\n            .map(|s| s.trim().to_string())\n            .filter(|s| !s.is_empty())\n            .collect();\n        self.exclude_paths.extend(env_paths);\n    }\n}\n```\n\n## Locations\n\n- `src/config.rs` lines 45-72\n\n## Estimated Impact\n\n- ~15 lines of duplicated code removed\n- Clearer code intent\n- Single function to maintain","status":"closed","priority":4,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:47:47.756692007-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T06:08:45.903267771-06:00","closed_at":"2026-01-21T06:08:45.903267771-06:00","close_reason":"Closed","labels":["simplification-opportunity"]}
{"id":"markdown-todo-extractor-6uj","title":"Implement CLI output trait for custom formatting","description":"Create trait/function to allow custom output formats (table, yaml, etc.) instead of JSON-only. This would allow a --format flag on commands. Should work with all CLI operations that use automatic registration.","status":"deferred","priority":3,"issue_type":"feature","owner":"jeff@jeffutter.com","created_at":"2026-01-20T19:35:18.972182877-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T10:36:52.735776821-06:00","dependencies":[{"issue_id":"markdown-todo-extractor-6uj","depends_on_id":"markdown-todo-extractor-b68","type":"blocks","created_at":"2026-01-20T19:35:34.167968323-06:00","created_by":"Jeffery Utter"},{"issue_id":"markdown-todo-extractor-6uj","depends_on_id":"markdown-todo-extractor-5i0","type":"blocks","created_at":"2026-01-20T19:35:34.195716504-06:00","created_by":"Jeffery Utter"}],"comments":[{"id":3,"issue_id":"markdown-todo-extractor-6uj","author":"Jeffery Utter","text":"Implementation Guide - CLI Output Trait\n\nCreate a trait/system to allow custom output formats beyond JSON.\n\nDepends on: All CLI operations must use automatic registration first (b68, 5i0)\n\nGoal: Add --format flag to allow output in different formats:\n- json (default, current behavior)\n- table (pretty-printed table for humans)\n- yaml (YAML format)\n- csv (for operations that return lists)\n\nSuggested Approach:\n\n1. Create OutputFormat trait in src/cli_router.rs or new file:\n   trait OutputFormat {\n       fn format(\u0026self, data: \u0026impl Serialize) -\u003e Result\u003cString, Box\u003cdyn Error\u003e\u003e;\n   }\n\n2. Implement for different formats:\n   - JsonFormat (current behavior)\n   - TableFormat (using prettytable-rs or comfy-table)\n   - YamlFormat (using serde_yaml)\n   - CsvFormat (using csv crate, for list-type responses)\n\n3. Add --format global flag to cli_router::build_cli()\n\n4. Update CliOperation trait:\n   - Add format parameter to execute_from_args()\n   - Or return raw data and let router handle formatting\n\n5. Update cli_router::execute_cli() to:\n   - Parse --format flag\n   - Create appropriate formatter\n   - Apply formatter to operation output\n\nDesign Decision: \nShould formatting happen IN each operation or AFTER in the router?\n- After in router = cleaner, operations return structured data\n- In operation = more flexible, operation-specific formatting\n\nRecommend: Router handles formatting (cleaner separation of concerns)\n\nTesting:\ncargo run -- tasks /tmp/test-vault --format json\ncargo run -- tasks /tmp/test-vault --format table\ncargo run -- list-tags /tmp/test-vault --format yaml","created_at":"2026-01-21T01:43:36Z"}]}
{"id":"markdown-todo-extractor-718","title":"Support reading multiple files in single request","description":"Enhance existing file reading capability to accept a list of file paths instead of just one, returning content for all requested files in a single call.","design":"# Implementation Plan: Multi-file Reading Support\n\n**Ticket**: markdown-todo-extractor-718\n**Goal**: Enhance file reading capability to accept a list of file paths and return content for all requested files in a single call.\n\n## Overview\n\nAdd a new `read_files` (plural) operation to complement the existing `read_file` (singular) operation. This enables efficient batch reading of multiple markdown files in a single request, which is required for the daily notes capability (markdown-todo-extractor-vw9).\n\n## Design Decision: New Operation vs. Extending Existing\n\n**Decision**: Create a new `read_files` operation alongside the existing `read_file` operation.\n\n**Rationale**:\n- **Simplicity**: Keeps single-file and multi-file use cases separate with clear semantics\n- **Backward compatibility**: Existing clients using `read_file` are unaffected\n- **Response structure**: Multi-file responses need metadata (success/failure per file) that single-file doesn't\n- **Error handling**: Single file can fail fast; multi-file should collect partial results\n- **Consistent with patterns**: The codebase uses plural operations for bulk operations (e.g., `search_tasks` returns multiple tasks)\n\n## Request/Response Design\n\n### ReadFilesRequest\n\n```rust\n#[derive(Debug, Deserialize, JsonSchema, clap::Parser)]\n#[command(name = \"read-files\", about = \"Read multiple markdown files\")]\npub struct ReadFilesRequest {\n    /// Vault path (CLI only)\n    #[arg(index = 1, required = true, help = \"Path to vault\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(skip)]\n    pub vault_path: Option\u003cPathBuf\u003e,\n\n    /// File paths relative to vault root\n    #[arg(index = 2, required = true, value_delimiter = ',',\n          help = \"Comma-separated file paths relative to vault root\")]\n    #[schemars(description = \"File paths relative to vault root\")]\n    pub file_paths: Vec\u003cString\u003e,\n\n    /// Continue on error (return partial results)\n    #[arg(long, help = \"Continue reading files even if some fail\")]\n    #[schemars(description = \"If true, continue on errors and return partial results\")]\n    pub continue_on_error: Option\u003cbool\u003e,\n}\n```\n\n**Key features**:\n- `file_paths: Vec\u003cString\u003e` - Array of relative paths\n- `value_delimiter = ','` - CLI can pass `file1.md,file2.md,file3.md`\n- `continue_on_error` - Determines error handling strategy\n\n### ReadFilesResponse\n\n```rust\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct ReadFilesResponse {\n    /// Successfully read files\n    pub files: Vec\u003cReadFileResult\u003e,\n\n    /// Total number of files requested\n    pub total_requested: usize,\n\n    /// Number of files successfully read\n    pub success_count: usize,\n\n    /// Number of files that failed\n    pub failure_count: usize,\n}\n\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct ReadFileResult {\n    /// File path relative to vault root\n    pub file_path: String,\n\n    /// File name only\n    pub file_name: String,\n\n    /// Whether this file was successfully read\n    pub success: bool,\n\n    /// File content (only present if success=true)\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub content: Option\u003cString\u003e,\n\n    /// Error message (only present if success=false)\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub error: Option\u003cString\u003e,\n}\n```\n\n**Design rationale**:\n- **Partial results**: Return both successful and failed reads with metadata\n- **Error per file**: Each file has its own success/error state\n- **Metadata counts**: Easy for clients to determine overall success\n\n## Error Handling Strategy\n\nTwo modes based on `continue_on_error`:\n\n### Mode 1: Fail Fast (default, continue_on_error=false)\n- Validate all paths first (existence, security, file type)\n- If any validation fails, return error immediately (no partial results)\n- All-or-nothing semantics\n\n### Mode 2: Partial Results (continue_on_error=true)\n- Process each file individually\n- Collect successes and failures\n- Return `ReadFilesResponse` with mixed results\n- Each failed file gets `success: false` with error message\n- Operation succeeds even if some files fail\n\n**Validation order** (fail fast mode):\n1. Check all paths are non-empty\n2. Canonicalize all paths (must exist)\n3. Verify all paths are within vault base (security)\n4. Verify all paths are .md files\n5. Only then read file contents\n\n## Implementation Details\n\n### File: `src/capabilities/files.rs`\n\n**Add operation metadata module**:\n```rust\npub mod read_files {\n    pub const DESCRIPTION: \u0026str = \"Read multiple markdown files in a single request. Returns content for all requested files with per-file success/error status.\";\n    pub const CLI_NAME: \u0026str = \"read-files\";\n    pub const HTTP_PATH: \u0026str = \"/api/files/read-multiple\";\n}\n```\n\n**Add to FileCapability**:\n```rust\nimpl FileCapability {\n    /// Read multiple markdown files\n    pub async fn read_files(\n        \u0026self,\n        request: ReadFilesRequest,\n    ) -\u003e CapabilityResult\u003cReadFilesResponse\u003e {\n        let continue_on_error = request.continue_on_error.unwrap_or(false);\n\n        // Validation phase (if fail-fast mode)\n        if !continue_on_error {\n            self.validate_all_paths(\u0026request.file_paths)?;\n        }\n\n        // Reading phase\n        let mut results = Vec::new();\n        let mut success_count = 0;\n        let mut failure_count = 0;\n\n        for file_path in \u0026request.file_paths {\n            match self.read_single_file(file_path) {\n                Ok(content) =\u003e {\n                    results.push(ReadFileResult {\n                        file_path: file_path.clone(),\n                        file_name: extract_file_name(file_path),\n                        success: true,\n                        content: Some(content),\n                        error: None,\n                    });\n                    success_count += 1;\n                }\n                Err(e) =\u003e {\n                    if continue_on_error {\n                        results.push(ReadFileResult {\n                            file_path: file_path.clone(),\n                            file_name: extract_file_name(file_path),\n                            success: false,\n                            content: None,\n                            error: Some(e.to_string()),\n                        });\n                        failure_count += 1;\n                    } else {\n                        return Err(e);\n                    }\n                }\n            }\n        }\n\n        Ok(ReadFilesResponse {\n            files: results,\n            total_requested: request.file_paths.len(),\n            success_count,\n            failure_count,\n        })\n    }\n\n    /// Validate all paths before reading (fail-fast mode)\n    fn validate_all_paths(\u0026self, file_paths: \u0026[String]) -\u003e CapabilityResult\u003c()\u003e {\n        // Check non-empty\n        if file_paths.is_empty() {\n            return Err(invalid_params(\"file_paths cannot be empty\"));\n        }\n\n        // Canonicalize base path once\n        let canonical_base = self.base_path.canonicalize()\n            .map_err(|e| internal_error(format!(\"Failed to resolve base path: {}\", e)))?;\n\n        // Validate each path\n        for file_path in file_paths {\n            let requested_path = PathBuf::from(file_path);\n            let full_path = self.base_path.join(\u0026requested_path);\n\n            // Check existence\n            let canonical_full = full_path.canonicalize()\n                .map_err(|_| invalid_params(format!(\"File not found: {}\", file_path)))?;\n\n            // Security check\n            if !canonical_full.starts_with(\u0026canonical_base) {\n                return Err(invalid_params(format!(\n                    \"Invalid path '{}': must be within vault\",\n                    file_path\n                )));\n            }\n\n            // File type check\n            if canonical_full.extension().and_then(|s| s.to_str()) != Some(\"md\") {\n                return Err(invalid_params(format!(\n                    \"Invalid file type '{}': only .md files allowed\",\n                    file_path\n                )));\n            }\n        }\n\n        Ok(())\n    }\n\n    /// Read a single file (internal helper, reused from read_file logic)\n    fn read_single_file(\u0026self, file_path: \u0026str) -\u003e CapabilityResult\u003cString\u003e {\n        // Same logic as read_file but extracted as helper\n        // Returns just the content string\n        // ... (reuse existing read_file validation + read logic)\n    }\n}\n```\n\n**Add ReadFilesOperation struct**:\n```rust\npub struct ReadFilesOperation {\n    capability: Arc\u003cFileCapability\u003e,\n}\n\nimpl ReadFilesOperation {\n    pub fn new(capability: Arc\u003cFileCapability\u003e) -\u003e Self {\n        Self { capability }\n    }\n}\n\n#[async_trait::async_trait]\nimpl crate::operation::Operation for ReadFilesOperation {\n    fn name(\u0026self) -\u003e \u0026'static str {\n        read_files::CLI_NAME\n    }\n\n    fn path(\u0026self) -\u003e \u0026'static str {\n        read_files::HTTP_PATH\n    }\n\n    fn description(\u0026self) -\u003e \u0026'static str {\n        read_files::DESCRIPTION\n    }\n\n    fn get_command(\u0026self) -\u003e clap::Command {\n        ReadFilesRequest::command()\n    }\n\n    async fn execute_json(\n        \u0026self,\n        json: serde_json::Value,\n    ) -\u003e Result\u003cserde_json::Value, rmcp::model::ErrorData\u003e {\n        crate::http_router::execute_json_operation(json, |req| {\n            self.capability.read_files(req)\n        })\n        .await\n    }\n\n    async fn execute_from_args(\n        \u0026self,\n        matches: \u0026clap::ArgMatches,\n        _registry: \u0026crate::capabilities::CapabilityRegistry,\n    ) -\u003e Result\u003cString, Box\u003cdyn std::error::Error\u003e\u003e {\n        let request = ReadFilesRequest::from_arg_matches(matches)?;\n\n        let response = if let Some(ref vault_path) = request.vault_path {\n            let config = Arc::new(Config::load_from_base_path(vault_path.as_path()));\n            let capability = FileCapability::new(vault_path.clone(), config);\n            let mut req_without_path = request;\n            req_without_path.vault_path = None;\n            capability.read_files(req_without_path).await?\n        } else {\n            self.capability.read_files(request).await?\n        };\n\n        Ok(serde_json::to_string_pretty(\u0026response)?)\n    }\n\n    fn input_schema(\u0026self) -\u003e serde_json::Value {\n        use schemars::schema_for;\n        serde_json::to_value(schema_for!(ReadFilesRequest)).unwrap()\n    }\n}\n```\n\n### File: `src/capabilities/mod.rs`\n\n**Register the operation**:\n```rust\npub fn create_operations(\u0026self) -\u003e Vec\u003cArc\u003cdyn Operation\u003e\u003e {\n    vec![\n        // ... existing operations ...\n        Arc::new(files::ReadFileOperation::new(self.files())),\n        Arc::new(files::ReadFilesOperation::new(self.files())),  // Add this\n    ]\n}\n```\n\n### File: `src/mcp.rs`\n\n**Add MCP tool**:\n```rust\n#[tool(description = \"Read multiple markdown files from the vault in a single request\")]\nasync fn read_files(\n    \u0026self,\n    Parameters(request): Parameters\u003cReadFilesRequest\u003e,\n) -\u003e Result\u003cJson\u003cReadFilesResponse\u003e, ErrorData\u003e {\n    let response = self.capability_registry.files().read_files(request).await?;\n    Ok(Json(response))\n}\n```\n\n## Code Reuse Strategy\n\n**Extract common logic from `read_file`**:\n\nThe existing `read_file` method has ~50 lines of logic. Extract the core into helper methods:\n\n1. `read_single_file(\u0026self, file_path: \u0026str) -\u003e CapabilityResult\u003cString\u003e`\n   - Canonicalize path\n   - Security validation\n   - File type validation\n   - Read content\n   - Returns content string\n\n2. Keep existing `read_file` method (for backward compatibility):\n   ```rust\n   pub async fn read_file(\u0026self, request: ReadFileRequest) -\u003e CapabilityResult\u003cReadFileResponse\u003e {\n       let content = self.read_single_file(\u0026request.file_path)?;\n\n       // Build response with metadata\n       Ok(ReadFileResponse {\n           content,\n           file_path: request.file_path.clone(),\n           file_name: extract_file_name(\u0026request.file_path),\n       })\n   }\n   ```\n\n3. New `read_files` calls `read_single_file` in a loop\n\n**Benefits**:\n- No code duplication\n- Single source of truth for validation logic\n- Easier to maintain and test\n\n## Critical Files\n\n### Files to Modify\n1. **`src/capabilities/files.rs`** (~150 lines added)\n   - Add `read_files` operation metadata module\n   - Add `ReadFilesRequest` and `ReadFilesResponse` structs\n   - Add `ReadFileResult` struct\n   - Add `FileCapability::read_files()` method\n   - Add `FileCapability::validate_all_paths()` helper\n   - Refactor: Extract `read_single_file()` from existing `read_file()`\n   - Add `ReadFilesOperation` struct with Operation trait impl\n\n2. **`src/capabilities/mod.rs`** (~2 lines)\n   - Register `ReadFilesOperation` in `create_operations()`\n\n3. **`src/mcp.rs`** (~10 lines)\n   - Add `read_files` MCP tool that delegates to capability\n\n### Files to Read (for reference)\n4. **`src/capabilities/tags.rs`** - Reference for array parameter patterns\n5. **`src/operation.rs`** - Operation trait definition\n6. **`src/error.rs`** - Error handling utilities\n\n## Verification Plan\n\n### Manual Testing\n\n**Setup test vault**:\n```bash\nmkdir -p /tmp/test_vault/subfolder\necho \"# Note 1\" \u003e /tmp/test_vault/note1.md\necho \"# Note 2\" \u003e /tmp/test_vault/note2.md\necho \"# Note 3\" \u003e /tmp/test_vault/subfolder/note3.md\n```\n\n**Test CLI - Success case**:\n```bash\ncargo run -- read-files /tmp/test_vault note1.md,note2.md,subfolder/note3.md\n# Expected: JSON with 3 successful files\n```\n\n**Test CLI - Partial failure (continue on error)**:\n```bash\ncargo run -- read-files /tmp/test_vault note1.md,nonexistent.md,note2.md --continue-on-error\n# Expected: 2 successes, 1 failure in response\n```\n\n**Test CLI - Fail fast**:\n```bash\ncargo run -- read-files /tmp/test_vault note1.md,nonexistent.md\n# Expected: Error, no partial results\n```\n\n**Test HTTP** (with server running):\n```bash\ncurl -X POST http://localhost:3000/api/files/read-multiple \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"file_paths\": [\"note1.md\", \"note2.md\"],\n    \"continue_on_error\": false\n  }'\n```\n\n**Test MCP** (using Claude Code or other MCP client):\n```typescript\nawait client.call(\"read_files\", {\n  file_paths: [\"note1.md\", \"note2.md\", \"subfolder/note3.md\"]\n});\n```\n\n### Edge Cases to Test\n\n1. **Empty array**: `file_paths: []` → Error\n2. **Single file**: `file_paths: [\"note1.md\"]` → Works (same as read_file but different response format)\n3. **Duplicate paths**: `file_paths: [\"note1.md\", \"note1.md\"]` → Read twice (client's choice)\n4. **Path traversal attempt**: `file_paths: [\"../../../etc/passwd\"]` → Error (security validation)\n5. **Non-md file**: `file_paths: [\"image.png\"]` → Error\n6. **Large batch**: 100 files → Test performance\n7. **Mixed success/failure**: Some files exist, some don't (with continue_on_error) → Partial results\n\n### Build and Quality Gates\n\n```bash\n# Format code\ncargo fmt\n\n# Run linter\ncargo clippy -- -D warnings\n\n# Build debug\ncargo build\n\n# Build release\ncargo build --release\n\n# Run the tool\ncargo run -- read-files /path/to/vault file1.md,file2.md\n```\n\n## Usage Examples\n\n### CLI\n```bash\n# Read multiple files\ncargo run -- read-files /vault note1.md,note2.md,subfolder/note3.md\n\n# With continue on error\ncargo run -- read-files /vault note1.md,missing.md --continue-on-error\n```\n\n### HTTP\n```bash\ncurl -X POST http://localhost:3000/api/files/read-multiple \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"file_paths\": [\"Daily/2025-01-20.md\", \"Daily/2025-01-21.md\", \"Daily/2025-01-22.md\"],\n    \"continue_on_error\": true\n  }'\n```\n\n### MCP (from daily notes capability)\n```rust\n// In daily_notes capability, can now batch read multiple daily notes\nlet file_paths: Vec\u003cString\u003e = dates\n    .iter()\n    .map(|date| format!(\"Daily/{}.md\", date))\n    .collect();\n\nlet request = ReadFilesRequest {\n    vault_path: None,\n    file_paths,\n    continue_on_error: Some(true),\n};\n\nlet response = file_capability.read_files(request).await?;\n\n// Filter successful reads\nlet daily_notes: Vec\u003cDailyNote\u003e = response.files\n    .into_iter()\n    .filter(|f| f.success)\n    .map(|f| DailyNote {\n        date: extract_date_from_path(\u0026f.file_path),\n        content: f.content.unwrap(),\n    })\n    .collect();\n```\n\n## Trade-offs and Rationale\n\n### New Operation vs. Optional Array Parameter\n\n**Considered**: Adding `file_paths: Option\u003cVec\u003cString\u003e\u003e` to existing `ReadFileRequest`\n\n**Rejected because**:\n- Complicates existing operation semantics\n- Response structure would need to be generic (single file vs. multiple files)\n- Error handling would be inconsistent\n- Breaks single responsibility principle\n\n**Chosen**: Separate `read_files` operation\n\n**Benefits**:\n- Clear separation of concerns\n- Each operation has simple, focused semantics\n- Easy to maintain and test independently\n- Clients can choose appropriate operation for their use case\n\n### Fail Fast vs. Continue on Error\n\n**Decision**: Support both modes via `continue_on_error` parameter\n\n**Rationale**:\n- Fail fast (default) - Best for strict validation use cases\n- Continue on error - Best for bulk operations where partial results are valuable\n- Daily notes use case benefits from partial results (some days may not have notes)\n- Gives clients flexibility to choose error handling strategy\n\n### Duplicate Path Handling\n\n**Decision**: Allow duplicates, read each path as requested\n\n**Rationale**:\n- Simple implementation (no deduplication logic)\n- Preserves client intent\n- If client wants deduplication, they can dedupe before calling\n- Edge case is unlikely in practice\n\n## Future Enhancements\n\n1. **Parallel reading**: Currently sequential; could use tokio tasks for parallel I/O\n2. **Caching**: Cache file contents for repeated reads\n3. **Size limits**: Add max total size or max files per request to prevent abuse\n4. **Streaming**: For very large batches, stream results as they're read\n5. **Metadata-only mode**: Return file metadata without content (for lightweight checks)","status":"open","priority":2,"issue_type":"feature","owner":"jeff@jeffutter.com","created_at":"2026-01-21T20:52:02.98399341-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-22T08:31:16.056874341-06:00","labels":["planned"]}
{"id":"markdown-todo-extractor-7i7","title":"Create generic JSON serialization wrapper for operations","description":"## Problem\n\nEvery HTTP operation has an identical `execute_json` implementation (~19 lines each) that:\n1. Deserializes request from `serde_json::Value`\n2. Calls the capability method\n3. Serializes response back to `serde_json::Value`\n\nThis pattern is **repeated identically in all 6 operations**:\n- `SearchTasksOperation` (tasks.rs)\n- `ExtractTagsOperation`, `ListTagsOperation`, `SearchByTagsOperation` (tags.rs)\n- `ListFilesOperation`, `ReadFileOperation` (files.rs)\n\n## Example of duplicated code\n\nFrom `tags.rs` (and nearly identical in 5 other places):\n```rust\nasync fn execute_json(\u0026self, json: serde_json::Value) -\u003e Result\u003cserde_json::Value, ErrorData\u003e {\n    let request: ListTagsRequest = serde_json::from_value(json).map_err(|e| ErrorData {\n        code: rmcp::model::ErrorCode(-32602),\n        message: Cow::from(format!(\"Invalid request parameters: {}\", e)),\n        data: None,\n    })?;\n\n    let response = self.capability.list_tags(request).await?;\n\n    serde_json::to_value(response).map_err(|e| ErrorData {\n        code: rmcp::model::ErrorCode(-32603),\n        message: Cow::from(format!(\"Failed to serialize response: {}\", e)),\n        data: None,\n    })\n}\n```\n\n## Proposed Solution\n\nCreate generic helpers in `http_router.rs` or a shared module:\n\n```rust\npub async fn execute_json_operation\u003cReq, Resp, F, Fut\u003e(\n    json: serde_json::Value,\n    operation: F,\n) -\u003e Result\u003cserde_json::Value, ErrorData\u003e\nwhere\n    Req: DeserializeOwned,\n    Resp: Serialize,\n    F: FnOnce(Req) -\u003e Fut,\n    Fut: Future\u003cOutput = CapabilityResult\u003cResp\u003e\u003e,\n{\n    let request: Req = deserialize_request(json)?;\n    let response = operation(request).await?;\n    serialize_response(response)\n}\n```\n\nThen each operation's `execute_json` becomes a one-liner:\n```rust\nasync fn execute_json(\u0026self, json: Value) -\u003e Result\u003cValue, ErrorData\u003e {\n    execute_json_operation(json, |req| self.capability.list_tags(req)).await\n}\n```\n\n## Estimated Impact\n\n- ~120 lines of duplicated serialization code eliminated\n- Single point of change for serialization logic\n- Consistent error handling across all operations\n\n## Dependencies\n\n- Should be done after \"Extract boilerplate error handling helpers\" for cleaner implementation","status":"closed","priority":2,"issue_type":"task","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:47:14.261216937-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T05:48:04.55133958-06:00","closed_at":"2026-01-21T05:48:04.55133958-06:00","close_reason":"Closed","labels":["simplification-opportunity"],"dependencies":[{"issue_id":"markdown-todo-extractor-7i7","depends_on_id":"markdown-todo-extractor-ie0","type":"blocks","created_at":"2026-01-20T22:47:46.37232196-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-84a","title":"Consider replacing config crate with simpler toml crate","description":"## Problem\n\nThe `config` crate (26KB) provides a comprehensive configuration framework with:\n- Multiple file format support (TOML, JSON, YAML, INI)\n- Environment variable binding\n- Layered configuration sources\n- Type coercion\n\nHowever, this project only uses:\n- TOML file loading from `.markdown-todo-extractor.toml`\n- Manual environment variable handling (done separately)\n\nThe `config` crate may be overkill for this use case.\n\n## Current Usage\n\nFrom `src/config.rs`:\n```rust\nuse config::{Config as ConfigBuilder, File};\n\nlet settings = ConfigBuilder::builder()\n    .add_source(File::with_name(\u0026config_path.to_string_lossy()).required(false))\n    .build()\n    .ok()?;\n\nsettings.try_deserialize::\u003cSelf\u003e().ok()\n```\n\n## Proposed Alternative\n\nUse the `toml` crate directly (~12KB):\n\n```rust\nuse std::fs;\nuse toml;\n\nlet content = fs::read_to_string(\u0026config_path).ok()?;\nlet config: Config = toml::from_str(\u0026content).ok()?;\n```\n\nOr with error handling:\n```rust\npub fn load_from_base_path(base_path: \u0026Path) -\u003e Self {\n    let config_path = base_path.join(\".markdown-todo-extractor.toml\");\n    \n    match fs::read_to_string(\u0026config_path) {\n        Ok(content) =\u003e toml::from_str(\u0026content).unwrap_or_default(),\n        Err(_) =\u003e Self::default(),\n    }\n}\n```\n\n## Trade-offs\n\n**Current (`config` crate):**\n- More flexible if we add JSON/YAML support later\n- Handles missing files gracefully\n- More complex API\n\n**Proposed (`toml` crate):**\n- Simpler, more direct\n- Smaller dependency\n- Explicit error handling\n- Limited to TOML format\n\n## Investigation Required\n\n1. Are there plans to support multiple config formats?\n2. Are other `config` crate features being used that I missed?\n\n## Estimated Impact\n\n- Smaller binary size\n- Simpler code\n- One less complex dependency","status":"closed","priority":4,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:48:50.12793518-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T11:10:51.135890992-06:00","closed_at":"2026-01-21T11:10:51.135894739-06:00","labels":["dependencies","simplification-opportunity"]}
{"id":"markdown-todo-extractor-92i","title":"Add test coverage for extractor.rs","description":"## Problem\n\nThe core task extraction module (`src/extractor.rs` - 421 lines) has **zero tests**. This is the most critical module containing:\n\n- Regex pattern matching for task detection (`- [ ]`, `- [x]`, `- [-]`, `- [?]`)\n- Metadata extraction (tags, dates, priorities)\n- Content cleaning (removing metadata from task text)\n- Sub-item detection and parsing\n\nWithout tests, regressions in regex patterns or extraction logic could go unnoticed.\n\n## Key Functions That Need Tests\n\n1. **`parse_task_line()`** - Core task detection\n   - Test various checkbox formats: `- [ ]`, `- [x]`, `- [-]`, `- [?]`\n   - Test edge cases: malformed checkboxes, nested lists, content after checkbox\n\n2. **`extract_tags()`** - Tag extraction\n   - Test single tag: `#work`\n   - Test multiple tags: `#work #urgent`\n   - Test tags with numbers: `#project1`\n   - Test edge case: `#` alone should not match\n\n3. **`extract_due_date()` / `extract_created_date()` / `extract_completed_date()`**\n   - Test emoji format: `📅 2025-12-10`\n   - Test text format: `due: 2025-12-10`\n   - Test function format: `@due(2025-12-10)`\n   - Test invalid dates: `📅 not-a-date`\n\n4. **`extract_priority()`**\n   - Test emoji priorities: `⏫` (urgent), `🔼` (high), `🔽` (low), `⏬` (lowest)\n   - Test text priority: `priority: high`\n\n5. **`clean_content()`**\n   - Verify metadata is removed from content\n   - Verify actual task text is preserved\n   - Test that cleaning doesn't remove too much\n\n6. **`is_sub_item()` / `parse_sub_item()`**\n   - Test indentation detection\n   - Test nested sub-items\n\n## Suggested Test Structure\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    mod parse_task_line {\n        use super::*;\n        \n        #[test]\n        fn test_unchecked_task() { ... }\n        \n        #[test]\n        fn test_completed_task() { ... }\n        \n        #[test]\n        fn test_cancelled_task() { ... }\n    }\n\n    mod metadata_extraction {\n        use super::*;\n        \n        #[test]\n        fn test_extract_single_tag() { ... }\n        \n        #[test]\n        fn test_extract_due_date_emoji() { ... }\n        // ... etc\n    }\n}\n```\n\n## Estimated Impact\n\n- ~200-300 lines of test code to add\n- Catches regressions in regex patterns\n- Documents expected behavior\n- Enables confident refactoring","status":"closed","priority":3,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:47:47.097212677-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T10:59:45.270192774-06:00","closed_at":"2026-01-21T10:59:45.270192774-06:00","close_reason":"Closed","labels":["simplification-opportunity","testing"]}
{"id":"markdown-todo-extractor-98n","title":"Note outline/structure capability","description":"Parse and expose markdown document structure. Methods: get_outline(note) for heading hierarchy, get_section(note, heading) to extract content under headings, search_by_heading(pattern).","design":"# Implementation Plan: Note Outline/Structure Capability\n\n## Overview\nAdd `OutlineCapability` with three operations for extracting and querying markdown document structure:\n1. **get_outline** - Extract heading hierarchy from a file\n2. **get_section** - Extract content under a specific heading\n3. **search_headings** - Find headings matching a pattern across files\n\n## Architecture Approach\n\n**Pattern**: Follow existing capability architecture (see TagCapability with 3 operations)\n\n**Core Components**:\n- `src/outline_extractor.rs` - Parsing logic and data structures\n- `src/capabilities/outline.rs` - Capability, operations, request/response structs\n- Update `src/capabilities/mod.rs` - Register capability and operations\n\n## Data Structures\n\n```rust\n// Core heading representation (supports both flat and hierarchical)\nstruct Heading {\n    title: String,\n    level: u8,              // 1-6 for # to ######\n    line_number: usize,\n    children: Vec\u003cHeading\u003e,  // Empty for flat mode\n}\n\n// Section with content\nstruct Section {\n    heading: Heading,\n    content: String,\n    start_line: usize,\n    end_line: usize,\n}\n\n// Search result\nstruct HeadingMatch {\n    heading: Heading,\n    file_path: String,\n    file_name: String,\n}\n```\n\n## Implementation Strategy\n\n**Parsing**: Use regex (consistent with existing TaskExtractor/TagExtractor)\n- Pattern: `^(#{1,6})\\s+(.+?)(?:\\s*\\{#[^}]*\\})?\\s*$`\n- Handles Obsidian heading IDs: `## Title {#custom-id}`\n- No new dependencies needed\n\n**Why regex over parser library**:\n- Consistent with codebase patterns\n- Heading detection is simple enough\n- Avoids dependency bloat\n- Can upgrade later if needed\n\n## Operation Details\n\n### 1. get_outline\n- **Input**: file_path, flat (bool)\n- **Output**: List of headings (hierarchical or flat)\n- **Algorithm**: Parse headings, optionally build tree structure using level-based stack\n\n### 2. get_section\n- **Input**: file_path, heading (title), include_subsections (bool)\n- **Output**: Section content between heading and next same/higher level\n- **Edge case**: Multiple headings with same title → return first + list others\n\n### 3. search_headings\n- **Input**: pattern (substring), subpath, min_level, max_level, limit\n- **Output**: Matching headings across files\n- **Pattern matching**: Case-insensitive substring (not regex for security)\n- **Parallelization**: Use rayon for multi-file processing\n\n## Key Implementation Details\n\n**Security** (pattern from FileCapability):\n- Canonicalize paths\n- Validate within base directory\n- Restrict to .md files\n- Respect path exclusions from config\n\n**Edge Cases**:\n- Headings in code blocks → Must ignore (check for ``` markers)\n- Empty files → Return empty list\n- Duplicate titles → Return all with disambiguation\n- Section at EOF → Handle correctly\n- Unicode in headings → Full support\n\n**Performance**:\n- No caching initially (files change frequently)\n- Parallel processing for multi-file operations\n- O(n) parsing where n = lines\n\n## File Changes\n\n**New files**:\n- `src/outline_extractor.rs` (~300-400 lines)\n- `src/capabilities/outline.rs` (~500-600 lines)\n\n**Modified files**:\n- `src/capabilities/mod.rs` - Register capability and operations\n\n**No changes needed**:\n- `src/main.rs` - Auto-registration via create_operations()\n- `Cargo.toml` - No new dependencies\n\n## Testing Strategy\n\n**Unit tests** in outline_extractor.rs:\n- Heading pattern matching (ATX style, with IDs, malformed)\n- Hierarchy building (simple, complex, flat)\n- Section extraction (with/without subsections, at EOF)\n- Search filtering (case-insensitive, level constraints)\n\n**Integration tests**:\n- Create fixtures in tests/fixtures/ with various heading structures\n- Test all operations via CLI, HTTP, and MCP interfaces\n\n**Critical edge cases**:\n- Headings in code blocks (must ignore)\n- Duplicate heading titles\n- Malformed headings (#NoSpace, ####### too many)\n- Unicode support\n\n## Implementation Sequence\n\n1. **Core extractor**: Create outline_extractor.rs with data structures and parsing\n2. **Hierarchy building**: Implement tree construction algorithm\n3. **Section extraction**: Implement boundary detection and content extraction\n4. **Capability integration**: Create capabilities/outline.rs with operations\n5. **Registration**: Update capabilities/mod.rs registry\n6. **Testing**: Unit and integration tests\n\n## Verification\n\nAfter implementation:\n1. **CLI test**: `cargo run -- outline /vault/path note.md`\n2. **HTTP test**: `POST /api/outline {\"file_path\": \"note.md\"}`\n3. **MCP test**: Call tools via MCP server\n4. **Unit tests**: `cargo test outline`\n5. **Performance**: Test on large vault with complex hierarchies\n\n## Critical Files\n\n- `src/outline_extractor.rs` (new) - Core parsing logic\n- `src/capabilities/outline.rs` (new) - Operations and capability\n- `src/capabilities/mod.rs` (modify) - Registration\n- `src/capabilities/files.rs` (reference) - Security pattern at lines 133-148\n- `src/extractor.rs` (reference) - Regex compilation pattern","status":"open","priority":2,"issue_type":"feature","owner":"jeff@jeffutter.com","created_at":"2026-01-21T20:52:03.839876249-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T23:17:56.364126074-06:00","labels":["planned"]}
{"id":"markdown-todo-extractor-afx","title":"Links and Backlinks capability","description":"Implement capability to extract [[wikilinks]], get backlinks for a list of notes, and generate link graph. Methods: search_links(), get_backlinks(notes[]), get_link_graph().","design":"# Links and Backlinks Capability - Design Document\n\n## Overview\n\nThis capability will extract [[wikilinks]] from markdown files, compute backlinks (which notes reference a given note), and generate a link graph for understanding note relationships.\n\n## Data Structures\n\n### LinkInfo\nRepresents a wikilink found in a markdown file:\n- `target`: String - The target note name (without brackets)\n- `display_text`: Option\u003cString\u003e - Custom display text if using [[target|display]]\n- `source_file`: String - File containing this link\n- `source_file_name`: String - Just the filename\n- `line_number`: usize - Line where link appears\n\n### BacklinkInfo\nRepresents a note that links to a target note:\n- `source_file`: String - File path that contains the link\n- `source_file_name`: String - Just the filename\n- `link_count`: usize - How many times source links to target\n- `links`: Vec\u003cLinkInfo\u003e - Individual link occurrences\n\n### LinkGraphNode\nRepresents a node in the link graph:\n- `file_path`: String\n- `file_name`: String\n- `outgoing_links`: Vec\u003cString\u003e - Notes this file links to\n- `incoming_links`: Vec\u003cString\u003e - Notes that link to this file\n- `outgoing_count`: usize\n- `incoming_count`: usize\n\n### LinkGraphEdge\nRepresents an edge in the graph:\n- `source`: String - Source file path\n- `target`: String - Target note name\n- `weight`: usize - Number of links from source to target\n\n## Operations\n\n### 1. search_links (search for wikilinks)\n**Purpose**: Extract all wikilinks from markdown files with filtering options.\n\n**Request (SearchLinksRequest)**:\n- `path`: Option\u003cPathBuf\u003e - CLI path override (skipped in HTTP/MCP)\n- `subpath`: Option\u003cString\u003e - Subpath within vault to search\n- `target`: Option\u003cString\u003e - Filter to links pointing to specific note\n- `source_file`: Option\u003cString\u003e - Filter to links from specific file\n- `limit`: Option\u003cusize\u003e - Max results to return\n\n**Response (SearchLinksResponse)**:\n- `links`: Vec\u003cLinkInfo\u003e\n- `total_count`: usize\n- `truncated`: bool\n\n**CLI**: `links \u003cpath\u003e [--target \u003cnote\u003e] [--source \u003cfile\u003e] [--limit \u003cn\u003e]`\n**HTTP**: `POST /api/links`\n\n### 2. get_backlinks\n**Purpose**: Find all notes that link to specified target notes.\n\n**Request (GetBacklinksRequest)**:\n- `path`: Option\u003cPathBuf\u003e - CLI path override\n- `targets`: Vec\u003cString\u003e - Note names to find backlinks for\n- `subpath`: Option\u003cString\u003e - Subpath within vault to search\n- `include_link_details`: Option\u003cbool\u003e - Include individual link occurrences\n\n**Response (GetBacklinksResponse)**:\n- `backlinks`: HashMap\u003cString, Vec\u003cBacklinkInfo\u003e\u003e - Map from target to backlinks\n- `total_files_scanned`: usize\n\n**CLI**: `backlinks \u003cpath\u003e --targets \u003cnote1,note2,...\u003e [--include-details]`\n**HTTP**: `POST /api/links/backlinks`\n\n### 3. get_link_graph\n**Purpose**: Generate a graph representation of all note links.\n\n**Request (GetLinkGraphRequest)**:\n- `path`: Option\u003cPathBuf\u003e - CLI path override\n- `subpath`: Option\u003cString\u003e - Subpath within vault\n- `include_orphans`: Option\u003cbool\u003e - Include notes with no links (default: false)\n- `max_depth`: Option\u003cusize\u003e - Max traversal depth from root (if specified)\n- `root`: Option\u003cString\u003e - Start from specific note (filters graph)\n\n**Response (GetLinkGraphResponse)**:\n- `nodes`: Vec\u003cLinkGraphNode\u003e\n- `edges`: Vec\u003cLinkGraphEdge\u003e\n- `total_nodes`: usize\n- `total_edges`: usize\n- `orphan_count`: usize\n\n**CLI**: `link-graph \u003cpath\u003e [--include-orphans] [--root \u003cnote\u003e] [--max-depth \u003cn\u003e]`\n**HTTP**: `POST /api/links/graph`\n\n## Implementation Components\n\n### LinkExtractor (new file: src/link_extractor.rs)\nCore extraction logic, following TagExtractor pattern:\n\n```rust\npub struct LinkExtractor {\n    config: Arc\u003cConfig\u003e,\n    wikilink_pattern: Regex,  // \\[\\[([^\\]|]+)(?:\\|([^\\]]+))?\\]\\]\n}\n```\n\nMethods:\n- `extract_links_from_file(path) -\u003e Result\u003cVec\u003cLinkInfo\u003e\u003e`\n- `extract_links_from_content(content, path) -\u003e Vec\u003cLinkInfo\u003e`\n- `extract_links(path) -\u003e Result\u003cVec\u003cLinkInfo\u003e\u003e` (dir traversal)\n- `find_backlinks(path, targets) -\u003e Result\u003cHashMap\u003cString, Vec\u003cBacklinkInfo\u003e\u003e\u003e`\n- `build_link_graph(path) -\u003e Result\u003c(Vec\u003cLinkGraphNode\u003e, Vec\u003cLinkGraphEdge\u003e)\u003e`\n\n### LinkCapability (new file: src/capabilities/links.rs)\nFollowing existing capability pattern:\n\n- Operation metadata modules: `search_links`, `get_backlinks`, `get_link_graph`\n- Request/Response structs with JsonSchema, clap::Parser derives\n- LinkCapability struct with async methods\n- Operation structs implementing `crate::operation::Operation` trait\n\n### Integration Points\n\n1. **CapabilityRegistry** (src/capabilities/mod.rs):\n   - Add `LinkCapability` field\n   - Add `links()` getter method\n   - Add operations to `create_operations()`\n\n2. **MCP Server** (src/mcp.rs):\n   - Add three new `#[tool]` methods delegating to LinkCapability\n   - Update ServerInfo instructions\n\n## Wikilink Parsing Rules\n\nSupport standard Obsidian wikilink formats:\n- `[[Note Name]]` - Basic link\n- `[[Note Name|Display Text]]` - Link with custom display\n- `[[folder/Note Name]]` - Link with path\n- `[[Note Name#Heading]]` - Link to heading (extract both note and heading)\n- `[[Note Name#^block-id]]` - Link to block (extract both note and block)\n\nThe regex pattern will be:\n```regex\n\\[\\[([^\\]|#^]+)(?:#([^\\]|^]+))?(?:\\^([^\\]|]+))?(?:\\|([^\\]]+))?\\]\\]\n```\n\n## Testing Strategy\n\nUnit tests in link_extractor.rs:\n- Wikilink pattern parsing (basic, with display, with path)\n- Heading and block reference extraction\n- Multi-link extraction from single line\n- Edge cases (empty content, malformed links)\n\nUnit tests in capabilities/links.rs:\n- search_links filtering by target/source\n- get_backlinks single and multiple targets\n- get_link_graph node/edge generation\n- Orphan node handling\n\nIntegration tests:\n- Full vault scanning with rayon parallelization\n- Path exclusion configuration\n- Subpath filtering\n\n## Files to Create/Modify\n\n**New Files:**\n1. `src/link_extractor.rs` - Core link extraction logic\n2. `src/capabilities/links.rs` - Capability and operations\n\n**Modified Files:**\n1. `src/capabilities/mod.rs` - Register LinkCapability\n2. `src/mcp.rs` - Add MCP tool handlers\n3. `src/main.rs` - (No changes needed - automatic via registry)\n4. `src/lib.rs` or equivalent module declaration - Export link_extractor\n\n## Implementation Order\n\n1. Create `src/link_extractor.rs` with LinkInfo struct and basic extraction\n2. Create `src/capabilities/links.rs` with SearchLinksOperation\n3. Register in CapabilityRegistry, add MCP handler\n4. Implement backlinks functionality\n5. Implement link graph functionality\n6. Add comprehensive tests\n\n## Considerations\n\n- **Performance**: Use rayon for parallel file scanning (following TagExtractor pattern)\n- **Memory**: For large vaults, link graph could be large - consider streaming/pagination\n- **Link resolution**: Just extract raw link targets, don't resolve to actual files (simpler, more flexible)\n- **Case sensitivity**: Links should preserve original case but matching can be case-insensitive","status":"open","priority":2,"issue_type":"feature","owner":"jeff@jeffutter.com","created_at":"2026-01-21T20:52:02.753910016-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T22:42:56.735836032-06:00","labels":["planned"]}
{"id":"markdown-todo-extractor-b68","title":"Implement CLI automatic registration for tags operations","description":"Apply CLI automatic registration pattern to 3 tag operations (extract_tags, list_tags, search_by_tags). Follow the same pattern used for tasks: add Parser derives to request structs, implement CliOperation for operation structs, update create_cli_operations() in capabilities/mod.rs.","status":"closed","priority":2,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T19:35:09.492684048-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T20:35:58.007247413-06:00","closed_at":"2026-01-20T20:35:58.007247413-06:00","close_reason":"Closed","comments":[{"id":1,"issue_id":"markdown-todo-extractor-b68","author":"Jeffery Utter","text":"Implementation Guide - Tags Operations\n\nFollow the CLI automatic registration pattern documented in CLAUDE.md section \"Adding CLI Automatic Registration for an Operation\".\n\nReference: src/capabilities/tasks.rs SearchTasksOperation for working example\n\nOperations to convert (3 total):\n1. ExtractTagsOperation (extract_tags) - CLI name: \"tags\"\n2. ListTagsOperation (list_tags) - CLI name: \"list-tags\"  \n3. SearchByTagsOperation (search_by_tags) - CLI name: \"search-by-tags\"\n\nFor EACH operation:\n1. Add Parser derive to request struct (ExtractTagsRequest, ListTagsRequest, SearchByTagsRequest)\n   - Import: use clap::{CommandFactory, FromArgMatches, Parser};\n   - Add Parser to #[derive(...)]\n   - Add #[command(name = \"...\", about = \"...\")] \n   - Add path field: path: Option\u003cPathBuf\u003e with proper annotations\n   - Add #[arg(...)] to all existing fields\n\n2. Implement CliOperation trait for operation struct\n   - command_name() - use existing CLI_NAME constant\n   - get_command() - call RequestStruct::command()\n   - execute_from_args() - parse args, handle path, call capability method, return JSON\n\n3. Register in create_cli_operations() in src/capabilities/mod.rs\n   - Uncomment the Arc::new(...) lines for tag operations\n\n4. Update src/cli.rs - remove manual routing\n   - Remove Tags/ListTags/SearchByTags from Commands enum\n   - Remove command structs (if any separate from request structs)\n   - Remove match arms from run_cli()\n   - Update the check at top of run_cli() to include these commands\n\nTesting:\ncargo run -- tags /tmp/test-vault\ncargo run -- list-tags /tmp/test-vault --min-count 2\ncargo run -- search-by-tags /tmp/test-vault --tags work","created_at":"2026-01-21T01:41:36Z"}]}
{"id":"markdown-todo-extractor-cpb","title":"Remove unused Capability trait or document its purpose","description":"## Problem\n\nThe `Capability` trait in `src/capabilities/mod.rs` defines methods that are **never called**:\n\n```rust\n#[allow(dead_code)]\npub trait Capability: Send + Sync + 'static {\n    fn id(\u0026self) -\u003e \u0026'static str;\n    fn description(\u0026self) -\u003e \u0026'static str;\n}\n```\n\nEach capability implements this trait:\n- `TaskCapability::id()` returns \"task\"\n- `TagCapability::id()` returns \"tags\"\n- `FileCapability::id()` returns \"files\"\n\nBut these methods are never used in the codebase:\n- Not used for routing\n- Not used for logging\n- Not used for capability discovery\n\nThe `#[allow(dead_code)]` attribute confirms this is known unused code.\n\n## Proposed Solutions\n\n**Option A: Remove the trait entirely**\n- If it serves no purpose, remove it\n- Capabilities don't need a shared trait if they're not used polymorphically\n\n**Option B: Use it for capability discovery**\n- Create a method to list all capabilities by id/description\n- Could be useful for MCP introspection or debugging\n\n**Option C: Document intended future use**\n- Add TODO comment explaining planned functionality\n- Or create a ticket for the feature that would use it\n\n## Locations\n\n- `src/capabilities/mod.rs` lines 19-25 (trait definition)\n- `src/capabilities/tasks.rs` (impl block)\n- `src/capabilities/tags.rs` (impl block)\n- `src/capabilities/files.rs` (impl block)\n\n## Estimated Impact\n\n- ~20 lines of unused trait code removed\n- Eliminates `#[allow(dead_code)]` on trait\n- Cleaner capability definitions","status":"closed","priority":4,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:48:49.786524936-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T11:06:11.977947391-06:00","closed_at":"2026-01-21T11:06:11.977951478-06:00","labels":["simplification-opportunity"]}
{"id":"markdown-todo-extractor-i6y","title":"Merge HttpOperation and CliOperation traits","description":"## Problem\n\nThere are two separate operation traits that each operation must implement:\n\n1. **HttpOperation** (http_router.rs lines 15-28):\n   - `path(\u0026self) -\u003e \u0026'static str`\n   - `description(\u0026self) -\u003e \u0026'static str`\n   - `execute_json(\u0026self, json: Value) -\u003e Result\u003cValue, ErrorData\u003e`\n\n2. **CliOperation** (cli_router.rs lines 5-25):\n   - `command_name(\u0026self) -\u003e \u0026'static str`\n   - `get_command(\u0026self) -\u003e clap::Command`\n   - `execute_from_args(\u0026self, matches: \u0026ArgMatches, registry: \u0026CapabilityRegistry) -\u003e Result\u003cString, ...\u003e`\n\nBoth traits require similar metadata (`path`/`command_name`, `description`) and result in:\n- Every operation implementing both traits (duplicate method signatures)\n- Two separate registration lists in `CapabilityRegistry`:\n  - `create_http_operations()` (mod.rs lines 106-118)\n  - `create_cli_operations()` (mod.rs lines 124-136)\n\n## Proposed Solution\n\nCreate a unified `Operation` trait:\n\n```rust\npub trait Operation: Send + Sync + 'static {\n    /// Unique identifier for the operation (used as HTTP path and CLI command name)\n    fn name(\u0026self) -\u003e \u0026'static str;\n    \n    /// Human-readable description\n    fn description(\u0026self) -\u003e \u0026'static str;\n    \n    /// Get the clap Command for CLI parsing\n    fn cli_command(\u0026self) -\u003e clap::Command;\n    \n    /// Execute with JSON input (for HTTP/MCP)\n    async fn execute_json(\u0026self, json: Value) -\u003e Result\u003cValue, ErrorData\u003e;\n    \n    /// Execute from CLI arguments\n    async fn execute_cli(\u0026self, matches: \u0026ArgMatches) -\u003e Result\u003cString, Box\u003cdyn Error\u003e\u003e;\n}\n```\n\nThen have a single registration:\n```rust\npub fn operations(\u0026self) -\u003e Vec\u003cArc\u003cdyn Operation\u003e\u003e {\n    vec![\n        Arc::new(SearchTasksOperation::new(self.tasks())),\n        Arc::new(ListTagsOperation::new(self.tags())),\n        // ... etc\n    ]\n}\n```\n\nAnd the routers can use the same list:\n```rust\n// HTTP router\nfor op in registry.operations() {\n    router = router.route(\u0026format!(\"/{}\", op.name()), post(/* ... */));\n}\n\n// CLI router\nfor op in registry.operations() {\n    let cmd = op.cli_command().name(op.name());\n    app = app.subcommand(cmd);\n}\n```\n\n## Estimated Impact\n\n- ~80 lines of duplicate trait implementations eliminated\n- Single source of truth for operation registration\n- Simplified architecture (one trait instead of two)\n\n## Considerations\n\n- Some operations might be HTTP-only or CLI-only in the future. Could use default implementations that return \"not supported\" error.\n- The current separation was intentional but the duplication cost outweighs the flexibility benefit at current scale.\n\n## Dependencies\n\n- Should be done after JSON serialization wrapper and CLI path handling consolidation","status":"closed","priority":2,"issue_type":"task","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:47:14.921767074-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T06:04:20.221270645-06:00","closed_at":"2026-01-21T06:04:20.221270645-06:00","close_reason":"Closed","labels":["simplification-opportunity"],"dependencies":[{"issue_id":"markdown-todo-extractor-i6y","depends_on_id":"markdown-todo-extractor-7i7","type":"blocks","created_at":"2026-01-20T22:47:46.718002424-06:00","created_by":"Jeffery Utter"},{"issue_id":"markdown-todo-extractor-i6y","depends_on_id":"markdown-todo-extractor-2ko","type":"blocks","created_at":"2026-01-20T22:47:46.747670222-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-ie0","title":"Extract boilerplate error handling helpers","description":"## Problem\n\nError handling is verbose and repetitive across the codebase. The same `ErrorData` construction pattern appears **20+ times**:\n\n```rust\n.map_err(|e| ErrorData {\n    code: ErrorCode(-32603),\n    message: Cow::from(format!(\"Failed to extract tasks: {}\", e)),\n    data: None,\n})?\n```\n\n## Locations\n\n- `src/capabilities/tasks.rs` (lines ~116-119, and similar patterns)\n- `src/capabilities/tags.rs` (multiple locations in operation impls)\n- `src/capabilities/files.rs` (multiple locations in operation impls)\n\n## Proposed Solution\n\nCreate helper functions in a shared module (e.g., `src/error.rs` or in `capabilities/mod.rs`):\n\n```rust\npub fn json_error(code: i32, msg: impl Into\u003cString\u003e) -\u003e ErrorData {\n    ErrorData {\n        code: ErrorCode(code),\n        message: Cow::from(msg.into()),\n        data: None,\n    }\n}\n\npub fn internal_error(msg: impl Into\u003cString\u003e) -\u003e ErrorData {\n    json_error(-32603, msg)\n}\n\npub fn invalid_params(msg: impl Into\u003cString\u003e) -\u003e ErrorData {\n    json_error(-32602, msg)\n}\n```\n\n## Estimated Impact\n\n- ~100 lines of duplicated error handling code eliminated\n- Single place to update error formatting\n- Better error categorization (use proper JSON-RPC error codes)\n\n## Research Notes\n\nAll internal errors currently use error code `-32603` (internal error). With proper helpers, we could distinguish:\n- `-32602` for invalid input\n- `-32603` for server errors\n- Custom codes for specific error types if needed","status":"closed","priority":2,"issue_type":"task","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:47:13.943209847-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T05:44:49.500541622-06:00","closed_at":"2026-01-21T05:44:49.500541622-06:00","close_reason":"Closed","labels":["simplification-opportunity"]}
{"id":"markdown-todo-extractor-j56","title":"Simplify CapabilityRegistry lazy initialization","description":"## Problem\n\nThe `CapabilityRegistry` uses `OnceLock` for lazy initialization of capabilities:\n\n```rust\npub struct CapabilityRegistry {\n    base_path: PathBuf,\n    config: Arc\u003cConfig\u003e,\n    task_capability: OnceLock\u003cArc\u003cTaskCapability\u003e\u003e,\n    tag_capability: OnceLock\u003cArc\u003cTagCapability\u003e\u003e,\n    file_capability: OnceLock\u003cArc\u003cFileCapability\u003e\u003e,\n}\n\npub fn tasks(\u0026self) -\u003e Arc\u003cTaskCapability\u003e {\n    self.task_capability\n        .get_or_init(|| {\n            Arc::new(TaskCapability::new(\n                self.base_path.clone(),\n                Arc::clone(\u0026self.config),\n            ))\n        })\n        .clone()\n}\n```\n\nThis pattern:\n1. Adds complexity with `OnceLock` + `Arc` + `.clone()` on every access\n2. Is designed for dynamic/lazy loading but capabilities are always loaded\n3. Requires initialization closure to clone `base_path` and `Arc` each time\n\n## Proposed Solution\n\nIf dynamic capability loading is not a requirement, simplify to eager initialization:\n\n```rust\npub struct CapabilityRegistry {\n    tasks: Arc\u003cTaskCapability\u003e,\n    tags: Arc\u003cTagCapability\u003e,\n    files: Arc\u003cFileCapability\u003e,\n}\n\nimpl CapabilityRegistry {\n    pub fn new(base_path: PathBuf) -\u003e Self {\n        let config = Arc::new(Config::load_from_base_path(\u0026base_path));\n        Self {\n            tasks: Arc::new(TaskCapability::new(base_path.clone(), Arc::clone(\u0026config))),\n            tags: Arc::new(TagCapability::new(base_path.clone(), Arc::clone(\u0026config))),\n            files: Arc::new(FileCapability::new(base_path, config)),\n        }\n    }\n\n    pub fn tasks(\u0026self) -\u003e Arc\u003cTaskCapability\u003e {\n        Arc::clone(\u0026self.tasks)\n    }\n    // ... similar for others\n}\n```\n\n## Trade-offs\n\n**Current approach benefits:**\n- Lazy initialization (only create capabilities when used)\n- Supports potential future dynamic capability loading\n\n**Proposed approach benefits:**\n- Simpler code (no OnceLock complexity)\n- No closure allocation per access\n- Fail-fast initialization (errors at startup vs first use)\n\n## Investigation Required\n\n1. Are there plans for dynamic capability loading?\n2. Is lazy initialization providing measurable benefit?\n3. Are any capabilities unused in typical workflows?\n\n## Estimated Impact\n\n- ~30 lines of simpler code\n- Removes `OnceLock` dependency\n- Clearer initialization flow","status":"closed","priority":4,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:48:19.582640382-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T11:09:14.905858314-06:00","closed_at":"2026-01-21T11:09:14.905861941-06:00","labels":["simplification-opportunity"]}
{"id":"markdown-todo-extractor-sdi","title":"Change file list output to visual rather than json","description":"Change the format of the file list output to an indented list.\n\nDirectory structure format:\n- One entry per line\n- Directories end with /\n- Children are indented 2 spaces relative to their parent\n- Files have no trailing slash\n\n## Example:\n\n```\nproject/\n  src/\n    main.py\n    utils.py\n  README.md\n```\n\n## Implementation Plan\n\n### 1. Create Visual Tree Formatter Function\n**Location**: `src/capabilities/files.rs`\n\nAdd a new function `format_tree_visual()` that converts a `FileTreeNode` to the indented visual format:\n\n```rust\nfn format_tree_visual(node: \u0026FileTreeNode, indent_level: usize) -\u003e String {\n    let mut output = String::new();\n    let indent = \"  \".repeat(indent_level);\n    \n    // Add current node\n    if node.is_directory {\n        output.push_str(\u0026format!(\"{}{}/\\n\", indent, node.name));\n    } else {\n        output.push_str(\u0026format!(\"{}{}\\n\", indent, node.name));\n    }\n    \n    // Recursively add children\n    for child in \u0026node.children {\n        output.push_str(\u0026format_tree_visual(child, indent_level + 1));\n    }\n    \n    output\n}\n```\n\n**Key implementation details**:\n- Use 2 spaces per indent level (not tabs)\n- Directories get a trailing `/`\n- Files have no trailing slash\n- Recursively process children with incremented indent level\n\n### 2. Add Visual Output Option to Response\n**Location**: `src/capabilities/files.rs`\n\nTwo approaches to consider:\n\n**Option A: Replace JSON response entirely**\n- Change `ListFilesResponse` to contain a `visual_tree: String` field\n- Remove or make optional the `root: FileTreeNode` field\n- CLI always outputs visual format\n- HTTP/MCP also outputs visual format\n\n**Option B: Add format parameter (more flexible)**\n- Add `format: Option\u003cString\u003e` to `ListFilesRequest` (values: \"json\", \"visual\")\n- Keep existing `FileTreeNode` structure\n- Add optional `visual_tree: Option\u003cString\u003e` to `ListFilesResponse`\n- Let caller choose format\n\n**Recommendation**: Start with Option A for simplicity. Can add Option B later if needed.\n\n### 3. Update CLI Output\n**Location**: `src/capabilities/files.rs:341-362` (ListFilesOperation CLI impl)\n\nChange the CLI operation to output the visual tree directly:\n\n```rust\nasync fn execute_from_args(...) -\u003e Result\u003cString, Box\u003cdyn std::error::Error\u003e\u003e {\n    // ... existing request parsing ...\n    \n    let response = /* ... get response ... */;\n    \n    // Output visual tree instead of JSON\n    Ok(response.visual_tree)\n}\n```\n\n**Current code**: Line 361 serializes to JSON\n**New code**: Return the visual tree string directly\n\n### 4. Update list_files Method\n**Location**: `src/capabilities/files.rs:121-177`\n\nAfter building the file tree with `build_file_tree()`:\n\n```rust\n// Build the file tree (existing code)\nlet (root, total_files, total_directories) = build_file_tree(...)?;\n\n// Generate visual representation\nlet visual_tree = format_tree_visual(\u0026root, 0);\n\nOk(ListFilesResponse {\n    visual_tree,\n    total_files,\n    total_directories,\n})\n```\n\n### 5. Update Response Struct\n**Location**: `src/capabilities/files.rs:56-62`\n\n```rust\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct ListFilesResponse {\n    pub visual_tree: String,\n    pub total_files: usize,\n    pub total_directories: usize,\n}\n```\n\n### 6. Handle Root Name Edge Case\nThe root directory name needs special handling:\n- If listing vault root, show the vault's directory name\n- If listing a subpath, show that subpath's name\n- Ensure consistent behavior with how `build_file_tree()` names the root\n\n### 7. Testing Strategy\n\nManual testing commands:\n```bash\n# Test basic listing\ncargo run -- list-files /path/to/vault\n\n# Test subpath\ncargo run -- list-files /path/to/vault --path \"subfolder\"\n\n# Test max depth\ncargo run -- list-files /path/to/vault --max-depth 2\n```\n\nExpected output format:\n```\nvault/\n  folder1/\n    file1.md\n    file2.md\n  folder2/\n    nested/\n      deep.md\n  root-file.md\n```\n\n### 8. Files to Modify\n\n1. `src/capabilities/files.rs`:\n   - Add `format_tree_visual()` function (~20 lines)\n   - Update `ListFilesResponse` struct (1 line change)\n   - Update `list_files()` method (add 2 lines)\n   - Update CLI `execute_from_args()` (change line 361)\n\n### 9. Backward Compatibility Considerations\n\n**Breaking changes**:\n- HTTP/MCP clients expecting JSON structure will break\n- This is acceptable if no external clients exist yet\n\n**If backward compatibility needed**:\n- Use Option B (format parameter) instead\n- Default to \"visual\" for CLI\n- Keep \"json\" as default for HTTP/MCP initially\n\n### 10. Edge Cases to Handle\n\n1. **Empty directories**: Still show with `/` suffix\n2. **Single file**: Should not have indent (at root level)\n3. **Deep nesting**: Verify indent math is correct\n4. **Special characters in names**: Ensure proper display\n5. **Excluded paths**: Already handled by `build_file_tree()`\n\n### Estimated Complexity\n- Low complexity change\n- ~30 lines of new code\n- ~5 lines of modifications to existing code\n- Main work is the formatter function and testing","status":"closed","priority":2,"issue_type":"feature","owner":"jeff@jeffutter.com","created_at":"2026-01-20T12:28:37.799886872-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T22:02:51.313707474-06:00","closed_at":"2026-01-20T22:02:51.313707474-06:00","close_reason":"Closed"}
{"id":"markdown-todo-extractor-thw","title":"Integrate tools_handler endpoint into capability registry","description":"## Summary\n\nCurrently `tools_handler` in `main.rs:25-51` hardcodes the available tools (only `search_tasks` and `extract_tags`), while the capability registry already has a `create_operations()` method that returns all registered operations. This creates maintenance burden and inconsistency - new operations must be manually added to both places.\n\n## Current State\n\n**Hardcoded tools_handler (main.rs:25-51):**\n```rust\nasync fn tools_handler() -\u003e impl axum::response::IntoResponse {\n    let search_tasks_schema = schema_for!(SearchTasksRequest);\n    let extract_tags_schema = schema_for!(ExtractTagsRequest);\n    // ... hardcoded list of 2 tools\n}\n```\n\n**Dynamic operation registry (capabilities/mod.rs:57-69):**\n```rust\npub fn create_operations(\u0026self) -\u003e Vec\u003cArc\u003cdyn Operation\u003e\u003e {\n    vec![\n        // 6 operations registered here\n    ]\n}\n```\n\n## Proposed Solution\n\n### 1. Extend the Operation trait (operation.rs)\n\nAdd a new method to provide JSON Schema for the input:\n\n```rust\n/// Get the JSON Schema for this operation's input\n///\n/// Returns the schema as a serde_json::Value for easy serialization.\n/// Implementations should use schemars::schema_for! on their request type.\nfn input_schema(\u0026self) -\u003e serde_json::Value;\n```\n\n### 2. Implement input_schema() for each operation\n\nEach operation already has a request type that derives `JsonSchema`. Add implementations like:\n\n```rust\nfn input_schema(\u0026self) -\u003e serde_json::Value {\n    serde_json::to_value(schema_for!(SearchTasksRequest)).unwrap()\n}\n```\n\n### 3. Update tools_handler to use the registry\n\n```rust\nasync fn tools_handler(\n    State(registry): State\u003cArc\u003cCapabilityRegistry\u003e\u003e,\n) -\u003e impl axum::response::IntoResponse {\n    let tools: Vec\u003c_\u003e = registry.create_operations()\n        .into_iter()\n        .map(|op| json!({\n            \"name\": op.name(),\n            \"description\": op.description(),\n            \"input_schema\": op.input_schema()\n        }))\n        .collect();\n\n    Json(json!({ \"tools\": tools }))\n}\n```\n\n### 4. Update router to pass state to tools_handler\n\nThe handler needs access to the capability registry via Axum state extraction.\n\n## Files to Modify\n\n1. `src/operation.rs` - Add `input_schema()` method to Operation trait\n2. `src/capabilities/tasks.rs` - Implement for SearchTasksOperation\n3. `src/capabilities/tags.rs` - Implement for ExtractTagsOperation, ListTagsOperation, SearchByTagsOperation\n4. `src/capabilities/files.rs` - Implement for ListFilesOperation, ReadFileOperation\n5. `src/main.rs` - Update tools_handler to use registry dynamically\n\n## Benefits\n\n- Single source of truth for operation metadata\n- New operations automatically appear in /tools endpoint\n- Consistent with existing HTTP and CLI registration patterns\n- No manual synchronization required\n\n## Testing\n\n1. Start HTTP server: `cargo run -- serve http --port 3000 /path/to/vault`\n2. Verify all 6 operations appear at `GET /tools`\n3. Confirm schemas match expected request structures","status":"closed","priority":2,"issue_type":"feature","assignee":"claude","owner":"jeff@jeffutter.com","created_at":"2026-01-21T11:40:33.743003156-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T12:39:43.891447188-06:00","closed_at":"2026-01-21T12:39:43.891447188-06:00","close_reason":"Closed"}
{"id":"markdown-todo-extractor-tx9","title":"Create Additional Tools","description":"# Create Additional Tools\n\nThis epic encompasses the creation of additional LLM tools to enhance the knowledge base interaction capabilities. The tools extend the existing RAG search functionality to provide more granular access to vault contents.\n\n## Overview\n\nThis epic adds four complementary tools that give the LLM more ways to explore and access the Obsidian vault:\n\n| Tool | Purpose | Child Ticket |\n|------|---------|--------------|\n| **read_file** | Read the full content of a specific file | tx9.2 |\n| **list_files** | Browse the directory structure | tx9.3 |\n| **list_tags** | Discover available tags with statistics | tx9.4 |\n| **search_by_tags** | Find files matching specific tags | tx9.5 |\n\n## Implementation Plan\n\n### Architecture Approach\n\nAll four tools follow the existing patterns established in `src/mcp.rs`:\n\n1. **MCP Tool Registration**: Use the `#[tool]` macro from `rmcp` to register each tool with the `TaskSearchService`\n2. **REST API Endpoints**: Add corresponding HTTP handlers in `main.rs` for the HTTP MCP server mode\n3. **Shared Extractors**: Reuse and extend existing extractors (`TaskExtractor`, `TagExtractor`) where applicable\n4. **Configuration Integration**: All tools should respect the existing `Config` path exclusion patterns\n\n### File Organization\n\nThe implementation will modify existing modules following the current structure:\n\n| File | New Additions |\n|------|---------------|\n| `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs` | 4 new tool methods, 8 new request/response structs |\n| `/home/jeffutter/src/markdown-todo-extractor/src/main.rs` | 4 new HTTP handler pairs, 4 new routes, updated tools_handler |\n| `/home/jeffutter/src/markdown-todo-extractor/src/tag_extractor.rs` | `TagCount` struct, `TaggedFile` struct, `extract_tags_with_counts()`, `search_by_tags()`, Config integration |\n| `/home/jeffutter/src/markdown-todo-extractor/src/cli.rs` | `SearchByTags` subcommand |\n| `/home/jeffutter/src/markdown-todo-extractor/Cargo.toml` | `tempfile` dev dependency |\n\n### Common Patterns\n\nAll tools should:\n\n1. **Accept paths relative to the base path** (vault root) with optional subpath parameter\n2. **Return JSON responses** with consistent error handling via `ErrorData` for MCP and HTTP status codes for REST\n3. **Support both MCP stdio and HTTP modes** with identical functionality\n4. **Include comprehensive tool descriptions** for LLM consumption via `#[tool(description = \"...\")]`\n5. **Use `JsonSchema` derive** for automatic schema generation\n6. **Respect `Config.exclude_paths`** for path exclusions (critical for tx9.3, tx9.4, tx9.5)\n\n### Shared Infrastructure Changes\n\nBefore implementing individual tools, these cross-cutting changes are required:\n\n#### 1. TagExtractor Config Integration (Required by tx9.4, tx9.5)\n\nThe current `TagExtractor` is a simple unit struct without configuration. It needs to be updated to:\n- Accept `Arc\u003cConfig\u003e` in constructor (matching `TaskExtractor` pattern)\n- Pass config to `collect_markdown_files()` function\n- Apply path exclusions during file collection\n\nThis change affects:\n- `src/tag_extractor.rs`: Add config field and update methods\n- `src/mcp.rs`: Update `TaskSearchService::new()` to pass config\n- `src/main.rs`: Update `AppState` initialization\n- `src/cli.rs`: Update Tags command to create TagExtractor with config\n\n#### 2. Dev Dependency Addition\n\nAdd `tempfile = \"3\"` to `Cargo.toml` for unit tests across all tools.\n\n### Execution Order\n\nThe tools can be implemented in parallel, but for optimized development with code reuse:\n\n```\nPhase 1 (Independent - can run in parallel):\n  ├── tx9.2: read_file tool (standalone, no dependencies)\n  └── tx9.3: list_files tool (standalone, no dependencies)\n\nPhase 2 (After TagExtractor Config Integration):\n  ├── tx9.4: list_tags tool (extends TagExtractor with counting)\n  └── tx9.5: search_by_tags tool (extends TagExtractor with search)\n```\n\n**Recommended single-developer sequence:**\n1. **tx9.2 (read_file)** - Simplest tool, establishes the pattern\n2. **tx9.3 (list_files)** - Standalone, no extractor dependencies\n3. **TagExtractor Config Integration** - Shared infrastructure for tx9.4 and tx9.5\n4. **tx9.4 (list_tags)** - Adds counting to TagExtractor\n5. **tx9.5 (search_by_tags)** - Can reuse tag extraction logic from tx9.4\n\n### Security Considerations\n\nAll file-reading tools (tx9.2, tx9.3) must implement path traversal protection:\n\n```rust\n// Pattern for all file-accessing tools\nlet canonical_base = self.base_path.canonicalize()?;\nlet canonical_full = full_path.canonicalize()?;\n\nif !canonical_full.starts_with(\u0026canonical_base) {\n    return Err(ErrorData {\n        code: ErrorCode(-32602),\n        message: Cow::from(\"Invalid path: path must be within the vault\"),\n        data: None,\n    });\n}\n```\n\nFor tx9.2 (read_file), additionally restrict to `.md` files only.\n\n### Testing Strategy\n\nEach tool should include:\n\n1. **Unit tests** for core logic (in respective module)\n2. **Integration tests** for MCP tool invocation (if feasible)\n3. **Manual testing** with CLI and MCP stdio modes\n\nCommon test scenarios across all tools:\n- Happy path with valid inputs\n- Empty/missing inputs handled gracefully\n- Path exclusions respected (tx9.3, tx9.4, tx9.5)\n- Security validation (path traversal blocked in tx9.2, tx9.3)\n- Large vault handling (tx9.3 may need truncation)\n\n### API Endpoint Summary\n\nAfter all tools are implemented, the HTTP server will expose:\n\n| Endpoint | Tool | Method |\n|----------|------|--------|\n| `/api/tasks` | search_tasks | GET/POST |\n| `/api/tags` | extract_tags | GET/POST |\n| `/api/file` | read_file | GET/POST |\n| `/api/files` | list_files | GET/POST |\n| `/api/tags/list` | list_tags | GET/POST |\n| `/api/search_by_tags` | search_by_tags | GET/POST |\n\n### Success Criteria\n\n- [ ] All four tools implemented with MCP and HTTP interfaces\n- [ ] All tools respect path exclusion configuration\n- [ ] Security: Path traversal attacks blocked\n- [ ] Unit tests pass for all new functionality\n- [ ] `cargo build --release` succeeds\n- [ ] `cargo clippy` passes\n- [ ] `cargo fmt --check` passes\n- [ ] Manual testing confirms tools work in both stdio and HTTP modes\n\n### Child Ticket Summary\n\nEach child ticket has a detailed implementation plan:\n\n| Ticket | Status | Key Implementation Details |\n|--------|--------|---------------------------|\n| **tx9.2** | Planned | `ReadFileRequest`/`ReadFileResponse`, path validation, .md restriction |\n| **tx9.3** | Planned | `ListFilesRequest`/`ListFilesResponse`/`FileTreeNode`, hierarchical tree, size limiting |\n| **tx9.4** | Planned | `TagCount` struct, `extract_tags_with_counts()`, sort by frequency |\n| **tx9.5** | Planned | `TaggedFile` struct, `search_by_tags()`, AND/OR logic, CLI subcommand |\n\nSee individual child tickets for complete implementation details.","status":"closed","priority":2,"issue_type":"epic","owner":"jeff@jeffutter.com","created_at":"2026-01-19T18:43:26.983361979-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-19T22:35:17.868790617-06:00","closed_at":"2026-01-19T22:35:17.868790617-06:00","close_reason":"Closed","labels":["planned"]}
{"id":"markdown-todo-extractor-tx9.2","title":"Create File Read Tool","description":"# Create File Read Tool\n\nCreate a tool that returns the contents of a file at a given path.\n\n## Requirements\n\n- The tool should return the contents of the file at a given path\n- See this for ideas: https://github.com/logancyang/obsidian-copilot/blob/master/src/tools/NoteTools.ts\n\n---\n\n## Implementation Plan\n\n### Overview\n\nThis tool will allow MCP clients to read the full contents of a markdown file from the Obsidian vault. The implementation follows the existing patterns in the codebase for MCP tools (see `search_tasks` and `extract_tags` in `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs`).\n\n### Design Decisions\n\n1. **Path Handling**: Accept a relative path (relative to the vault base path). This follows the pattern used by Obsidian Copilot's NoteTools.ts which requires paths relative to vault root.\n\n2. **Security**: Validate that the resolved path is within the base path to prevent path traversal attacks (e.g., `../../../etc/passwd`).\n\n3. **File Extension**: Only allow reading `.md` files to stay consistent with the tool's purpose as a markdown task extractor.\n\n4. **Response Format**: Return a structured response with:\n   - `content`: The file contents\n   - `file_path`: The resolved path (relative to vault)\n   - `file_name`: The file name only\n   - Optionally: metadata like modification time\n\n5. **Error Handling**: Follow existing patterns using `ErrorData` with appropriate error codes:\n   - Invalid path (path traversal attempt)\n   - File not found\n   - File not a markdown file\n   - Read error\n\n6. **No Chunking Initially**: Unlike Obsidian Copilot which chunks large files into 200-line segments, start with a simpler implementation that returns the full file. Chunking can be added later if needed for very large files.\n\n### Implementation Steps\n\n#### Step 1: Add Request/Response Types in `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs`\n\nAdd new structs after the existing `ExtractTagsResponse`:\n\n```rust\n/// Parameters for the read_file tool\n#[derive(Debug, Deserialize, JsonSchema)]\npub struct ReadFileRequest {\n    #[schemars(description = \"Path to the file relative to the vault root (e.g., 'Notes/my-note.md')\")]\n    pub path: String,\n}\n\n/// Response for the read_file tool\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct ReadFileResponse {\n    /// The full content of the file\n    pub content: String,\n    /// The file path relative to the vault root\n    pub file_path: String,\n    /// Just the file name\n    pub file_name: String,\n}\n```\n\n#### Step 2: Add the MCP Tool Method in `TaskSearchService`\n\nAdd a new method with the `#[tool]` attribute inside the `#[tool_router] impl TaskSearchService` block:\n\n```rust\n#[tool(\n    description = \"Read the full contents of a markdown file from the vault\"\n)]\nasync fn read_file(\n    \u0026self,\n    Parameters(request): Parameters\u003cReadFileRequest\u003e,\n) -\u003e Result\u003cJson\u003cReadFileResponse\u003e, ErrorData\u003e {\n    // 1. Construct the full path\n    let requested_path = PathBuf::from(\u0026request.path);\n    let full_path = self.base_path.join(\u0026requested_path);\n\n    // 2. Canonicalize paths for security check\n    let canonical_base = self.base_path.canonicalize().map_err(|e| ErrorData {\n        code: ErrorCode(-32603),\n        message: Cow::from(format!(\"Failed to resolve base path: {}\", e)),\n        data: None,\n    })?;\n\n    let canonical_full = full_path.canonicalize().map_err(|e| ErrorData {\n        code: ErrorCode(-32602), // Invalid params\n        message: Cow::from(format!(\"File not found: {}\", request.path)),\n        data: None,\n    })?;\n\n    // 3. Security: Ensure path is within base directory\n    if !canonical_full.starts_with(\u0026canonical_base) {\n        return Err(ErrorData {\n            code: ErrorCode(-32602),\n            message: Cow::from(\"Invalid path: path must be within the vault\"),\n            data: None,\n        });\n    }\n\n    // 4. Validate it's a markdown file\n    if canonical_full.extension().and_then(|s| s.to_str()) != Some(\"md\") {\n        return Err(ErrorData {\n            code: ErrorCode(-32602),\n            message: Cow::from(\"Invalid file type: only .md files can be read\"),\n            data: None,\n        });\n    }\n\n    // 5. Read the file content\n    let content = std::fs::read_to_string(\u0026canonical_full).map_err(|e| ErrorData {\n        code: ErrorCode(-32603),\n        message: Cow::from(format!(\"Failed to read file: {}\", e)),\n        data: None,\n    })?;\n\n    // 6. Get relative path for response\n    let relative_path = canonical_full\n        .strip_prefix(\u0026canonical_base)\n        .unwrap_or(\u0026canonical_full)\n        .to_string_lossy()\n        .to_string();\n\n    let file_name = canonical_full\n        .file_name()\n        .unwrap_or_default()\n        .to_string_lossy()\n        .to_string();\n\n    Ok(Json(ReadFileResponse {\n        content,\n        file_path: relative_path,\n        file_name,\n    }))\n}\n```\n\n#### Step 3: Add HTTP REST Endpoint (Optional but Recommended)\n\nFor consistency with existing endpoints, add REST handlers in `/home/jeffutter/src/markdown-todo-extractor/src/main.rs`:\n\n1. Add handler functions:\n\n```rust\n/// HTTP handler for reading a file (GET with query params)\nasync fn file_handler_get(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    query: axum::extract::Query\u003cmcp::ReadFileRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ReadFileResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    read_file_impl(state, query.0).await\n}\n\n/// HTTP handler for reading a file (POST with JSON body)\nasync fn file_handler_post(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    axum::Json(request): axum::Json\u003cmcp::ReadFileRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ReadFileResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    read_file_impl(state, request).await\n}\n\n/// Shared implementation for file reading\nasync fn read_file_impl(\n    state: AppState,\n    request: mcp::ReadFileRequest,\n) -\u003e Result\u003caxum::Json\u003cmcp::ReadFileResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    // Similar validation logic as the MCP handler\n    let requested_path = std::path::PathBuf::from(\u0026request.path);\n    let full_path = state.base_path.join(\u0026requested_path);\n\n    // Security: canonicalize and check path\n    let canonical_base = state.base_path.canonicalize().map_err(|e| {\n        (axum::http::StatusCode::INTERNAL_SERVER_ERROR, format!(\"Failed to resolve base path: {}\", e))\n    })?;\n\n    let canonical_full = full_path.canonicalize().map_err(|_| {\n        (axum::http::StatusCode::NOT_FOUND, format!(\"File not found: {}\", request.path))\n    })?;\n\n    if !canonical_full.starts_with(\u0026canonical_base) {\n        return Err((axum::http::StatusCode::BAD_REQUEST, \"Invalid path: path must be within the vault\".to_string()));\n    }\n\n    if canonical_full.extension().and_then(|s| s.to_str()) != Some(\"md\") {\n        return Err((axum::http::StatusCode::BAD_REQUEST, \"Invalid file type: only .md files can be read\".to_string()));\n    }\n\n    let content = std::fs::read_to_string(\u0026canonical_full).map_err(|e| {\n        (axum::http::StatusCode::INTERNAL_SERVER_ERROR, format!(\"Failed to read file: {}\", e))\n    })?;\n\n    let relative_path = canonical_full\n        .strip_prefix(\u0026canonical_base)\n        .unwrap_or(\u0026canonical_full)\n        .to_string_lossy()\n        .to_string();\n\n    let file_name = canonical_full\n        .file_name()\n        .unwrap_or_default()\n        .to_string_lossy()\n        .to_string();\n\n    Ok(axum::Json(mcp::ReadFileResponse {\n        content,\n        file_path: relative_path,\n        file_name,\n    }))\n}\n```\n\n2. Add route in router:\n\n```rust\n.route(\n    \"/api/file\",\n    axum::routing::get(file_handler_get).post(file_handler_post),\n)\n```\n\n3. Update `tools_handler` to include the new tool schema.\n\n4. Update console output messages.\n\n#### Step 4: Add Unit Tests\n\nAdd tests in a new `#[cfg(test)]` module in `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs` or create a separate test file:\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::fs;\n    use tempfile::TempDir;\n\n    #[tokio::test]\n    async fn test_read_file_success() {\n        let temp_dir = TempDir::new().unwrap();\n        let file_path = temp_dir.path().join(\"test.md\");\n        fs::write(\u0026file_path, \"# Test\\n\\nContent here\").unwrap();\n\n        let service = TaskSearchService::new(temp_dir.path().to_path_buf());\n        let request = ReadFileRequest {\n            path: \"test.md\".to_string(),\n        };\n\n        let result = service.read_file(Parameters(request)).await;\n        assert!(result.is_ok());\n\n        let response = result.unwrap().0;\n        assert_eq!(response.content, \"# Test\\n\\nContent here\");\n        assert_eq!(response.file_name, \"test.md\");\n    }\n\n    #[tokio::test]\n    async fn test_read_file_not_found() {\n        let temp_dir = TempDir::new().unwrap();\n        let service = TaskSearchService::new(temp_dir.path().to_path_buf());\n        let request = ReadFileRequest {\n            path: \"nonexistent.md\".to_string(),\n        };\n\n        let result = service.read_file(Parameters(request)).await;\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_read_file_path_traversal_blocked() {\n        let temp_dir = TempDir::new().unwrap();\n        let service = TaskSearchService::new(temp_dir.path().to_path_buf());\n        let request = ReadFileRequest {\n            path: \"../../../etc/passwd\".to_string(),\n        };\n\n        let result = service.read_file(Parameters(request)).await;\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_read_file_non_markdown_rejected() {\n        let temp_dir = TempDir::new().unwrap();\n        let file_path = temp_dir.path().join(\"test.txt\");\n        fs::write(\u0026file_path, \"Not markdown\").unwrap();\n\n        let service = TaskSearchService::new(temp_dir.path().to_path_buf());\n        let request = ReadFileRequest {\n            path: \"test.txt\".to_string(),\n        };\n\n        let result = service.read_file(Parameters(request)).await;\n        assert!(result.is_err());\n    }\n}\n```\n\nAdd `tempfile` as a dev dependency in `/home/jeffutter/src/markdown-todo-extractor/Cargo.toml`:\n\n```toml\n[dev-dependencies]\ntempfile = \"3\"\n```\n\n### Files to Modify\n\n| File | Changes |\n|------|---------|\n| `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs` | Add `ReadFileRequest`, `ReadFileResponse` structs and `read_file` tool method |\n| `/home/jeffutter/src/markdown-todo-extractor/src/main.rs` | Add HTTP handlers and route for `/api/file` endpoint |\n| `/home/jeffutter/src/markdown-todo-extractor/Cargo.toml` | Add `tempfile` dev dependency for tests |\n\n### Testing Strategy\n\n1. **Unit Tests**: Test the core logic with various scenarios (success, not found, path traversal, non-markdown)\n2. **Manual Testing**: Test with actual MCP client (Claude Desktop or similar)\n   ```bash\n   # Start the server\n   cargo run -- --mcp-stdio /path/to/vault\n   \n   # Or HTTP mode\n   cargo run -- --mcp-http /path/to/vault\n   ```\n\n### Future Enhancements (Out of Scope)\n\n- **Chunking**: For very large files, add optional `chunk_index` parameter similar to Obsidian Copilot\n- **Link Extraction**: Parse and return wiki-style links from the content\n- **Metadata Extraction**: Return YAML frontmatter separately parsed\n- **Line Range**: Allow reading specific line ranges from a file","status":"closed","priority":2,"issue_type":"feature","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-19T18:49:51.850252653-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-19T22:27:03.960128433-06:00","closed_at":"2026-01-19T22:27:03.960128433-06:00","close_reason":"Closed","labels":["planned"],"dependencies":[{"issue_id":"markdown-todo-extractor-tx9.2","depends_on_id":"markdown-todo-extractor-tx9","type":"parent-child","created_at":"2026-01-19T18:49:51.855943654-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-tx9.3","title":"Create file list tool","description":"# Create file list tool\n\nCreate a tool to list the directory tree of the obsidian vault.\n\n- The tree should include all files and folders.\n- Reference this for ideas: https://github.com/logancyang/obsidian-copilot/blob/master/src/tools/FileTreeTools.ts\n\nThis tool will provide the LLM with a complete view of the vault's file structure, enabling it to:\n- Understand the organization of notes and folders\n- Navigate to specific files or folders\n- Discover what documents exist in a particular area\n\n## Implementation Plan\n\n### 1. Add Request/Response Types in `src/mcp.rs`\n\n```rust\n/// Parameters for the list_files tool\n#[derive(Debug, Deserialize, JsonSchema)]\npub struct ListFilesRequest {\n    #[schemars(description = \"Subpath within the vault to list (optional, defaults to vault root)\")]\n    pub path: Option\u003cString\u003e,\n    \n    #[schemars(description = \"Maximum depth to traverse (optional, defaults to unlimited)\")]\n    pub max_depth: Option\u003cusize\u003e,\n    \n    #[schemars(description = \"Include file sizes in output (optional, defaults to false)\")]\n    pub include_sizes: Option\u003cbool\u003e,\n}\n\n/// A node in the file tree\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct FileTreeNode {\n    pub name: String,\n    pub path: String,\n    pub is_directory: bool,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub size_bytes: Option\u003cu64\u003e,\n    #[serde(skip_serializing_if = \"Vec::is_empty\", default)]\n    pub children: Vec\u003cFileTreeNode\u003e,\n}\n\n/// Response for the list_files tool\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct ListFilesResponse {\n    pub root: FileTreeNode,\n    pub total_files: usize,\n    pub total_directories: usize,\n}\n```\n\n### 2. Implement MCP Tool Method in `TaskSearchService`\n\n```rust\n#[tool(description = \"List the directory tree of the vault. Returns a hierarchical view of all files and folders. Useful for understanding vault structure and finding files.\")]\nasync fn list_files(\n    \u0026self,\n    Parameters(request): Parameters\u003cListFilesRequest\u003e,\n) -\u003e Result\u003cJson\u003cListFilesResponse\u003e, ErrorData\u003e {\n    // Resolve the search path\n    let search_path = if let Some(ref subpath) = request.path {\n        self.base_path.join(subpath)\n    } else {\n        self.base_path.clone()\n    };\n    \n    // Validate path is within vault\n    // Build the file tree recursively\n    // Respect Config path exclusions\n    // Apply max_depth if specified\n}\n```\n\n### 3. Helper Function for Tree Building\n\nAdd a helper function to recursively build the tree:\n\n```rust\nfn build_file_tree(\n    path: \u0026Path,\n    base_path: \u0026Path,\n    config: \u0026Config,\n    current_depth: usize,\n    max_depth: Option\u003cusize\u003e,\n    include_sizes: bool,\n) -\u003e Result\u003c(FileTreeNode, usize, usize), Box\u003cdyn std::error::Error\u003e\u003e {\n    // Check depth limit\n    // Check if path should be excluded via config\n    // Read directory entries\n    // Recursively process subdirectories\n    // Collect file entries\n    // Return node with accumulated counts\n}\n```\n\n### 4. Output Format Considerations\n\nTwo output format options:\n\n**Option A: Hierarchical (Recommended)**\n```json\n{\n  \"root\": {\n    \"name\": \"vault\",\n    \"path\": \"\",\n    \"is_directory\": true,\n    \"children\": [\n      {\"name\": \"Projects\", \"path\": \"Projects\", \"is_directory\": true, \"children\": [...]},\n      {\"name\": \"note.md\", \"path\": \"note.md\", \"is_directory\": false}\n    ]\n  },\n  \"total_files\": 150,\n  \"total_directories\": 25\n}\n```\n\n**Option B: Flat List (Alternative)**\n```json\n{\n  \"files\": [\"Projects/plan.md\", \"Notes/idea.md\"],\n  \"directories\": [\"Projects\", \"Notes\"],\n  \"total_files\": 150,\n  \"total_directories\": 25\n}\n```\n\nUse hierarchical format as it better represents the tree structure and matches the reference implementation.\n\n### 5. Size Management\n\nLike the obsidian-copilot reference, implement size limits:\n- If the JSON response exceeds a threshold (e.g., 500KB), simplify by:\n  - Omitting file lists and showing only directory structure\n  - Or truncating at a certain depth\n- Add a `truncated` field to indicate if output was limited\n\n### 6. Add HTTP Endpoint in `main.rs`\n\n```rust\n/// HTTP handler for listing files (GET)\nasync fn list_files_handler_get(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    query: axum::extract::Query\u003cmcp::ListFilesRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ListFilesResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    list_files_impl(state, query.0).await\n}\n\n/// HTTP handler for listing files (POST)\nasync fn list_files_handler_post(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    axum::Json(request): axum::Json\u003cmcp::ListFilesRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ListFilesResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    list_files_impl(state, request).await\n}\n```\n\nAdd route:\n```rust\n.route(\n    \"/api/files\",\n    axum::routing::get(list_files_handler_get).post(list_files_handler_post),\n)\n```\n\n### 7. Update Tools Handler\n\nAdd to `/tools` endpoint:\n```rust\nlet list_files_schema = schema_for!(ListFilesRequest);\n// Add to tools array\n```\n\n### 8. Configuration Integration\n\nThe tool should respect the existing `Config.exclude_paths` patterns, filtering out excluded directories and files from the tree output.\n\n### 9. Testing\n\n- Test tree building for nested directories\n- Test max_depth limiting\n- Test path exclusion via Config\n- Test empty directory handling\n- Test size limiting for large vaults\n\n### Files to Modify\n\n1. `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs` - Add tool and types\n2. `/home/jeffutter/src/markdown-todo-extractor/src/main.rs` - Add HTTP handlers and routes","status":"closed","priority":2,"issue_type":"feature","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-19T18:50:16.158158253-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-19T22:29:03.667089571-06:00","closed_at":"2026-01-19T22:29:03.667089571-06:00","close_reason":"Closed","labels":["planned"],"dependencies":[{"issue_id":"markdown-todo-extractor-tx9.3","depends_on_id":"markdown-todo-extractor-tx9","type":"parent-child","created_at":"2026-01-19T18:50:16.15909786-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-tx9.4","title":"Tag List Tool","description":"# Tag List Tool\n\nCreate a tool to list all tags with their document counts.\n\n## Requirements\n\n- In the list, include the tag name and number of documents that reference it.\n- Tags should be pulled from the vault\n- Tags are in the YAML frontmatter of the files in the `tags` key\n- Reference this for ideas: https://github.com/logancyang/obsidian-copilot/blob/master/src/tools/TagTools.ts\n\nThis tool will provide the LLM with a complete inventory of all tags in the vault, along with statistics on how many documents reference each tag. This enables:\n- Understanding the tag taxonomy used in the vault\n- Finding commonly used vs. rarely used tags\n- Discovering topics with the most content\n- Supporting tag-based search and filtering decisions\n\n## Implementation Plan\n\n### Overview\n\nAdd a new `list_tags` MCP tool that returns all tags from YAML frontmatter with document counts. The existing `extract_tags` tool returns only unique tag names; this new tool adds statistics. This follows the pattern established by the Obsidian Copilot TagTools.ts reference, but is focused on frontmatter tags only (as specified in the requirements).\n\n### 1. Add `TagCount` Struct and Counting Method to `src/tag_extractor.rs`\n\nAdd a new struct to hold tag statistics and a method to extract tags with counts.\n\n**Add imports at the top of the file:**\n```rust\nuse schemars::JsonSchema;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n```\n\n**Add the `TagCount` struct after the existing `TagExtractor` struct definition:**\n```rust\n/// Tag with occurrence statistics\n#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]\npub struct TagCount {\n    /// The tag name (without # prefix)\n    pub tag: String,\n    /// Number of documents containing this tag\n    pub document_count: usize,\n}\n```\n\n**Add a new method to `TagExtractor` implementation:**\n```rust\n/// Extract all tags with document counts from markdown files in the given path\n/// Returns tags sorted by document_count descending, then alphabetically\npub fn extract_tags_with_counts(\n    \u0026self,\n    path: \u0026Path,\n) -\u003e Result\u003cVec\u003cTagCount\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n    let files = if path.is_file() {\n        vec![path.to_path_buf()]\n    } else {\n        collect_markdown_files(path)?\n    };\n\n    // Track which documents contain each tag\n    // Key: tag name, Value: set of file paths that contain this tag\n    let tag_documents: HashMap\u003cString, std::collections::HashSet\u003cPathBuf\u003e\u003e = files\n        .par_iter()\n        .filter_map(|file_path| {\n            self.extract_tags_from_file(file_path)\n                .ok()\n                .map(|tags| (file_path.clone(), tags))\n        })\n        .fold(\n            || HashMap::new(),\n            |mut acc: HashMap\u003cString, std::collections::HashSet\u003cPathBuf\u003e\u003e, (file_path, tags)| {\n                // Deduplicate tags within the same file (a file counts once per tag)\n                let unique_tags: std::collections::HashSet\u003cString\u003e = tags.into_iter().collect();\n                for tag in unique_tags {\n                    acc.entry(tag)\n                        .or_insert_with(std::collections::HashSet::new)\n                        .insert(file_path.clone());\n                }\n                acc\n            },\n        )\n        .reduce(\n            || HashMap::new(),\n            |mut a, b| {\n                for (tag, files) in b {\n                    a.entry(tag)\n                        .or_insert_with(std::collections::HashSet::new)\n                        .extend(files);\n                }\n                a\n            },\n        );\n\n    // Convert to Vec\u003cTagCount\u003e sorted by document_count desc, then tag name asc\n    let mut result: Vec\u003cTagCount\u003e = tag_documents\n        .into_iter()\n        .map(|(tag, files)| TagCount {\n            tag,\n            document_count: files.len(),\n        })\n        .collect();\n\n    result.sort_by(|a, b| {\n        b.document_count\n            .cmp(\u0026a.document_count)\n            .then_with(|| a.tag.cmp(\u0026b.tag))\n    });\n\n    Ok(result)\n}\n```\n\n**Key design decisions:**\n- Use `HashSet\u003cPathBuf\u003e` to track unique documents per tag (a document counts once even if it has the same tag multiple times in frontmatter)\n- Sort by document_count descending (most popular tags first), then alphabetically for stable ordering\n- Reuse existing `extract_tags_from_file` method to maintain consistency\n\n### 2. Add Request/Response Types to `src/mcp.rs`\n\n**Add import for `TagCount` at the top:**\n```rust\nuse crate::tag_extractor::{TagCount, TagExtractor};\n```\n\n**Add new request/response structs:**\n```rust\n/// Parameters for the list_tags tool\n#[derive(Debug, Deserialize, JsonSchema)]\npub struct ListTagsRequest {\n    #[schemars(description = \"Subpath within the vault to search (optional, defaults to entire vault)\")]\n    pub path: Option\u003cString\u003e,\n\n    #[schemars(description = \"Minimum document count to include a tag (optional, defaults to 1)\")]\n    pub min_count: Option\u003cusize\u003e,\n\n    #[schemars(description = \"Maximum number of tags to return (optional, defaults to all)\")]\n    pub limit: Option\u003cusize\u003e,\n}\n\n/// Response for the list_tags tool\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct ListTagsResponse {\n    /// List of tags with their document counts\n    pub tags: Vec\u003cTagCount\u003e,\n    /// Total number of unique tags found (before filtering/limiting)\n    pub total_unique_tags: usize,\n    /// Whether the results were truncated due to limit parameter\n    pub truncated: bool,\n}\n```\n\n### 3. Add MCP Tool Method to `TaskSearchService` in `src/mcp.rs`\n\nAdd the new tool method inside the `#[tool_router] impl TaskSearchService` block:\n\n```rust\n#[tool(description = \"List all tags in the vault with document counts. Returns tags sorted by frequency (most common first). Useful for understanding the tag taxonomy, finding popular topics, and discovering content organization patterns.\")]\nasync fn list_tags(\n    \u0026self,\n    Parameters(request): Parameters\u003cListTagsRequest\u003e,\n) -\u003e Result\u003cJson\u003cListTagsResponse\u003e, ErrorData\u003e {\n    // Resolve search path\n    let search_path = if let Some(ref subpath) = request.path {\n        self.base_path.join(subpath)\n    } else {\n        self.base_path.clone()\n    };\n\n    // Extract tags with counts\n    let mut tags = self\n        .tag_extractor\n        .extract_tags_with_counts(\u0026search_path)\n        .map_err(|e| ErrorData {\n            code: ErrorCode(-32603),\n            message: Cow::from(format!(\"Failed to extract tags: {}\", e)),\n            data: None,\n        })?;\n\n    // Track total before filtering\n    let total_unique_tags = tags.len();\n\n    // Filter by min_count if specified\n    if let Some(min_count) = request.min_count {\n        tags.retain(|t| t.document_count \u003e= min_count);\n    }\n\n    // Apply limit if specified\n    let truncated = if let Some(limit) = request.limit {\n        if tags.len() \u003e limit {\n            tags.truncate(limit);\n            true\n        } else {\n            false\n        }\n    } else {\n        false\n    };\n\n    Ok(Json(ListTagsResponse {\n        tags,\n        total_unique_tags,\n        truncated,\n    }))\n}\n```\n\n### 4. Add HTTP Endpoint Handlers to `src/main.rs`\n\n**Add HTTP handler functions:**\n```rust\n/// HTTP handler for listing tags with counts (GET)\nasync fn list_tags_handler_get(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    query: axum::extract::Query\u003cmcp::ListTagsRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ListTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    list_tags_impl(state, query.0).await\n}\n\n/// HTTP handler for listing tags with counts (POST)\nasync fn list_tags_handler_post(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    axum::Json(request): axum::Json\u003cmcp::ListTagsRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ListTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    list_tags_impl(state, request).await\n}\n\n/// Shared implementation for listing tags with counts\nasync fn list_tags_impl(\n    state: AppState,\n    request: mcp::ListTagsRequest,\n) -\u003e Result\u003caxum::Json\u003cmcp::ListTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    // Resolve search path\n    let search_path = if let Some(ref subpath) = request.path {\n        state.base_path.join(subpath)\n    } else {\n        state.base_path.clone()\n    };\n\n    // Extract tags with counts\n    let mut tags = state\n        .tag_extractor\n        .extract_tags_with_counts(\u0026search_path)\n        .map_err(|e| {\n            (\n                axum::http::StatusCode::INTERNAL_SERVER_ERROR,\n                format!(\"Failed to extract tags: {}\", e),\n            )\n        })?;\n\n    // Track total before filtering\n    let total_unique_tags = tags.len();\n\n    // Filter by min_count if specified\n    if let Some(min_count) = request.min_count {\n        tags.retain(|t| t.document_count \u003e= min_count);\n    }\n\n    // Apply limit if specified\n    let truncated = if let Some(limit) = request.limit {\n        if tags.len() \u003e limit {\n            tags.truncate(limit);\n            true\n        } else {\n            false\n        }\n    } else {\n        false\n    };\n\n    Ok(axum::Json(mcp::ListTagsResponse {\n        tags,\n        total_unique_tags,\n        truncated,\n    }))\n}\n```\n\n**Add the route in the router configuration (in the `if args.mcp_http` block):**\n```rust\n.route(\n    \"/api/tags/list\",\n    axum::routing::get(list_tags_handler_get).post(list_tags_handler_post),\n)\n```\n\n**Add the new tool to the `tools_handler` function:**\n```rust\nasync fn tools_handler() -\u003e impl axum::response::IntoResponse {\n    use axum::Json;\n    use mcp::{ExtractTagsRequest, ListTagsRequest, SearchTasksRequest};\n    use schemars::schema_for;\n    use serde_json::json;\n\n    let search_tasks_schema = schema_for!(SearchTasksRequest);\n    let extract_tags_schema = schema_for!(ExtractTagsRequest);\n    let list_tags_schema = schema_for!(ListTagsRequest);\n\n    let tools = json!({\n        \"tools\": [\n            {\n                \"name\": \"search_tasks\",\n                \"description\": \"Search for tasks in Markdown files with optional filtering by status, dates, and tags\",\n                \"input_schema\": search_tasks_schema\n            },\n            {\n                \"name\": \"extract_tags\",\n                \"description\": \"Extract all unique tags from YAML frontmatter in Markdown files\",\n                \"input_schema\": extract_tags_schema\n            },\n            {\n                \"name\": \"list_tags\",\n                \"description\": \"List all tags in the vault with document counts. Returns tags sorted by frequency.\",\n                \"input_schema\": list_tags_schema\n            }\n        ]\n    });\n\n    Json(tools)\n}\n```\n\n**Update the startup message to include the new endpoint:**\n```rust\neprintln!(\"  - GET/POST http://{}/api/tags/list\", addr);\n```\n\n### 5. Add Tests to `src/tag_extractor.rs`\n\nAdd tests at the end of the `#[cfg(test)] mod tests` block:\n\n```rust\n#[test]\nfn test_extract_tags_with_counts_single_file() {\n    let extractor = TagExtractor::new();\n    let temp_dir = std::env::temp_dir().join(\"test_tags_counts\");\n    std::fs::create_dir_all(\u0026temp_dir).unwrap();\n    \n    let content = r#\"---\ntags:\n  - rust\n  - programming\n---\n# Content\n\"#;\n    let file_path = temp_dir.join(\"test1.md\");\n    std::fs::write(\u0026file_path, content).unwrap();\n    \n    let counts = extractor.extract_tags_with_counts(\u0026temp_dir).unwrap();\n    \n    assert_eq!(counts.len(), 2);\n    assert!(counts.iter().any(|t| t.tag == \"rust\" \u0026\u0026 t.document_count == 1));\n    assert!(counts.iter().any(|t| t.tag == \"programming\" \u0026\u0026 t.document_count == 1));\n    \n    std::fs::remove_dir_all(\u0026temp_dir).ok();\n}\n\n#[test]\nfn test_extract_tags_with_counts_multiple_files() {\n    let extractor = TagExtractor::new();\n    let temp_dir = std::env::temp_dir().join(\"test_tags_counts_multi\");\n    std::fs::create_dir_all(\u0026temp_dir).unwrap();\n    \n    // File 1: has rust and programming tags\n    let content1 = r#\"---\ntags:\n  - rust\n  - programming\n---\n\"#;\n    std::fs::write(temp_dir.join(\"file1.md\"), content1).unwrap();\n    \n    // File 2: has rust and cli tags\n    let content2 = r#\"---\ntags:\n  - rust\n  - cli\n---\n\"#;\n    std::fs::write(temp_dir.join(\"file2.md\"), content2).unwrap();\n    \n    let counts = extractor.extract_tags_with_counts(\u0026temp_dir).unwrap();\n    \n    // rust appears in 2 documents, programming and cli in 1 each\n    let rust = counts.iter().find(|t| t.tag == \"rust\").unwrap();\n    assert_eq!(rust.document_count, 2);\n    \n    let programming = counts.iter().find(|t| t.tag == \"programming\").unwrap();\n    assert_eq!(programming.document_count, 1);\n    \n    let cli = counts.iter().find(|t| t.tag == \"cli\").unwrap();\n    assert_eq!(cli.document_count, 1);\n    \n    // Should be sorted by count desc\n    assert_eq!(counts[0].tag, \"rust\");\n    \n    std::fs::remove_dir_all(\u0026temp_dir).ok();\n}\n\n#[test]\nfn test_extract_tags_with_counts_duplicate_in_same_file() {\n    let extractor = TagExtractor::new();\n    let temp_dir = std::env::temp_dir().join(\"test_tags_counts_dup\");\n    std::fs::create_dir_all(\u0026temp_dir).unwrap();\n    \n    // File with duplicate tag (should only count once per document)\n    let content = r#\"---\ntags:\n  - rust\n  - rust\n  - programming\n---\n\"#;\n    std::fs::write(temp_dir.join(\"file.md\"), content).unwrap();\n    \n    let counts = extractor.extract_tags_with_counts(\u0026temp_dir).unwrap();\n    \n    let rust = counts.iter().find(|t| t.tag == \"rust\").unwrap();\n    assert_eq!(rust.document_count, 1); // Should be 1, not 2\n    \n    std::fs::remove_dir_all(\u0026temp_dir).ok();\n}\n\n#[test]\nfn test_extract_tags_with_counts_empty_vault() {\n    let extractor = TagExtractor::new();\n    let temp_dir = std::env::temp_dir().join(\"test_tags_counts_empty\");\n    std::fs::create_dir_all(\u0026temp_dir).unwrap();\n    \n    let counts = extractor.extract_tags_with_counts(\u0026temp_dir).unwrap();\n    assert!(counts.is_empty());\n    \n    std::fs::remove_dir_all(\u0026temp_dir).ok();\n}\n```\n\n### Files to Modify\n\n1. `/home/jeffutter/src/markdown-todo-extractor/src/tag_extractor.rs`\n   - Add imports for `schemars`, `serde`, `HashMap`\n   - Add `TagCount` struct\n   - Add `extract_tags_with_counts` method\n   - Add unit tests\n\n2. `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs`\n   - Update import to include `TagCount`\n   - Add `ListTagsRequest` struct\n   - Add `ListTagsResponse` struct\n   - Add `list_tags` tool method\n\n3. `/home/jeffutter/src/markdown-todo-extractor/src/main.rs`\n   - Add `list_tags_handler_get` function\n   - Add `list_tags_handler_post` function\n   - Add `list_tags_impl` function\n   - Add route for `/api/tags/list`\n   - Update `tools_handler` to include new tool schema\n   - Update startup message to show new endpoint\n\n### Tool Naming Rationale\n\nThe existing `extract_tags` tool returns just unique tag names (a simple list). The new `list_tags` tool returns tags with statistics. Both tools serve different purposes:\n\n- `extract_tags`: Quick list of all unique tags (lightweight, existing functionality)\n- `list_tags`: Detailed tag statistics with document counts (new functionality)\n\nThis follows the pattern seen in the Obsidian Copilot reference where tag listing is a distinct operation from tag extraction.\n\n### API Examples\n\n**MCP Tool Call:**\n```json\n{\n  \"tool\": \"list_tags\",\n  \"arguments\": {\n    \"path\": \"Projects\",\n    \"min_count\": 2,\n    \"limit\": 50\n  }\n}\n```\n\n**HTTP GET:**\n```\nGET /api/tags/list?path=Projects\u0026min_count=2\u0026limit=50\n```\n\n**HTTP POST:**\n```json\nPOST /api/tags/list\n{\n  \"path\": \"Projects\",\n  \"min_count\": 2,\n  \"limit\": 50\n}\n```\n\n**Response:**\n```json\n{\n  \"tags\": [\n    {\"tag\": \"work\", \"document_count\": 15},\n    {\"tag\": \"project\", \"document_count\": 10},\n    {\"tag\": \"meeting\", \"document_count\": 7}\n  ],\n  \"total_unique_tags\": 45,\n  \"truncated\": false\n}\n```","status":"closed","priority":2,"issue_type":"feature","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-19T18:50:38.764378029-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-19T22:31:35.344875943-06:00","closed_at":"2026-01-19T22:31:35.344875943-06:00","close_reason":"Closed","labels":["planned"],"dependencies":[{"issue_id":"markdown-todo-extractor-tx9.4","depends_on_id":"markdown-todo-extractor-tx9","type":"parent-child","created_at":"2026-01-19T18:50:38.764956166-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-tx9.5","title":"Tag Search Tool","description":"# Tag Search Tool\n\nCreate a tool to list files based on tags.\n\n## Requirements\n\n- Tags should be searched for in the vault.\n- Tags are in the YAML frontmatter of files under the 'tags' key\n\n\nThis tool enables the LLM to find documents that match specific tags from their YAML frontmatter. Unlike the semantic search in RAG, this provides exact tag-based filtering, useful for:\n- Finding all documents with a particular tag\n- Finding documents that match multiple tags (AND/OR logic)\n- Browsing content by topic/category\n\n---\n\n## Implementation Plan\n\n### Overview\n\nCreate a new `search_by_tags` MCP tool and REST API endpoint that finds markdown files matching specified frontmatter tags. The tool will extend the existing `TagExtractor` module and follow the established patterns in the codebase.\n\n### Architecture Decision\n\nExtend the existing `TagExtractor` rather than creating a new extractor module. The `TagExtractor` already has:\n- YAML frontmatter parsing logic (`extract_frontmatter`, `parse_tags_from_frontmatter`)\n- File collection mechanism (`collect_markdown_files`)\n- Parallel processing with rayon\n\nAdding tag search functionality to this module maintains cohesion and reduces code duplication.\n\n---\n\n### Step 1: Add Config Integration to TagExtractor\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/tag_extractor.rs`\n\nCurrently `TagExtractor` is a simple unit struct without configuration. Update to match `TaskExtractor` pattern:\n\n```rust\nuse crate::config::Config;\nuse std::sync::Arc;\n\n/// Extractor for YAML frontmatter tags\npub struct TagExtractor {\n    config: Arc\u003cConfig\u003e,\n}\n\nimpl TagExtractor {\n    pub fn new(config: Arc\u003cConfig\u003e) -\u003e Self {\n        Self { config }\n    }\n}\n```\n\nUpdate `collect_markdown_files` to accept `\u0026Config` and apply path exclusions:\n\n```rust\nfn collect_markdown_files(dir: \u0026Path, config: \u0026Config) -\u003e Result\u003cVec\u003cPathBuf\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n    let mut files = Vec::new();\n\n    if dir.is_dir() {\n        for entry in fs::read_dir(dir)? {\n            let entry = entry?;\n            let path = entry.path();\n\n            // Skip excluded paths\n            if config.should_exclude(\u0026path) {\n                continue;\n            }\n\n            if path.is_dir() {\n                files.extend(collect_markdown_files(\u0026path, config)?);\n            } else if path.extension().and_then(|s| s.to_str()) == Some(\"md\") {\n                files.push(path);\n            }\n        }\n    }\n\n    Ok(files)\n}\n```\n\nUpdate `extract_tags` to pass config:\n\n```rust\npub fn extract_tags(\u0026self, path: \u0026Path) -\u003e Result\u003cVec\u003cString\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n    let files = if path.is_file() {\n        vec![path.to_path_buf()]\n    } else {\n        collect_markdown_files(path, \u0026self.config)?\n    };\n    // ... rest unchanged\n}\n```\n\n---\n\n### Step 2: Add TaggedFile Struct and Search Method\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/tag_extractor.rs`\n\nAdd new struct and search method:\n\n```rust\nuse schemars::JsonSchema;\n\n/// Represents a file that matches tag search criteria\n#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]\npub struct TaggedFile {\n    /// Absolute path to the file\n    pub file_path: String,\n    /// File name without path\n    pub file_name: String,\n    /// Tags that matched the search criteria\n    pub matched_tags: Vec\u003cString\u003e,\n    /// All tags found in the file's frontmatter\n    pub all_tags: Vec\u003cString\u003e,\n}\n\nimpl TagExtractor {\n    /// Make the internal method public for single file tag extraction\n    pub fn get_file_tags(\u0026self, file_path: \u0026Path) -\u003e Result\u003cVec\u003cString\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n        self.extract_tags_from_file(file_path)\n    }\n\n    /// Search for files by tags with AND/OR logic\n    /// \n    /// # Arguments\n    /// * `path` - Directory to search\n    /// * `tags` - Tags to search for\n    /// * `match_all` - If true, file must have ALL tags (AND logic). If false, file must have ANY tag (OR logic)\n    pub fn search_by_tags(\n        \u0026self,\n        path: \u0026Path,\n        tags: \u0026[String],\n        match_all: bool,\n    ) -\u003e Result\u003cVec\u003cTaggedFile\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n        let files = if path.is_file() {\n            vec![path.to_path_buf()]\n        } else {\n            collect_markdown_files(path, \u0026self.config)?\n        };\n\n        // Normalize search tags to lowercase for case-insensitive comparison\n        let search_tags: Vec\u003cString\u003e = tags.iter().map(|t| t.to_lowercase()).collect();\n\n        let results: Vec\u003cTaggedFile\u003e = files\n            .par_iter()\n            .filter_map(|file_path| {\n                // Extract tags from file\n                let all_tags = self.extract_tags_from_file(file_path).ok()?;\n                \n                if all_tags.is_empty() {\n                    return None;\n                }\n\n                // Normalize file tags for comparison\n                let normalized_tags: Vec\u003cString\u003e = all_tags.iter().map(|t| t.to_lowercase()).collect();\n\n                // Find which search tags match this file\n                let matched_tags: Vec\u003cString\u003e = search_tags\n                    .iter()\n                    .filter(|search_tag| normalized_tags.contains(search_tag))\n                    .cloned()\n                    .collect();\n\n                // Apply match logic\n                let matches = if match_all {\n                    // AND logic: all search tags must be present\n                    matched_tags.len() == search_tags.len()\n                } else {\n                    // OR logic: at least one search tag must be present\n                    !matched_tags.is_empty()\n                };\n\n                if matches {\n                    Some(TaggedFile {\n                        file_path: file_path.to_string_lossy().to_string(),\n                        file_name: file_path.file_name()?.to_string_lossy().to_string(),\n                        matched_tags,\n                        all_tags,\n                    })\n                } else {\n                    None\n                }\n            })\n            .collect();\n\n        Ok(results)\n    }\n}\n```\n\n---\n\n### Step 3: Add MCP Tool in mcp.rs\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs`\n\nAdd request/response types:\n\n```rust\nuse crate::tag_extractor::TaggedFile;\n\n/// Parameters for the search_by_tags tool\n#[derive(Debug, Deserialize, JsonSchema)]\npub struct SearchByTagsRequest {\n    #[schemars(description = \"Tags to search for\")]\n    pub tags: Vec\u003cString\u003e,\n\n    #[schemars(description = \"If true, file must have ALL tags (AND logic). If false, file must have ANY tag (OR logic). Default: false\")]\n    pub match_all: Option\u003cbool\u003e,\n\n    #[schemars(description = \"Subpath within the base directory to search (optional)\")]\n    pub subpath: Option\u003cString\u003e,\n\n    #[schemars(description = \"Limit the number of files returned\")]\n    pub limit: Option\u003cusize\u003e,\n}\n\n/// Response for the search_by_tags tool\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct SearchByTagsResponse {\n    pub files: Vec\u003cTaggedFile\u003e,\n    pub total_count: usize,\n}\n```\n\nAdd tool method to `TaskSearchService`:\n\n```rust\n#[tool(description = \"Search for files by YAML frontmatter tags with AND/OR matching\")]\nasync fn search_by_tags(\n    \u0026self,\n    Parameters(request): Parameters\u003cSearchByTagsRequest\u003e,\n) -\u003e Result\u003cJson\u003cSearchByTagsResponse\u003e, ErrorData\u003e {\n    // Determine the search path (base path + optional subpath)\n    let search_path = if let Some(subpath) = request.subpath {\n        self.base_path.join(subpath)\n    } else {\n        self.base_path.clone()\n    };\n\n    let match_all = request.match_all.unwrap_or(false);\n\n    // Search for files by tags\n    let mut files = self\n        .tag_extractor\n        .search_by_tags(\u0026search_path, \u0026request.tags, match_all)\n        .map_err(|e| ErrorData {\n            code: ErrorCode(-32603),\n            message: Cow::from(format!(\"Failed to search by tags: {}\", e)),\n            data: None,\n        })?;\n\n    let total_count = files.len();\n\n    // Apply limit if specified\n    if let Some(limit) = request.limit {\n        files.truncate(limit);\n    }\n\n    Ok(Json(SearchByTagsResponse { files, total_count }))\n}\n```\n\n---\n\n### Step 4: Add REST API Endpoints in main.rs\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/main.rs`\n\nAdd HTTP handlers following the existing pattern:\n\n```rust\n/// HTTP handler for searching by tags (GET with query params)\nasync fn search_by_tags_handler_get(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    query: axum::extract::Query\u003cmcp::SearchByTagsRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::SearchByTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    search_by_tags_impl(state, query.0).await\n}\n\n/// HTTP handler for searching by tags (POST with JSON body)\nasync fn search_by_tags_handler_post(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    axum::Json(request): axum::Json\u003cmcp::SearchByTagsRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::SearchByTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    search_by_tags_impl(state, request).await\n}\n\n/// Shared implementation for tag search\nasync fn search_by_tags_impl(\n    state: AppState,\n    request: mcp::SearchByTagsRequest,\n) -\u003e Result\u003caxum::Json\u003cmcp::SearchByTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    // Determine the search path (base path + optional subpath)\n    let search_path = if let Some(ref subpath) = request.subpath {\n        state.base_path.join(subpath)\n    } else {\n        state.base_path.clone()\n    };\n\n    let match_all = request.match_all.unwrap_or(false);\n\n    // Search for files by tags\n    let mut files = state\n        .tag_extractor\n        .search_by_tags(\u0026search_path, \u0026request.tags, match_all)\n        .map_err(|e| {\n            (\n                axum::http::StatusCode::INTERNAL_SERVER_ERROR,\n                format!(\"Failed to search by tags: {}\", e),\n            )\n        })?;\n\n    let total_count = files.len();\n\n    // Apply limit if specified\n    if let Some(limit) = request.limit {\n        files.truncate(limit);\n    }\n\n    Ok(axum::Json(mcp::SearchByTagsResponse { files, total_count }))\n}\n```\n\nRegister routes in the router:\n\n```rust\n.route(\n    \"/api/search_by_tags\",\n    axum::routing::get(search_by_tags_handler_get).post(search_by_tags_handler_post),\n)\n```\n\nUpdate `tools_handler()` to include the new tool schema:\n\n```rust\nasync fn tools_handler() -\u003e impl axum::response::IntoResponse {\n    use mcp::{ExtractTagsRequest, SearchByTagsRequest, SearchTasksRequest};\n    use schemars::schema_for;\n    // ...\n    let search_by_tags_schema = schema_for!(SearchByTagsRequest);\n\n    let tools = json!({\n        \"tools\": [\n            // ... existing tools ...\n            {\n                \"name\": \"search_by_tags\",\n                \"description\": \"Search for files by YAML frontmatter tags with AND/OR matching\",\n                \"input_schema\": search_by_tags_schema\n            }\n        ]\n    });\n    // ...\n}\n```\n\nUpdate startup messages:\n\n```rust\neprintln!(\"  - GET/POST http://{}/api/search_by_tags\", addr);\n```\n\n---\n\n### Step 5: Add CLI Subcommand in cli.rs\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/cli.rs`\n\nAdd new subcommand to `Commands` enum:\n\n```rust\n#[derive(Subcommand, Debug)]\npub enum Commands {\n    /// Extract and filter tasks from markdown files\n    Tasks(Box\u003cTasksCommand\u003e),\n    /// Extract all unique tags from markdown files\n    Tags {\n        /// Path to file or folder to scan\n        #[arg(required = true)]\n        path: PathBuf,\n    },\n    /// Search for files by tags\n    SearchByTags {\n        /// Path to file or folder to scan\n        #[arg(required = true)]\n        path: PathBuf,\n\n        /// Tags to search for (comma-separated)\n        #[arg(long, value_delimiter = ',', required = true)]\n        tags: Vec\u003cString\u003e,\n\n        /// Require all tags to match (AND logic) instead of any (OR logic)\n        #[arg(long)]\n        match_all: bool,\n\n        /// Limit number of results\n        #[arg(long)]\n        limit: Option\u003cusize\u003e,\n    },\n}\n```\n\nAdd handling in `run_cli()`:\n\n```rust\nSome(Commands::SearchByTags { path, tags, match_all, limit }) =\u003e {\n    // Load configuration from the path\n    let config = Arc::new(Config::load_from_base_path(\u0026path));\n\n    // Create tag extractor\n    let extractor = TagExtractor::new(config);\n\n    // Search for files by tags\n    let mut files = extractor.search_by_tags(path, \u0026tags, match_all)?;\n\n    // Apply limit if specified\n    if let Some(limit) = limit {\n        files.truncate(limit);\n    }\n\n    // Output as JSON\n    let json = serde_json::to_string_pretty(\u0026files)?;\n    println!(\"{}\", json);\n\n    Ok(())\n}\n```\n\n---\n\n### Step 6: Update Existing Code for Config Integration\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs`\n\nUpdate `TaskSearchService::new()` to pass config to TagExtractor:\n\n```rust\npub fn new(base_path: PathBuf) -\u003e Self {\n    let config = Arc::new(Config::load_from_base_path(\u0026base_path));\n\n    Self {\n        tool_router: Self::tool_router(),\n        base_path,\n        task_extractor: Arc::new(TaskExtractor::new(config.clone())),\n        tag_extractor: Arc::new(TagExtractor::new(config)),  // Updated\n    }\n}\n```\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/main.rs`\n\nUpdate `AppState` initialization:\n\n```rust\nlet app_state = AppState {\n    base_path: base_path.clone(),\n    task_extractor: Arc::new(extractor::TaskExtractor::new(config.clone())),\n    tag_extractor: Arc::new(tag_extractor::TagExtractor::new(config.clone())),  // Updated\n    config,\n};\n```\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/cli.rs`\n\nUpdate Tags command to pass config:\n\n```rust\nSome(Commands::Tags { path }) =\u003e {\n    // Load configuration from the path\n    let config = Arc::new(Config::load_from_base_path(\u0026path));\n\n    // Create tag extractor\n    let extractor = TagExtractor::new(config);\n    // ... rest unchanged\n}\n```\n\n---\n\n### Step 7: Add Unit Tests\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/tag_extractor.rs`\n\nAdd tests for the new functionality:\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::io::Write;\n    use tempfile::TempDir;\n\n    fn create_test_config() -\u003e Arc\u003cConfig\u003e {\n        Arc::new(Config::default())\n    }\n\n    fn create_test_file(dir: \u0026Path, name: \u0026str, content: \u0026str) -\u003e PathBuf {\n        let path = dir.join(name);\n        let mut file = std::fs::File::create(\u0026path).unwrap();\n        file.write_all(content.as_bytes()).unwrap();\n        path\n    }\n\n    #[test]\n    fn test_search_by_tags_or_logic() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = create_test_config();\n        let extractor = TagExtractor::new(config);\n\n        // Create test files\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - rust\\n  - cli\\n---\\n# File 1\");\n        create_test_file(temp_dir.path(), \"file2.md\", \"---\\ntags:\\n  - python\\n  - cli\\n---\\n# File 2\");\n        create_test_file(temp_dir.path(), \"file3.md\", \"---\\ntags:\\n  - java\\n---\\n# File 3\");\n\n        // Search with OR logic (default)\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"rust\".to_string(), \"python\".to_string()], false).unwrap();\n\n        assert_eq!(results.len(), 2);\n        assert!(results.iter().any(|f| f.file_name == \"file1.md\"));\n        assert!(results.iter().any(|f| f.file_name == \"file2.md\"));\n    }\n\n    #[test]\n    fn test_search_by_tags_and_logic() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = create_test_config();\n        let extractor = TagExtractor::new(config);\n\n        // Create test files\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - rust\\n  - cli\\n---\\n# File 1\");\n        create_test_file(temp_dir.path(), \"file2.md\", \"---\\ntags:\\n  - rust\\n---\\n# File 2\");\n\n        // Search with AND logic\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"rust\".to_string(), \"cli\".to_string()], true).unwrap();\n\n        assert_eq!(results.len(), 1);\n        assert_eq!(results[0].file_name, \"file1.md\");\n    }\n\n    #[test]\n    fn test_search_by_tags_case_insensitive() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = create_test_config();\n        let extractor = TagExtractor::new(config);\n\n        // Create test file with mixed case tags\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - Rust\\n  - CLI\\n---\\n# File 1\");\n\n        // Search with lowercase\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"rust\".to_string()], false).unwrap();\n        assert_eq!(results.len(), 1);\n\n        // Search with uppercase\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"RUST\".to_string()], false).unwrap();\n        assert_eq!(results.len(), 1);\n    }\n\n    #[test]\n    fn test_search_by_tags_empty_result() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = create_test_config();\n        let extractor = TagExtractor::new(config);\n\n        // Create test file\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - rust\\n---\\n# File 1\");\n\n        // Search for non-existent tag\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"nonexistent\".to_string()], false).unwrap();\n        assert!(results.is_empty());\n    }\n\n    #[test]\n    fn test_search_by_tags_respects_exclusions() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = Arc::new(Config {\n            exclude_paths: vec![\"excluded\".to_string()],\n        });\n        let extractor = TagExtractor::new(config);\n\n        // Create test files\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - rust\\n---\\n# File 1\");\n        \n        // Create excluded directory\n        let excluded_dir = temp_dir.path().join(\"excluded\");\n        std::fs::create_dir(\u0026excluded_dir).unwrap();\n        create_test_file(\u0026excluded_dir, \"file2.md\", \"---\\ntags:\\n  - rust\\n---\\n# File 2\");\n\n        // Search should not include excluded file\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"rust\".to_string()], false).unwrap();\n        assert_eq!(results.len(), 1);\n        assert_eq!(results[0].file_name, \"file1.md\");\n    }\n\n    #[test]\n    fn test_tagged_file_contains_all_tags() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = create_test_config();\n        let extractor = TagExtractor::new(config);\n\n        // Create test file with multiple tags\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - rust\\n  - cli\\n  - tool\\n---\\n# File 1\");\n\n        // Search for one tag\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"rust\".to_string()], false).unwrap();\n\n        assert_eq!(results.len(), 1);\n        assert_eq!(results[0].matched_tags, vec![\"rust\".to_string()]);\n        assert_eq!(results[0].all_tags, vec![\"rust\".to_string(), \"cli\".to_string(), \"tool\".to_string()]);\n    }\n}\n```\n\nNote: Add `tempfile = \"3\"` to dev-dependencies in Cargo.toml for tests.\n\n---\n\n### File Changes Summary\n\n| File | Changes |\n|------|---------|\n| `Cargo.toml` | Add `tempfile = \"3\"` to dev-dependencies |\n| `src/tag_extractor.rs` | Add Config integration, `TaggedFile` struct, `search_by_tags()` method, `get_file_tags()` method, unit tests |\n| `src/mcp.rs` | Add `SearchByTagsRequest`, `SearchByTagsResponse`, `search_by_tags` tool, update constructor |\n| `src/main.rs` | Add REST API handlers and routes for `/api/search_by_tags`, update AppState, update tools_handler |\n| `src/cli.rs` | Add `SearchByTags` subcommand, update Tags command for Config |\n\n---\n\n### API Examples\n\n**MCP Tool Call:**\n```json\n{\n  \"tool\": \"search_by_tags\",\n  \"arguments\": {\n    \"tags\": [\"project\", \"active\"],\n    \"match_all\": true,\n    \"limit\": 20\n  }\n}\n```\n\n**REST API Call:**\n```bash\n# GET with query params\ncurl \"http://localhost:8000/api/search_by_tags?tags=project,active\u0026match_all=true\u0026limit=20\"\n\n# POST with JSON body\ncurl -X POST http://localhost:8000/api/search_by_tags \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"tags\": [\"project\", \"active\"], \"match_all\": true, \"limit\": 20}'\n```\n\n**CLI:**\n```bash\n# Search for files with ANY of the tags (OR logic - default)\nmarkdown-todo-extractor search-by-tags /path/to/vault --tags project,active\n\n# Search for files with ALL tags (AND logic)\nmarkdown-todo-extractor search-by-tags /path/to/vault --tags project,active --match-all\n\n# With limit\nmarkdown-todo-extractor search-by-tags /path/to/vault --tags project --limit 10\n```\n\n**Response Format:**\n```json\n{\n  \"files\": [\n    {\n      \"file_path\": \"/vault/projects/ProjectA.md\",\n      \"file_name\": \"ProjectA.md\",\n      \"matched_tags\": [\"project\", \"active\"],\n      \"all_tags\": [\"project\", \"active\", \"2024\"]\n    }\n  ],\n  \"total_count\": 1\n}\n```\n\n---\n\n### Testing Checklist\n\n1. [ ] Unit tests pass for tag search logic (AND/OR)\n2. [ ] Unit tests pass for case-insensitive matching\n3. [ ] Unit tests pass for path exclusions\n4. [ ] MCP tool works via stdio\n5. [ ] REST API endpoints work (GET and POST)\n6. [ ] CLI subcommand works\n7. [ ] AND logic correctly filters files (must have ALL tags)\n8. [ ] OR logic correctly includes files (must have ANY tag)\n9. [ ] Path exclusions are respected\n10. [ ] Limit parameter works\n11. [ ] Subpath parameter works\n12. [ ] Empty results handled gracefully\n13. [ ] Files without frontmatter are skipped gracefully\n14. [ ] `cargo build --release` succeeds\n15. [ ] `cargo clippy` passes\n16. [ ] `cargo fmt --check` passes","status":"closed","priority":2,"issue_type":"feature","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-19T18:51:01.199607478-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-19T22:35:17.861300724-06:00","closed_at":"2026-01-19T22:35:17.861300724-06:00","close_reason":"Closed","labels":["planned"],"dependencies":[{"issue_id":"markdown-todo-extractor-tx9.5","depends_on_id":"markdown-todo-extractor-tx9","type":"parent-child","created_at":"2026-01-19T18:51:01.200457025-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-v0k","title":"Refactor into capability-based modules with unified trait interface","description":"Refactor the project architecture to support multiple interfaces (MCP, HTTP, CLI) through a common trait system.\n\n## Architecture Overview\n\nSplit capabilities into focused modules:\n- Tag listing/retrieval\n- Task search  \n- File list/fetch\n\n## Module Design\n\nEach module should:\n1. Handle logic for its specific capability area\n2. Expose a type implementing a common trait\n3. Support all three interface types through the trait\n\n## Trait Requirements\n\nCreate a common trait that provides:\n- MCP server integration\n- HTTP REST-style endpoint support\n- CLI command interface\n\nWhen implemented, the trait should enable near-automatic exposure via all three interfaces.\n\n## Benefits\n\n- Consistent interface across all capability areas\n- Single implementation supports multiple access patterns\n- Easier to add new capabilities in the future\n- Cleaner separation of concerns","status":"closed","priority":2,"issue_type":"feature","owner":"jeff@jeffutter.com","created_at":"2026-01-19T23:21:12.250198134-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T11:13:13.51127458-06:00","closed_at":"2026-01-20T11:13:13.51127458-06:00","close_reason":"Closed"}
{"id":"markdown-todo-extractor-ved","title":"Full-text search capability","description":"Implement content search across vault with regex/fuzzy matching. Need to design efficient implementation strategy. Methods: search_content(query, filters).","design":"# Implementation Plan: Full-Text Search Capability\n\n**Issue**: markdown-todo-extractor-ved - Full-text search capability\n\n## Overview\n\nImplement a content search capability that allows searching across markdown files in the vault with support for:\n- Regex pattern matching\n- Case-insensitive text search\n- Fuzzy/approximate matching\n- Configurable context lines around matches\n- File path patterns (glob), modified date filters, and result limits\n\n## Architecture Approach\n\nFollow the established capability-based architecture pattern where a single operation automatically exposes functionality via HTTP, CLI, and MCP interfaces.\n\n**Components to create:**\n1. `src/search_extractor.rs` - Core search logic and file processing\n2. `src/capabilities/search.rs` - Capability and operation definitions\n3. Update `src/capabilities/mod.rs` - Register new capability\n4. Update `Cargo.toml` - Add fuzzy matching dependency\n\n## Critical Files\n\n### New Files\n- `src/search_extractor.rs` - SearchExtractor struct with search methods\n- `src/capabilities/search.rs` - SearchCapability and SearchContentOperation\n\n### Modified Files\n- `src/capabilities/mod.rs` - Add search module and register capability\n- `Cargo.toml` - Add `nucleo-matcher` crate for fuzzy matching\n\n## Detailed Design\n\n### 1. Search Extractor (`src/search_extractor.rs`)\n\n**Purpose**: Core search logic, file traversal, pattern matching\n\n**Key components**:\n\n```rust\npub struct SearchExtractor {\n    config: Arc\u003cConfig\u003e,\n}\n\npub struct SearchMatch {\n    pub file_path: String,\n    pub file_name: String,\n    pub line_number: usize,\n    pub matched_line: String,\n    pub context_before: Vec\u003cString\u003e,  // Lines before match\n    pub context_after: Vec\u003cString\u003e,   // Lines after match\n    pub match_type: MatchType,        // Regex, Text, or Fuzzy\n}\n\npub struct SearchResult {\n    pub matches: Vec\u003cSearchMatch\u003e,\n    pub total_matches: usize,\n    pub files_searched: usize,\n    pub truncated: bool,  // True if results were limited\n}\n\npub enum MatchType {\n    Regex,\n    Text,\n    Fuzzy,\n}\n```\n\n**Methods**:\n- `search_content()` - Main entry point, delegates to specific search type\n- `search_regex()` - Regex pattern search with compiled patterns\n- `search_text()` - Simple case-insensitive substring search\n- `search_fuzzy()` - Fuzzy matching using nucleo-matcher\n- `process_file()` - Read file, search lines, extract context\n- `collect_markdown_files()` - Reuse pattern from tag_extractor.rs\n\n**Implementation notes**:\n- Use `rayon::par_iter()` for parallel file processing\n- Compile regex patterns once and pass to worker threads (Arc\u003cRegex\u003e)\n- Respect `config.should_exclude()` for path filtering\n- Read files with `fs::read_to_string()` like existing extractors\n- For context extraction: collect N lines before/after maintaining line numbers\n\n### 2. Capability Module (`src/capabilities/search.rs`)\n\n**Structure** (following established pattern):\n\n```rust\n// Request struct - serves as CLI args, HTTP params, and MCP input\n#[derive(Debug, Deserialize, Serialize, JsonSchema, Parser)]\n#[command(name = \"search-content\", about = \"Search for content across markdown files\")]\npub struct SearchContentRequest {\n    // CLI-only path field\n    #[arg(index = 1)]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(skip)]\n    pub path: Option\u003cPathBuf\u003e,\n\n    // Search query\n    #[arg(long, short = 'q', required = true)]\n    #[schemars(description = \"Search query (text, regex, or fuzzy pattern)\")]\n    pub query: String,\n\n    // Search mode\n    #[arg(long, default_value = \"text\")]\n    #[schemars(description = \"Search mode: 'text', 'regex', or 'fuzzy'\")]\n    pub mode: Option\u003cString\u003e,  // \"text\", \"regex\", \"fuzzy\"\n\n    // Case sensitivity (for text mode)\n    #[arg(long)]\n    #[schemars(description = \"Case-sensitive search (default: false)\")]\n    pub case_sensitive: Option\u003cbool\u003e,\n\n    // Context lines\n    #[arg(long, short = 'C', default_value = \"2\")]\n    #[schemars(description = \"Number of context lines before and after match\")]\n    pub context: Option\u003cusize\u003e,\n\n    // File path filter (glob pattern)\n    #[arg(long)]\n    #[schemars(description = \"File path pattern (glob) to filter search\")]\n    pub file_pattern: Option\u003cString\u003e,\n\n    // Modified date filter\n    #[arg(long)]\n    #[schemars(description = \"Only search files modified after this date (YYYY-MM-DD)\")]\n    pub modified_after: Option\u003cString\u003e,\n\n    #[arg(long)]\n    #[schemars(description = \"Only search files modified before this date (YYYY-MM-DD)\")]\n    pub modified_before: Option\u003cString\u003e,\n\n    // Result limits\n    #[arg(long, default_value = \"100\")]\n    #[schemars(description = \"Maximum total matches to return\")]\n    pub max_results: Option\u003cusize\u003e,\n\n    #[arg(long, default_value = \"10\")]\n    #[schemars(description = \"Maximum matches per file\")]\n    pub max_per_file: Option\u003cusize\u003e,\n\n    // Subpath (for HTTP/MCP - alternative to CLI path)\n    #[arg(skip)]\n    #[schemars(description = \"Optional subpath within base path to search\")]\n    pub subpath: Option\u003cPathBuf\u003e,\n}\n\n// Response struct\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct SearchContentResponse {\n    pub matches: Vec\u003cSearchMatch\u003e,\n    pub total_matches: usize,\n    pub files_searched: usize,\n    pub truncated: bool,\n}\n\n// Capability struct\npub struct SearchCapability {\n    base_path: PathBuf,\n    search_extractor: Arc\u003cSearchExtractor\u003e,\n}\n\nimpl SearchCapability {\n    pub fn new(base_path: PathBuf, config: Arc\u003cConfig\u003e) -\u003e Self {\n        Self {\n            base_path,\n            search_extractor: Arc::new(SearchExtractor::new(config)),\n        }\n    }\n\n    pub async fn search_content(\n        \u0026self,\n        request: SearchContentRequest,\n    ) -\u003e CapabilityResult\u003cSearchContentResponse\u003e {\n        // Resolve search path\n        let search_path = if let Some(ref subpath) = request.subpath {\n            self.base_path.join(subpath)\n        } else {\n            self.base_path.clone()\n        };\n\n        // Validate and execute search based on mode\n        let mode = request.mode.as_deref().unwrap_or(\"text\");\n        let result = match mode {\n            \"regex\" =\u003e self.search_extractor.search_regex(\u0026search_path, \u0026request),\n            \"fuzzy\" =\u003e self.search_extractor.search_fuzzy(\u0026search_path, \u0026request),\n            _ =\u003e self.search_extractor.search_text(\u0026search_path, \u0026request),\n        }.map_err(|e| internal_error(format!(\"Search failed: {}\", e)))?;\n\n        Ok(SearchContentResponse {\n            matches: result.matches,\n            total_matches: result.total_matches,\n            files_searched: result.files_searched,\n            truncated: result.truncated,\n        })\n    }\n}\n\n// Operation struct\npub struct SearchContentOperation {\n    capability: Arc\u003cSearchCapability\u003e,\n}\n\nimpl SearchContentOperation {\n    pub fn new(capability: Arc\u003cSearchCapability\u003e) -\u003e Self {\n        Self { capability }\n    }\n}\n\n// Operation metadata module\npub mod search_content {\n    pub const DESCRIPTION: \u0026str = \"Search for content across markdown files with regex, text, or fuzzy matching\";\n    pub const CLI_NAME: \u0026str = \"search-content\";\n    pub const HTTP_PATH: \u0026str = \"/api/search/content\";\n}\n\n// Implement Operation trait\n#[async_trait]\nimpl Operation for SearchContentOperation {\n    fn name(\u0026self) -\u003e \u0026'static str {\n        search_content::CLI_NAME\n    }\n\n    fn path(\u0026self) -\u003e \u0026'static str {\n        search_content::HTTP_PATH\n    }\n\n    fn description(\u0026self) -\u003e \u0026'static str {\n        search_content::DESCRIPTION\n    }\n\n    fn get_command(\u0026self) -\u003e clap::Command {\n        SearchContentRequest::command()\n    }\n\n    async fn execute_json(\u0026self, json: Value) -\u003e Result\u003cValue, ErrorData\u003e {\n        execute_json_operation(json, |req| self.capability.search_content(req)).await\n    }\n\n    async fn execute_from_args(\n        \u0026self,\n        matches: \u0026ArgMatches,\n        _registry: \u0026CapabilityRegistry,\n    ) -\u003e Result\u003cString, Box\u003cdyn Error\u003e\u003e {\n        let request = SearchContentRequest::from_arg_matches(matches)?;\n\n        // Handle CLI-specific path\n        let response = if let Some(ref path) = request.path {\n            let config = Arc::new(Config::load_from_base_path(path.as_path()));\n            let capability = SearchCapability::new(path.clone(), config);\n            let mut req_without_path = request;\n            req_without_path.path = None;\n            capability.search_content(req_without_path).await?\n        } else {\n            self.capability.search_content(request).await?\n        };\n\n        Ok(serde_json::to_string_pretty(\u0026response)?)\n    }\n\n    fn input_schema(\u0026self) -\u003e serde_json::Value {\n        schemars::schema_for!(SearchContentRequest)\n    }\n}\n```\n\n### 3. Registration (`src/capabilities/mod.rs`)\n\nAdd to the CapabilityRegistry:\n\n```rust\npub mod search;\n\npub struct CapabilityRegistry {\n    // ... existing fields ...\n    search_capability: Arc\u003cSearchCapability\u003e,\n}\n\nimpl CapabilityRegistry {\n    pub fn new(base_path: PathBuf, config: Arc\u003cConfig\u003e) -\u003e Self {\n        Self {\n            // ... existing initialization ...\n            search_capability: Arc::new(SearchCapability::new(base_path.clone(), config.clone())),\n        }\n    }\n\n    pub fn search(\u0026self) -\u003e Arc\u003cSearchCapability\u003e {\n        Arc::clone(\u0026self.search_capability)\n    }\n\n    pub fn create_operations(\u0026self) -\u003e Vec\u003cArc\u003cdyn Operation\u003e\u003e {\n        vec![\n            // ... existing operations ...\n            Arc::new(search::SearchContentOperation::new(self.search())),\n        ]\n    }\n}\n```\n\n### 4. Dependencies (`Cargo.toml`)\n\nAdd fuzzy matching support:\n\n```toml\n[dependencies]\n# ... existing dependencies ...\nnucleo-matcher = \"0.3\"  # Fast fuzzy matcher (used by Helix editor)\n```\n\n**Why nucleo-matcher?**\n- High-performance fuzzy matching (used in production by Helix editor)\n- Simple API\n- No complex configuration needed\n- Good for interactive search scenarios\n\n## Implementation Details\n\n### File Filtering Strategy\n\n1. **Path exclusions**: Use existing `config.should_exclude()` from Config\n2. **Glob patterns**: Use `glob::Pattern` (already a dependency) for `file_pattern` matching\n3. **Modified date**: Use `std::fs::metadata().modified()` with date comparison\n\n### Performance Optimizations\n\n1. **Parallel processing**: Use `rayon::par_iter()` for file-level parallelism\n2. **Early termination**: Stop searching when `max_results` reached\n3. **Compiled patterns**: Create regex patterns once, share via `Arc\u003cRegex\u003e` across threads\n4. **File reading**: Use `std::fs::read_to_string()` (same as existing extractors)\n\n### Search Mode Details\n\n**Text mode** (default):\n- Simple case-insensitive substring search using `.to_lowercase().contains()`\n- Fast, no regex overhead\n- Good for simple queries\n\n**Regex mode**:\n- Compile pattern with `Regex::new()`\n- Use case-insensitive flag if specified: `(?i)pattern`\n- Return error if pattern is invalid\n- Good for complex pattern matching\n\n**Fuzzy mode**:\n- Use `nucleo_matcher::Matcher` with default config\n- Match against each line\n- Score matches and sort by relevance\n- Good for approximate matching (typos, variations)\n\n### Context Extraction Algorithm\n\nFor each match at line N with context size C:\n1. Read file into `Vec\u003cString\u003e` (split by lines)\n2. Find matching line at index N\n3. Extract lines `[N-C .. N)` for context_before (handle bounds)\n4. Extract lines `(N .. N+C]` for context_after (handle bounds)\n5. Store with proper line numbers\n\n### Result Limiting Strategy\n\n1. **Per-file limit**: Stop processing file after `max_per_file` matches\n2. **Total limit**: Track global match count, stop all processing when reached\n3. **Set truncated flag**: Indicate if results were limited\n4. **Files searched counter**: Track how many files were examined before stopping\n\n## MCP Integration\n\nThe operation will automatically be exposed via MCP when registered. Update `src/mcp.rs` tool instructions:\n\nAdd to `get_info()` instructions:\n```markdown\n### search-content\nSearch for content across markdown files with multiple matching modes.\n\n**Parameters:**\n- `query` (required): Search query string\n- `mode`: Search mode - \"text\" (default), \"regex\", or \"fuzzy\"\n- `case_sensitive`: Enable case-sensitive search (default: false)\n- `context`: Number of context lines before/after match (default: 2)\n- `file_pattern`: Glob pattern to filter files (e.g., \"projects/**/*.md\")\n- `modified_after/before`: Filter by file modification date (YYYY-MM-DD)\n- `max_results`: Maximum total matches (default: 100)\n- `max_per_file`: Maximum matches per file (default: 10)\n- `subpath`: Optional subpath within vault to search\n\n**Returns:** List of matches with file paths, line numbers, content, and context\n\n**Example:**\n```json\n{\n  \"query\": \"TODO\",\n  \"mode\": \"text\",\n  \"context\": 3,\n  \"file_pattern\": \"projects/**\",\n  \"max_results\": 50\n}\n```\n```\n\n## Verification Plan\n\n### Unit Tests\n1. Test each search mode (text, regex, fuzzy) with sample content\n2. Test context extraction with edge cases (beginning/end of file)\n3. Test file filtering (glob patterns, date ranges)\n4. Test result limiting (per-file and total)\n5. Test path exclusion integration\n\n### Integration Tests\n1. **CLI**: `cargo run -- search-content /path/to/vault -q \"search term\" --mode text --context 2`\n2. **HTTP**: `curl \"http://localhost:3000/api/search/content?query=TODO\u0026mode=text\u0026context=2\"`\n3. **MCP**: Test via MCP client calling search_content tool\n\n### Manual Testing\n1. Create test vault with sample markdown files\n2. Test with various queries:\n   - Simple text: \"project\"\n   - Regex pattern: `\\d{4}-\\d{2}-\\d{2}` (dates)\n   - Fuzzy: \"aproximate\" finding \"approximate\"\n3. Test filters:\n   - File patterns: `projects/**/*.md`\n   - Date ranges: files modified this week\n   - Result limits: verify truncation\n4. Test context display: verify before/after lines are correct\n5. Test edge cases:\n   - Empty query\n   - Invalid regex\n   - No matches\n   - Matches at beginning/end of file\n\n### Performance Testing\n1. Test on large vault (1000+ files)\n2. Verify parallel processing is working (CPU usage)\n3. Ensure result limiting prevents runaway queries\n4. Check memory usage with large result sets\n\n## Error Handling\n\nHandle these scenarios gracefully:\n- Invalid regex pattern → return ErrorData with clear message\n- File read errors → log warning, skip file, continue search\n- Invalid date format → return ErrorData with validation message\n- Invalid mode value → default to \"text\" mode\n- Empty query → return empty results (or error?)\n- Path doesn't exist → return ErrorData\n\n## CLI Examples\n\n```bash\n# Simple text search\ncargo run -- search-content /vault -q \"TODO\"\n\n# Regex search for dates\ncargo run -- search-content /vault -q '\\d{4}-\\d{2}-\\d{2}' --mode regex\n\n# Fuzzy search with more context\ncargo run -- search-content /vault -q \"aproximate\" --mode fuzzy --context 5\n\n# Search in specific folder with limits\ncargo run -- search-content /vault -q \"meeting\" --file-pattern \"projects/**\" --max-results 20\n\n# Search recent files\ncargo run -- search-content /vault -q \"sprint\" --modified-after 2025-01-01\n```\n\n## Implementation Order\n\n1. Add `nucleo-matcher` to Cargo.toml\n2. Create `src/search_extractor.rs` with core search logic\n3. Create `src/capabilities/search.rs` with capability and operation\n4. Update `src/capabilities/mod.rs` to register capability\n5. Add unit tests for search_extractor\n6. Test CLI manually\n7. Test HTTP endpoint manually\n8. Test MCP tool via client\n9. Add integration tests\n10. Update documentation (CLAUDE.md if needed)\n\n## Success Criteria\n\n- [ ] All three search modes work correctly (text, regex, fuzzy)\n- [ ] Context lines are extracted accurately\n- [ ] File filtering works (glob patterns, date ranges)\n- [ ] Result limits prevent overwhelming responses\n- [ ] Operation auto-registers for HTTP, CLI, and MCP\n- [ ] Path exclusions from config are respected\n- [ ] Parallel processing provides good performance\n- [ ] Error handling is robust and user-friendly\n- [ ] Tests pass (unit and integration)\n- [ ] Manual testing confirms expected behavior","status":"open","priority":2,"issue_type":"feature","owner":"jeff@jeffutter.com","created_at":"2026-01-21T20:52:03.198816314-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-22T08:14:21.732949069-06:00","labels":["planned"]}
{"id":"markdown-todo-extractor-vw9","title":"Daily notes capability","description":"Support for querying daily notes by date or date range. Methods: get_daily_note(date), search_daily_notes(date_range). May leverage multi-file reading once available.","design":"# Daily Notes Capability Implementation Plan\n\n## Overview\n\nImplement a daily notes capability for markdown-todo-extractor that enables querying Obsidian daily notes by date or date range. This addresses beads issue `markdown-todo-extractor-vw9`.\n\n**Dependency**: Issue `markdown-todo-extractor-718` (multi-file reading) is currently OPEN. We will implement Phase 1 with single-file support now, then enhance with multi-file capability once the dependency is resolved.\n\n## Architecture\n\n### New Capability: DailyNoteCapability\n\nFollowing the established pattern (tasks, tags, files), create a new capability module with:\n\n1. **Two operations**:\n   - `get_daily_note(date)` - Retrieve a single daily note for a specific date\n   - `search_daily_notes(start_date, end_date)` - Find all daily notes in a date range (metadata only in Phase 1)\n\n2. **Configuration** - Add `daily_note_patterns` to Config struct:\n   - Default: `[\"YYYY-MM-DD.md\"]`\n   - Support patterns like: `Daily/YYYY-MM-DD.md`, `YYYY/MM/DD.md`\n   - Configurable via `.markdown-todo-extractor.toml` and environment variable\n\n3. **Date handling** - No external date library (consistent with existing code):\n   - Simple YYYY-MM-DD string parsing\n   - Lexicographic comparison for sorting/filtering\n   - Manual validation of month/day ranges\n\n## Implementation Details\n\n### 1. Configuration Extension\n\n**File**: `src/config.rs` (+15 lines)\n\nAdd to Config struct:\n```rust\n#[derive(Debug, Clone, Deserialize, Default)]\npub struct Config {\n    #[serde(default)]\n    pub exclude_paths: Vec\u003cString\u003e,\n\n    #[serde(default = \"default_daily_note_patterns\")]\n    pub daily_note_patterns: Vec\u003cString\u003e,\n}\n\nfn default_daily_note_patterns() -\u003e Vec\u003cString\u003e {\n    vec![\"YYYY-MM-DD.md\".to_string()]\n}\n```\n\nUpdate `merge_from_env()` to support `MARKDOWN_TODO_EXTRACTOR_DAILY_NOTE_PATTERNS` environment variable.\n\n### 2. New Module Structure\n\n**File**: `src/capabilities/daily_notes.rs` (~350 lines)\n\n**Module organization**:\n```rust\n// Submodules for utilities\nmod date_utils;   // Date parsing, validation, range generation\nmod pattern;      // Pattern matching, file discovery\n\n// Operation metadata modules\npub mod get_daily_note { /* CLI_NAME, HTTP_PATH, DESCRIPTION */ }\npub mod search_daily_notes { /* CLI_NAME, HTTP_PATH, DESCRIPTION */ }\n\n// Request/Response structs (dual derives: Parser + JsonSchema)\npub struct GetDailyNoteRequest { /* vault_path, date */ }\npub struct GetDailyNoteResponse { /* content, file_path, date, found */ }\npub struct SearchDailyNotesRequest { /* vault_path, start_date, end_date, limit, sort */ }\npub struct SearchDailyNotesResponse { /* notes: Vec\u003cDailyNoteMetadata\u003e, total_count */ }\n\n// Capability\npub struct DailyNoteCapability { /* base_path, config, file_capability */ }\nimpl DailyNoteCapability {\n    pub async fn get_daily_note(...) -\u003e CapabilityResult\u003cGetDailyNoteResponse\u003e\n    pub async fn search_daily_notes(...) -\u003e CapabilityResult\u003cSearchDailyNotesResponse\u003e\n}\n\n// Operations\npub struct GetDailyNoteOperation { capability: Arc\u003cDailyNoteCapability\u003e }\npub struct SearchDailyNotesOperation { capability: Arc\u003cDailyNoteCapability\u003e }\nimpl Operation for GetDailyNoteOperation { /* ... */ }\nimpl Operation for SearchDailyNotesOperation { /* ... */ }\n```\n\n**Key helper functions**:\n- `date_utils::validate_date(date_str)` - Validate YYYY-MM-DD format\n- `date_utils::parse_date(date_str)` - Extract (year, month, day) tuple\n- `date_utils::date_range(start, end)` - Generate Vec of dates in range\n- `pattern::apply_pattern(pattern, year, month, day)` - Substitute placeholders\n- `pattern::find_daily_note(base_path, date, patterns, config)` - Find matching file\n\n### 3. Core Logic: get_daily_note\n\n**Algorithm**:\n1. Validate date format (YYYY-MM-DD)\n2. For each pattern in `config.daily_note_patterns`:\n   - Substitute YYYY, MM, DD placeholders with date components\n   - Construct full path: `base_path.join(pattern)`\n   - Check if file exists and is not excluded\n3. If multiple matches → Error with details\n4. If one match → Delegate to `FileCapability.read_file()`\n5. If no match → Return `found: false` (soft error, not exception)\n\n**Security**: Leverage existing FileCapability security validation (canonicalization, starts_with check).\n\n### 4. Core Logic: search_daily_notes\n\n**Algorithm**:\n1. Validate start_date and end_date (default to last 30 days if not provided)\n2. Generate date range using `date_utils::date_range()`\n3. For each date, call `pattern::find_daily_note()` to check existence\n4. Collect metadata for found files (no content in Phase 1)\n5. Sort by date (ascending or descending per `sort` parameter)\n6. Apply limit\n7. Return `Vec\u003cDailyNoteMetadata\u003e`\n\n**Constraints**:\n- Limit maximum range to 365 days to prevent abuse\n- Default to descending sort (newest first)\n\n### 5. Registration\n\n**File**: `src/capabilities/mod.rs` (+25 lines)\n\n```rust\npub mod daily_notes;\nuse self::daily_notes::DailyNoteCapability;\n\npub struct CapabilityRegistry {\n    // ... existing fields ...\n    daily_note_capability: Arc\u003cDailyNoteCapability\u003e,\n}\n\nimpl CapabilityRegistry {\n    pub fn new(base_path: PathBuf, config: Arc\u003cConfig\u003e) -\u003e Self {\n        let file_cap = Arc::new(FileCapability::new(base_path.clone(), Arc::clone(\u0026config)));\n\n        Self {\n            // ... existing initializations ...\n            daily_note_capability: Arc::new(DailyNoteCapability::new(\n                base_path,\n                Arc::clone(\u0026config),\n                file_cap,\n            )),\n        }\n    }\n\n    pub fn daily_notes(\u0026self) -\u003e Arc\u003cDailyNoteCapability\u003e {\n        Arc::clone(\u0026self.daily_note_capability)\n    }\n\n    pub fn create_operations(\u0026self) -\u003e Vec\u003cArc\u003cdyn Operation\u003e\u003e {\n        vec![\n            // ... existing operations ...\n            Arc::new(daily_notes::GetDailyNoteOperation::new(self.daily_notes())),\n            Arc::new(daily_notes::SearchDailyNotesOperation::new(self.daily_notes())),\n        ]\n    }\n}\n```\n\n**File**: `src/mcp.rs` (+30 lines)\n\nAdd two `#[tool]` methods:\n```rust\n#[tool(description = \"Get daily note for a specific date\")]\nasync fn get_daily_note(\u0026self, Parameters(request): Parameters\u003cGetDailyNoteRequest\u003e)\n    -\u003e Result\u003cJson\u003cGetDailyNoteResponse\u003e, ErrorData\u003e\n\n#[tool(description = \"Search daily notes by date range\")]\nasync fn search_daily_notes(\u0026self, Parameters(request): Parameters\u003cSearchDailyNotesRequest\u003e)\n    -\u003e Result\u003cJson\u003cSearchDailyNotesResponse\u003e, ErrorData\u003e\n```\n\nBoth delegate to `self.capability_registry.daily_notes()` methods.\n\n### 6. Error Handling\n\n**Strategy**: Follow existing patterns\n\n- **Invalid date format** → `invalid_params(\"Expected YYYY-MM-DD format\")`\n- **Missing daily note** → Soft error (return `found: false`, not exception)\n- **Multiple matches** → `internal_error(\"Multiple daily notes found for date X\")`\n- **Date range too large** → `invalid_params(\"Date range limited to 365 days\")`\n- **Invalid date values** → `invalid_params(\"Invalid month/day\")`\n\n## Critical Files\n\n### Files to Create\n1. **`src/capabilities/daily_notes.rs`** - Main capability module (~350 lines)\n   - Request/Response types\n   - DailyNoteCapability implementation\n   - Operation wrappers\n   - Submodule declarations\n\n2. **`src/capabilities/daily_notes/date_utils.rs`** - Date utilities (~150 lines)\n   - validate_date, parse_date, format_date\n   - date_range generation\n   - Leap year handling\n\n3. **`src/capabilities/daily_notes/pattern.rs`** - Pattern matching (~120 lines)\n   - apply_pattern (substitute YYYY/MM/DD)\n   - find_daily_note (discovery with security checks)\n\n### Files to Modify\n4. **`src/config.rs`** - Add daily_note_patterns field (~15 lines)\n   - New field with default\n   - Environment variable support\n\n5. **`src/capabilities/mod.rs`** - Register capability (~25 lines)\n   - Add daily_notes module\n   - Add to CapabilityRegistry\n   - Register operations\n\n6. **`src/mcp.rs`** - Add MCP tools (~30 lines)\n   - get_daily_note tool\n   - search_daily_notes tool\n\n## Verification Plan\n\n### Unit Tests\n1. **Date utilities** (`daily_notes/date_utils.rs`):\n   - Valid/invalid date formats\n   - Leap year handling\n   - Date range generation\n   - Edge cases (2000-01-01, 9999-12-31)\n\n2. **Pattern matching** (`daily_notes/pattern.rs`):\n   - Pattern substitution (YYYY/MM/DD)\n   - File discovery (found, not found, multiple)\n   - Exclusion filtering\n   - Security validation\n\n3. **Capability methods**:\n   - get_daily_note with found/not found/multiple matches\n   - search_daily_notes with various date ranges\n   - Configuration variations\n\n### Integration Tests\nCreate `tests/daily_notes_integration.rs`:\n1. Setup test vault with sample daily notes\n2. Test get_daily_note end-to-end\n3. Test search_daily_notes with date ranges\n4. Test pattern configuration variations\n5. Test CLI, HTTP, and MCP interfaces\n\n### Manual Testing\n\n**Setup test vault**:\n```bash\nmkdir -p /tmp/test_vault/Daily\necho \"# 2025-01-20\" \u003e /tmp/test_vault/2025-01-20.md\necho \"# 2025-01-21\" \u003e /tmp/test_vault/Daily/2025-01-21.md\necho \"# 2025-01-22\" \u003e /tmp/test_vault/2025-01-22.md\n```\n\n**Test CLI**:\n```bash\n# Get single daily note\ncargo run -- get-daily-note /tmp/test_vault 2025-01-22\n\n# Search date range\ncargo run -- search-daily-notes /tmp/test_vault \\\n    --start-date 2025-01-20 --end-date 2025-01-22 \\\n    --sort desc\n```\n\n**Test HTTP** (with server running):\n```bash\ncurl -X POST http://localhost:3000/api/daily-notes \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"date\": \"2025-01-22\"}'\n\ncurl -X POST http://localhost:3000/api/daily-notes/search \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"start_date\": \"2025-01-20\", \"end_date\": \"2025-01-22\"}'\n```\n\n**Test MCP** (use Claude Code or other MCP client):\n```typescript\nawait client.call(\"get-daily-note\", { date: \"2025-01-22\" });\nawait client.call(\"search-daily-notes\", {\n    start_date: \"2025-01-20\",\n    end_date: \"2025-01-22\"\n});\n```\n\n### Build and Quality Gates\n```bash\n# Format code\ncargo fmt\n\n# Run linter\ncargo clippy -- -D warnings\n\n# Run all tests\ncargo test\n\n# Build release\ncargo build --release\n```\n\n## Implementation Sequence\n\n1. **Config extension** - Add daily_note_patterns to Config\n2. **Date utilities** - Implement date_utils module with tests\n3. **Pattern matching** - Implement pattern module with tests\n4. **Capability** - Implement DailyNoteCapability with both operations\n5. **Operations** - Implement Operation trait wrappers\n6. **Registration** - Wire up to registry, MCP\n7. **Testing** - Unit tests, integration tests, manual verification\n8. **Documentation** - Update CLAUDE.md with usage examples\n\n## Trade-offs and Rationale\n\n### Phase 1 vs Waiting for Multi-file\n**Decision**: Implement single-file now, enhance later\n\n**Rationale**:\n- Single-file has immediate value (common use case)\n- Unblocks progress while multi-file feature develops\n- Enhancement is additive (won't break existing API)\n- Users can iterate manually if they need multiple notes\n\n### Simple Date Parsing vs chrono\n**Decision**: Simple string-based parsing\n\n**Rationale**:\n- Consistent with existing codebase (no chrono dependency)\n- YYYY-MM-DD is lexicographically sortable\n- Day-level granularity is sufficient\n- Avoids dependency bloat\n\n### Configuration vs Auto-detection\n**Decision**: Explicit configuration with sensible defaults\n\n**Rationale**:\n- Users know exactly what's happening\n- No vault scanning at startup (performance)\n- Supports custom patterns\n- Auto-detection can be added later as convenience\n\n### Soft Error vs Hard Error for Missing Notes\n**Decision**: Soft error (return `found: false`)\n\n**Rationale**:\n- Missing daily notes are normal (users don't write every day)\n- Better UX for clients (handle gracefully without exceptions)\n- Consistent with search operations (empty results, not errors)\n\n## Future Enhancements\n\n### Phase 2: Multi-file Reading (after markdown-todo-extractor-718)\n- Add `include_content: bool` parameter to SearchDailyNotesRequest\n- When true, fetch full content for all matching notes\n- Use new multi-file reading capability from FileCapability\n- Update SearchDailyNotesResponse to optionally include content\n\n### Additional Enhancements\n- Auto-detect daily note patterns from vault structure\n- Support relative date queries (\"last 7 days\", \"this month\")\n- Cache discovered daily notes for performance\n- Support for alternative date formats (if commonly requested)","status":"open","priority":2,"issue_type":"feature","owner":"jeff@jeffutter.com","created_at":"2026-01-21T20:52:03.412562986-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-22T07:06:58.761607247-06:00","labels":["planned"],"dependencies":[{"issue_id":"markdown-todo-extractor-vw9","depends_on_id":"markdown-todo-extractor-718","type":"blocks","created_at":"2026-01-21T20:52:09.594952143-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-zxt","title":"Clarify ambiguous exclusion pattern matching semantics in config","description":"## Problem\n\nIn `src/config.rs`, the exclusion matching logic tries **both** glob patterns AND substring matching:\n\n```rust\npub fn should_exclude_path(\u0026self, path: \u0026Path) -\u003e bool {\n    let path_str = path.to_string_lossy();\n    for pattern_str in \u0026self.exclude_paths {\n        // Try to compile as glob pattern\n        if let Ok(pattern) = Pattern::new(pattern_str)\n            \u0026\u0026 pattern.matches(\u0026path_str)\n        {\n            return true;\n        }\n\n        // Also check if the path contains the pattern as a substring\n        if path_str.contains(pattern_str) {\n            return true;\n        }\n    }\n    false\n}\n```\n\nThis dual-matching causes confusion:\n\n1. **Pattern \"test\" matches as both**:\n   - Glob: matches file named \"test\"\n   - Substring: matches \"test\", \"testing\", \"my_test_file\", etc.\n\n2. **Users may expect only one behavior**:\n   - If they write `test`, do they expect exact match or substring?\n   - If they write `**/test/**`, they clearly want glob behavior\n\n3. **Order matters but shouldn't**:\n   - If glob pattern is invalid but matches as substring, it still works\n   - This makes debugging harder\n\n## Proposed Solutions\n\n**Option A: Explicit syntax differentiation**\n```toml\nexclude_paths = [\n    \"substring:Template\",     # Substring match\n    \"glob:**/Archive/**\"      # Glob pattern\n]\n```\n\n**Option B: Glob-only with documentation**\n- Only support glob patterns\n- Document that `*test*` should be used instead of `test`\n- Simpler, more predictable behavior\n\n**Option C: Substring-only for simple strings**\n- If pattern contains glob chars (`*`, `?`, `[`), treat as glob\n- Otherwise, treat as substring\n- Current behavior but documented\n\n## Locations\n\n- `src/config.rs` lines 74-94 (`should_exclude_path` method)\n\n## Estimated Impact\n\n- Clearer user experience\n- More predictable behavior\n- Reduced support confusion","status":"deferred","priority":3,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:48:49.456287699-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T10:36:35.664114774-06:00","labels":["simplification-opportunity"]}
