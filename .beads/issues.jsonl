{"id":"markdown-todo-extractor-5i0","title":"Implement CLI automatic registration for files operations","description":"Apply CLI automatic registration pattern to 2 file operations (list_files, read_file). Follow the same pattern used for tasks: add Parser derives to request structs, implement CliOperation for operation structs, update create_cli_operations() in capabilities/mod.rs.","status":"closed","priority":2,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T19:35:13.508211743-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T21:11:31.744212654-06:00","closed_at":"2026-01-20T21:11:31.744212654-06:00","close_reason":"Closed","dependencies":[{"issue_id":"markdown-todo-extractor-5i0","depends_on_id":"markdown-todo-extractor-b68","type":"blocks","created_at":"2026-01-20T19:35:29.746867336-06:00","created_by":"Jeffery Utter"}],"comments":[{"id":2,"issue_id":"markdown-todo-extractor-5i0","author":"Jeffery Utter","text":"Implementation Guide - Files Operations\n\nFollow the CLI automatic registration pattern documented in CLAUDE.md section \"Adding CLI Automatic Registration for an Operation\".\n\nReference: src/capabilities/tasks.rs SearchTasksOperation for working example\n\nDepends on: markdown-todo-extractor-b68 (tags operations must be converted first)\n\nOperations to convert (2 total):\n1. ListFilesOperation (list_files) - CLI name: \"list-files\"\n2. ReadFileOperation (read_file) - CLI name: \"read-file\"\n\nFor EACH operation:\n1. Add Parser derive to request struct (ListFilesRequest, ReadFileRequest)\n   - Import: use clap::{CommandFactory, FromArgMatches, Parser};\n   - Add Parser to #[derive(...)]\n   - Add #[command(name = \"...\", about = \"...\")]\n   - For ReadFileRequest: may need special handling since it takes vault_path + file_path\n   - Add #[arg(...)] to all existing fields\n\n2. Implement CliOperation trait for operation struct\n   - command_name() - use existing CLI_NAME constant\n   - get_command() - call RequestStruct::command()\n   - execute_from_args() - parse args, handle path, call capability method, return JSON\n\n3. Register in create_cli_operations() in src/capabilities/mod.rs\n   - Uncomment the Arc::new(...) lines for file operations\n\n4. Update src/cli.rs - remove manual routing\n   - Remove ListFiles/ReadFile from Commands enum\n   - Remove command structs\n   - Remove match arms from run_cli()\n   - Update the check at top of run_cli() to include these commands\n\n5. Once all operations converted, simplify run_cli()\n   - Remove the conditional check for specific commands\n   - Make ALL commands use automatic routing\n   - Remove the old match statement entirely\n   - Keep only the automatic routing path\n\nTesting:\ncargo run -- list-files /tmp/test-vault --max-depth 2\ncargo run -- read-file /tmp/test-vault test.md","created_at":"2026-01-21T01:43:17Z"}]}
{"id":"markdown-todo-extractor-6uj","title":"Implement CLI output trait for custom formatting","description":"Create trait/function to allow custom output formats (table, yaml, etc.) instead of JSON-only. This would allow a --format flag on commands. Should work with all CLI operations that use automatic registration.","status":"open","priority":3,"issue_type":"feature","owner":"jeff@jeffutter.com","created_at":"2026-01-20T19:35:18.972182877-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T19:35:18.972182877-06:00","dependencies":[{"issue_id":"markdown-todo-extractor-6uj","depends_on_id":"markdown-todo-extractor-b68","type":"blocks","created_at":"2026-01-20T19:35:34.167968323-06:00","created_by":"Jeffery Utter"},{"issue_id":"markdown-todo-extractor-6uj","depends_on_id":"markdown-todo-extractor-5i0","type":"blocks","created_at":"2026-01-20T19:35:34.195716504-06:00","created_by":"Jeffery Utter"}],"comments":[{"id":3,"issue_id":"markdown-todo-extractor-6uj","author":"Jeffery Utter","text":"Implementation Guide - CLI Output Trait\n\nCreate a trait/system to allow custom output formats beyond JSON.\n\nDepends on: All CLI operations must use automatic registration first (b68, 5i0)\n\nGoal: Add --format flag to allow output in different formats:\n- json (default, current behavior)\n- table (pretty-printed table for humans)\n- yaml (YAML format)\n- csv (for operations that return lists)\n\nSuggested Approach:\n\n1. Create OutputFormat trait in src/cli_router.rs or new file:\n   trait OutputFormat {\n       fn format(\u0026self, data: \u0026impl Serialize) -\u003e Result\u003cString, Box\u003cdyn Error\u003e\u003e;\n   }\n\n2. Implement for different formats:\n   - JsonFormat (current behavior)\n   - TableFormat (using prettytable-rs or comfy-table)\n   - YamlFormat (using serde_yaml)\n   - CsvFormat (using csv crate, for list-type responses)\n\n3. Add --format global flag to cli_router::build_cli()\n\n4. Update CliOperation trait:\n   - Add format parameter to execute_from_args()\n   - Or return raw data and let router handle formatting\n\n5. Update cli_router::execute_cli() to:\n   - Parse --format flag\n   - Create appropriate formatter\n   - Apply formatter to operation output\n\nDesign Decision: \nShould formatting happen IN each operation or AFTER in the router?\n- After in router = cleaner, operations return structured data\n- In operation = more flexible, operation-specific formatting\n\nRecommend: Router handles formatting (cleaner separation of concerns)\n\nTesting:\ncargo run -- tasks /tmp/test-vault --format json\ncargo run -- tasks /tmp/test-vault --format table\ncargo run -- list-tags /tmp/test-vault --format yaml","created_at":"2026-01-21T01:43:36Z"}]}
{"id":"markdown-todo-extractor-b68","title":"Implement CLI automatic registration for tags operations","description":"Apply CLI automatic registration pattern to 3 tag operations (extract_tags, list_tags, search_by_tags). Follow the same pattern used for tasks: add Parser derives to request structs, implement CliOperation for operation structs, update create_cli_operations() in capabilities/mod.rs.","status":"closed","priority":2,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T19:35:09.492684048-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T20:35:58.007247413-06:00","closed_at":"2026-01-20T20:35:58.007247413-06:00","close_reason":"Closed","comments":[{"id":1,"issue_id":"markdown-todo-extractor-b68","author":"Jeffery Utter","text":"Implementation Guide - Tags Operations\n\nFollow the CLI automatic registration pattern documented in CLAUDE.md section \"Adding CLI Automatic Registration for an Operation\".\n\nReference: src/capabilities/tasks.rs SearchTasksOperation for working example\n\nOperations to convert (3 total):\n1. ExtractTagsOperation (extract_tags) - CLI name: \"tags\"\n2. ListTagsOperation (list_tags) - CLI name: \"list-tags\"  \n3. SearchByTagsOperation (search_by_tags) - CLI name: \"search-by-tags\"\n\nFor EACH operation:\n1. Add Parser derive to request struct (ExtractTagsRequest, ListTagsRequest, SearchByTagsRequest)\n   - Import: use clap::{CommandFactory, FromArgMatches, Parser};\n   - Add Parser to #[derive(...)]\n   - Add #[command(name = \"...\", about = \"...\")] \n   - Add path field: path: Option\u003cPathBuf\u003e with proper annotations\n   - Add #[arg(...)] to all existing fields\n\n2. Implement CliOperation trait for operation struct\n   - command_name() - use existing CLI_NAME constant\n   - get_command() - call RequestStruct::command()\n   - execute_from_args() - parse args, handle path, call capability method, return JSON\n\n3. Register in create_cli_operations() in src/capabilities/mod.rs\n   - Uncomment the Arc::new(...) lines for tag operations\n\n4. Update src/cli.rs - remove manual routing\n   - Remove Tags/ListTags/SearchByTags from Commands enum\n   - Remove command structs (if any separate from request structs)\n   - Remove match arms from run_cli()\n   - Update the check at top of run_cli() to include these commands\n\nTesting:\ncargo run -- tags /tmp/test-vault\ncargo run -- list-tags /tmp/test-vault --min-count 2\ncargo run -- search-by-tags /tmp/test-vault --tags work","created_at":"2026-01-21T01:41:36Z"}]}
{"id":"markdown-todo-extractor-sdi","title":"Change file list output to visual rather than json","description":"Change the format of the file list output to an indented list.\n\nDirectory structure format:\n- One entry per line\n- Directories end with /\n- Children are indented 2 spaces relative to their parent\n- Files have no trailing slash\n\n## Example:\n\n```\nproject/\n  src/\n    main.py\n    utils.py\n  README.md\n```\n\n## Implementation Plan\n\n### 1. Create Visual Tree Formatter Function\n**Location**: `src/capabilities/files.rs`\n\nAdd a new function `format_tree_visual()` that converts a `FileTreeNode` to the indented visual format:\n\n```rust\nfn format_tree_visual(node: \u0026FileTreeNode, indent_level: usize) -\u003e String {\n    let mut output = String::new();\n    let indent = \"  \".repeat(indent_level);\n    \n    // Add current node\n    if node.is_directory {\n        output.push_str(\u0026format!(\"{}{}/\\n\", indent, node.name));\n    } else {\n        output.push_str(\u0026format!(\"{}{}\\n\", indent, node.name));\n    }\n    \n    // Recursively add children\n    for child in \u0026node.children {\n        output.push_str(\u0026format_tree_visual(child, indent_level + 1));\n    }\n    \n    output\n}\n```\n\n**Key implementation details**:\n- Use 2 spaces per indent level (not tabs)\n- Directories get a trailing `/`\n- Files have no trailing slash\n- Recursively process children with incremented indent level\n\n### 2. Add Visual Output Option to Response\n**Location**: `src/capabilities/files.rs`\n\nTwo approaches to consider:\n\n**Option A: Replace JSON response entirely**\n- Change `ListFilesResponse` to contain a `visual_tree: String` field\n- Remove or make optional the `root: FileTreeNode` field\n- CLI always outputs visual format\n- HTTP/MCP also outputs visual format\n\n**Option B: Add format parameter (more flexible)**\n- Add `format: Option\u003cString\u003e` to `ListFilesRequest` (values: \"json\", \"visual\")\n- Keep existing `FileTreeNode` structure\n- Add optional `visual_tree: Option\u003cString\u003e` to `ListFilesResponse`\n- Let caller choose format\n\n**Recommendation**: Start with Option A for simplicity. Can add Option B later if needed.\n\n### 3. Update CLI Output\n**Location**: `src/capabilities/files.rs:341-362` (ListFilesOperation CLI impl)\n\nChange the CLI operation to output the visual tree directly:\n\n```rust\nasync fn execute_from_args(...) -\u003e Result\u003cString, Box\u003cdyn std::error::Error\u003e\u003e {\n    // ... existing request parsing ...\n    \n    let response = /* ... get response ... */;\n    \n    // Output visual tree instead of JSON\n    Ok(response.visual_tree)\n}\n```\n\n**Current code**: Line 361 serializes to JSON\n**New code**: Return the visual tree string directly\n\n### 4. Update list_files Method\n**Location**: `src/capabilities/files.rs:121-177`\n\nAfter building the file tree with `build_file_tree()`:\n\n```rust\n// Build the file tree (existing code)\nlet (root, total_files, total_directories) = build_file_tree(...)?;\n\n// Generate visual representation\nlet visual_tree = format_tree_visual(\u0026root, 0);\n\nOk(ListFilesResponse {\n    visual_tree,\n    total_files,\n    total_directories,\n})\n```\n\n### 5. Update Response Struct\n**Location**: `src/capabilities/files.rs:56-62`\n\n```rust\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct ListFilesResponse {\n    pub visual_tree: String,\n    pub total_files: usize,\n    pub total_directories: usize,\n}\n```\n\n### 6. Handle Root Name Edge Case\nThe root directory name needs special handling:\n- If listing vault root, show the vault's directory name\n- If listing a subpath, show that subpath's name\n- Ensure consistent behavior with how `build_file_tree()` names the root\n\n### 7. Testing Strategy\n\nManual testing commands:\n```bash\n# Test basic listing\ncargo run -- list-files /path/to/vault\n\n# Test subpath\ncargo run -- list-files /path/to/vault --path \"subfolder\"\n\n# Test max depth\ncargo run -- list-files /path/to/vault --max-depth 2\n```\n\nExpected output format:\n```\nvault/\n  folder1/\n    file1.md\n    file2.md\n  folder2/\n    nested/\n      deep.md\n  root-file.md\n```\n\n### 8. Files to Modify\n\n1. `src/capabilities/files.rs`:\n   - Add `format_tree_visual()` function (~20 lines)\n   - Update `ListFilesResponse` struct (1 line change)\n   - Update `list_files()` method (add 2 lines)\n   - Update CLI `execute_from_args()` (change line 361)\n\n### 9. Backward Compatibility Considerations\n\n**Breaking changes**:\n- HTTP/MCP clients expecting JSON structure will break\n- This is acceptable if no external clients exist yet\n\n**If backward compatibility needed**:\n- Use Option B (format parameter) instead\n- Default to \"visual\" for CLI\n- Keep \"json\" as default for HTTP/MCP initially\n\n### 10. Edge Cases to Handle\n\n1. **Empty directories**: Still show with `/` suffix\n2. **Single file**: Should not have indent (at root level)\n3. **Deep nesting**: Verify indent math is correct\n4. **Special characters in names**: Ensure proper display\n5. **Excluded paths**: Already handled by `build_file_tree()`\n\n### Estimated Complexity\n- Low complexity change\n- ~30 lines of new code\n- ~5 lines of modifications to existing code\n- Main work is the formatter function and testing","status":"closed","priority":2,"issue_type":"feature","owner":"jeff@jeffutter.com","created_at":"2026-01-20T12:28:37.799886872-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T22:02:51.313707474-06:00","closed_at":"2026-01-20T22:02:51.313707474-06:00","close_reason":"Closed"}
{"id":"markdown-todo-extractor-tx9","title":"Create Additional Tools","description":"# Create Additional Tools\n\nThis epic encompasses the creation of additional LLM tools to enhance the knowledge base interaction capabilities. The tools extend the existing RAG search functionality to provide more granular access to vault contents.\n\n## Overview\n\nThis epic adds four complementary tools that give the LLM more ways to explore and access the Obsidian vault:\n\n| Tool | Purpose | Child Ticket |\n|------|---------|--------------|\n| **read_file** | Read the full content of a specific file | tx9.2 |\n| **list_files** | Browse the directory structure | tx9.3 |\n| **list_tags** | Discover available tags with statistics | tx9.4 |\n| **search_by_tags** | Find files matching specific tags | tx9.5 |\n\n## Implementation Plan\n\n### Architecture Approach\n\nAll four tools follow the existing patterns established in `src/mcp.rs`:\n\n1. **MCP Tool Registration**: Use the `#[tool]` macro from `rmcp` to register each tool with the `TaskSearchService`\n2. **REST API Endpoints**: Add corresponding HTTP handlers in `main.rs` for the HTTP MCP server mode\n3. **Shared Extractors**: Reuse and extend existing extractors (`TaskExtractor`, `TagExtractor`) where applicable\n4. **Configuration Integration**: All tools should respect the existing `Config` path exclusion patterns\n\n### File Organization\n\nThe implementation will modify existing modules following the current structure:\n\n| File | New Additions |\n|------|---------------|\n| `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs` | 4 new tool methods, 8 new request/response structs |\n| `/home/jeffutter/src/markdown-todo-extractor/src/main.rs` | 4 new HTTP handler pairs, 4 new routes, updated tools_handler |\n| `/home/jeffutter/src/markdown-todo-extractor/src/tag_extractor.rs` | `TagCount` struct, `TaggedFile` struct, `extract_tags_with_counts()`, `search_by_tags()`, Config integration |\n| `/home/jeffutter/src/markdown-todo-extractor/src/cli.rs` | `SearchByTags` subcommand |\n| `/home/jeffutter/src/markdown-todo-extractor/Cargo.toml` | `tempfile` dev dependency |\n\n### Common Patterns\n\nAll tools should:\n\n1. **Accept paths relative to the base path** (vault root) with optional subpath parameter\n2. **Return JSON responses** with consistent error handling via `ErrorData` for MCP and HTTP status codes for REST\n3. **Support both MCP stdio and HTTP modes** with identical functionality\n4. **Include comprehensive tool descriptions** for LLM consumption via `#[tool(description = \"...\")]`\n5. **Use `JsonSchema` derive** for automatic schema generation\n6. **Respect `Config.exclude_paths`** for path exclusions (critical for tx9.3, tx9.4, tx9.5)\n\n### Shared Infrastructure Changes\n\nBefore implementing individual tools, these cross-cutting changes are required:\n\n#### 1. TagExtractor Config Integration (Required by tx9.4, tx9.5)\n\nThe current `TagExtractor` is a simple unit struct without configuration. It needs to be updated to:\n- Accept `Arc\u003cConfig\u003e` in constructor (matching `TaskExtractor` pattern)\n- Pass config to `collect_markdown_files()` function\n- Apply path exclusions during file collection\n\nThis change affects:\n- `src/tag_extractor.rs`: Add config field and update methods\n- `src/mcp.rs`: Update `TaskSearchService::new()` to pass config\n- `src/main.rs`: Update `AppState` initialization\n- `src/cli.rs`: Update Tags command to create TagExtractor with config\n\n#### 2. Dev Dependency Addition\n\nAdd `tempfile = \"3\"` to `Cargo.toml` for unit tests across all tools.\n\n### Execution Order\n\nThe tools can be implemented in parallel, but for optimized development with code reuse:\n\n```\nPhase 1 (Independent - can run in parallel):\n  ├── tx9.2: read_file tool (standalone, no dependencies)\n  └── tx9.3: list_files tool (standalone, no dependencies)\n\nPhase 2 (After TagExtractor Config Integration):\n  ├── tx9.4: list_tags tool (extends TagExtractor with counting)\n  └── tx9.5: search_by_tags tool (extends TagExtractor with search)\n```\n\n**Recommended single-developer sequence:**\n1. **tx9.2 (read_file)** - Simplest tool, establishes the pattern\n2. **tx9.3 (list_files)** - Standalone, no extractor dependencies\n3. **TagExtractor Config Integration** - Shared infrastructure for tx9.4 and tx9.5\n4. **tx9.4 (list_tags)** - Adds counting to TagExtractor\n5. **tx9.5 (search_by_tags)** - Can reuse tag extraction logic from tx9.4\n\n### Security Considerations\n\nAll file-reading tools (tx9.2, tx9.3) must implement path traversal protection:\n\n```rust\n// Pattern for all file-accessing tools\nlet canonical_base = self.base_path.canonicalize()?;\nlet canonical_full = full_path.canonicalize()?;\n\nif !canonical_full.starts_with(\u0026canonical_base) {\n    return Err(ErrorData {\n        code: ErrorCode(-32602),\n        message: Cow::from(\"Invalid path: path must be within the vault\"),\n        data: None,\n    });\n}\n```\n\nFor tx9.2 (read_file), additionally restrict to `.md` files only.\n\n### Testing Strategy\n\nEach tool should include:\n\n1. **Unit tests** for core logic (in respective module)\n2. **Integration tests** for MCP tool invocation (if feasible)\n3. **Manual testing** with CLI and MCP stdio modes\n\nCommon test scenarios across all tools:\n- Happy path with valid inputs\n- Empty/missing inputs handled gracefully\n- Path exclusions respected (tx9.3, tx9.4, tx9.5)\n- Security validation (path traversal blocked in tx9.2, tx9.3)\n- Large vault handling (tx9.3 may need truncation)\n\n### API Endpoint Summary\n\nAfter all tools are implemented, the HTTP server will expose:\n\n| Endpoint | Tool | Method |\n|----------|------|--------|\n| `/api/tasks` | search_tasks | GET/POST |\n| `/api/tags` | extract_tags | GET/POST |\n| `/api/file` | read_file | GET/POST |\n| `/api/files` | list_files | GET/POST |\n| `/api/tags/list` | list_tags | GET/POST |\n| `/api/search_by_tags` | search_by_tags | GET/POST |\n\n### Success Criteria\n\n- [ ] All four tools implemented with MCP and HTTP interfaces\n- [ ] All tools respect path exclusion configuration\n- [ ] Security: Path traversal attacks blocked\n- [ ] Unit tests pass for all new functionality\n- [ ] `cargo build --release` succeeds\n- [ ] `cargo clippy` passes\n- [ ] `cargo fmt --check` passes\n- [ ] Manual testing confirms tools work in both stdio and HTTP modes\n\n### Child Ticket Summary\n\nEach child ticket has a detailed implementation plan:\n\n| Ticket | Status | Key Implementation Details |\n|--------|--------|---------------------------|\n| **tx9.2** | Planned | `ReadFileRequest`/`ReadFileResponse`, path validation, .md restriction |\n| **tx9.3** | Planned | `ListFilesRequest`/`ListFilesResponse`/`FileTreeNode`, hierarchical tree, size limiting |\n| **tx9.4** | Planned | `TagCount` struct, `extract_tags_with_counts()`, sort by frequency |\n| **tx9.5** | Planned | `TaggedFile` struct, `search_by_tags()`, AND/OR logic, CLI subcommand |\n\nSee individual child tickets for complete implementation details.","status":"closed","priority":2,"issue_type":"epic","owner":"jeff@jeffutter.com","created_at":"2026-01-19T18:43:26.983361979-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-19T22:35:17.868790617-06:00","closed_at":"2026-01-19T22:35:17.868790617-06:00","close_reason":"Closed","labels":["planned"]}
{"id":"markdown-todo-extractor-tx9.2","title":"Create File Read Tool","description":"# Create File Read Tool\n\nCreate a tool that returns the contents of a file at a given path.\n\n## Requirements\n\n- The tool should return the contents of the file at a given path\n- See this for ideas: https://github.com/logancyang/obsidian-copilot/blob/master/src/tools/NoteTools.ts\n\n---\n\n## Implementation Plan\n\n### Overview\n\nThis tool will allow MCP clients to read the full contents of a markdown file from the Obsidian vault. The implementation follows the existing patterns in the codebase for MCP tools (see `search_tasks` and `extract_tags` in `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs`).\n\n### Design Decisions\n\n1. **Path Handling**: Accept a relative path (relative to the vault base path). This follows the pattern used by Obsidian Copilot's NoteTools.ts which requires paths relative to vault root.\n\n2. **Security**: Validate that the resolved path is within the base path to prevent path traversal attacks (e.g., `../../../etc/passwd`).\n\n3. **File Extension**: Only allow reading `.md` files to stay consistent with the tool's purpose as a markdown task extractor.\n\n4. **Response Format**: Return a structured response with:\n   - `content`: The file contents\n   - `file_path`: The resolved path (relative to vault)\n   - `file_name`: The file name only\n   - Optionally: metadata like modification time\n\n5. **Error Handling**: Follow existing patterns using `ErrorData` with appropriate error codes:\n   - Invalid path (path traversal attempt)\n   - File not found\n   - File not a markdown file\n   - Read error\n\n6. **No Chunking Initially**: Unlike Obsidian Copilot which chunks large files into 200-line segments, start with a simpler implementation that returns the full file. Chunking can be added later if needed for very large files.\n\n### Implementation Steps\n\n#### Step 1: Add Request/Response Types in `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs`\n\nAdd new structs after the existing `ExtractTagsResponse`:\n\n```rust\n/// Parameters for the read_file tool\n#[derive(Debug, Deserialize, JsonSchema)]\npub struct ReadFileRequest {\n    #[schemars(description = \"Path to the file relative to the vault root (e.g., 'Notes/my-note.md')\")]\n    pub path: String,\n}\n\n/// Response for the read_file tool\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct ReadFileResponse {\n    /// The full content of the file\n    pub content: String,\n    /// The file path relative to the vault root\n    pub file_path: String,\n    /// Just the file name\n    pub file_name: String,\n}\n```\n\n#### Step 2: Add the MCP Tool Method in `TaskSearchService`\n\nAdd a new method with the `#[tool]` attribute inside the `#[tool_router] impl TaskSearchService` block:\n\n```rust\n#[tool(\n    description = \"Read the full contents of a markdown file from the vault\"\n)]\nasync fn read_file(\n    \u0026self,\n    Parameters(request): Parameters\u003cReadFileRequest\u003e,\n) -\u003e Result\u003cJson\u003cReadFileResponse\u003e, ErrorData\u003e {\n    // 1. Construct the full path\n    let requested_path = PathBuf::from(\u0026request.path);\n    let full_path = self.base_path.join(\u0026requested_path);\n\n    // 2. Canonicalize paths for security check\n    let canonical_base = self.base_path.canonicalize().map_err(|e| ErrorData {\n        code: ErrorCode(-32603),\n        message: Cow::from(format!(\"Failed to resolve base path: {}\", e)),\n        data: None,\n    })?;\n\n    let canonical_full = full_path.canonicalize().map_err(|e| ErrorData {\n        code: ErrorCode(-32602), // Invalid params\n        message: Cow::from(format!(\"File not found: {}\", request.path)),\n        data: None,\n    })?;\n\n    // 3. Security: Ensure path is within base directory\n    if !canonical_full.starts_with(\u0026canonical_base) {\n        return Err(ErrorData {\n            code: ErrorCode(-32602),\n            message: Cow::from(\"Invalid path: path must be within the vault\"),\n            data: None,\n        });\n    }\n\n    // 4. Validate it's a markdown file\n    if canonical_full.extension().and_then(|s| s.to_str()) != Some(\"md\") {\n        return Err(ErrorData {\n            code: ErrorCode(-32602),\n            message: Cow::from(\"Invalid file type: only .md files can be read\"),\n            data: None,\n        });\n    }\n\n    // 5. Read the file content\n    let content = std::fs::read_to_string(\u0026canonical_full).map_err(|e| ErrorData {\n        code: ErrorCode(-32603),\n        message: Cow::from(format!(\"Failed to read file: {}\", e)),\n        data: None,\n    })?;\n\n    // 6. Get relative path for response\n    let relative_path = canonical_full\n        .strip_prefix(\u0026canonical_base)\n        .unwrap_or(\u0026canonical_full)\n        .to_string_lossy()\n        .to_string();\n\n    let file_name = canonical_full\n        .file_name()\n        .unwrap_or_default()\n        .to_string_lossy()\n        .to_string();\n\n    Ok(Json(ReadFileResponse {\n        content,\n        file_path: relative_path,\n        file_name,\n    }))\n}\n```\n\n#### Step 3: Add HTTP REST Endpoint (Optional but Recommended)\n\nFor consistency with existing endpoints, add REST handlers in `/home/jeffutter/src/markdown-todo-extractor/src/main.rs`:\n\n1. Add handler functions:\n\n```rust\n/// HTTP handler for reading a file (GET with query params)\nasync fn file_handler_get(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    query: axum::extract::Query\u003cmcp::ReadFileRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ReadFileResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    read_file_impl(state, query.0).await\n}\n\n/// HTTP handler for reading a file (POST with JSON body)\nasync fn file_handler_post(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    axum::Json(request): axum::Json\u003cmcp::ReadFileRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ReadFileResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    read_file_impl(state, request).await\n}\n\n/// Shared implementation for file reading\nasync fn read_file_impl(\n    state: AppState,\n    request: mcp::ReadFileRequest,\n) -\u003e Result\u003caxum::Json\u003cmcp::ReadFileResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    // Similar validation logic as the MCP handler\n    let requested_path = std::path::PathBuf::from(\u0026request.path);\n    let full_path = state.base_path.join(\u0026requested_path);\n\n    // Security: canonicalize and check path\n    let canonical_base = state.base_path.canonicalize().map_err(|e| {\n        (axum::http::StatusCode::INTERNAL_SERVER_ERROR, format!(\"Failed to resolve base path: {}\", e))\n    })?;\n\n    let canonical_full = full_path.canonicalize().map_err(|_| {\n        (axum::http::StatusCode::NOT_FOUND, format!(\"File not found: {}\", request.path))\n    })?;\n\n    if !canonical_full.starts_with(\u0026canonical_base) {\n        return Err((axum::http::StatusCode::BAD_REQUEST, \"Invalid path: path must be within the vault\".to_string()));\n    }\n\n    if canonical_full.extension().and_then(|s| s.to_str()) != Some(\"md\") {\n        return Err((axum::http::StatusCode::BAD_REQUEST, \"Invalid file type: only .md files can be read\".to_string()));\n    }\n\n    let content = std::fs::read_to_string(\u0026canonical_full).map_err(|e| {\n        (axum::http::StatusCode::INTERNAL_SERVER_ERROR, format!(\"Failed to read file: {}\", e))\n    })?;\n\n    let relative_path = canonical_full\n        .strip_prefix(\u0026canonical_base)\n        .unwrap_or(\u0026canonical_full)\n        .to_string_lossy()\n        .to_string();\n\n    let file_name = canonical_full\n        .file_name()\n        .unwrap_or_default()\n        .to_string_lossy()\n        .to_string();\n\n    Ok(axum::Json(mcp::ReadFileResponse {\n        content,\n        file_path: relative_path,\n        file_name,\n    }))\n}\n```\n\n2. Add route in router:\n\n```rust\n.route(\n    \"/api/file\",\n    axum::routing::get(file_handler_get).post(file_handler_post),\n)\n```\n\n3. Update `tools_handler` to include the new tool schema.\n\n4. Update console output messages.\n\n#### Step 4: Add Unit Tests\n\nAdd tests in a new `#[cfg(test)]` module in `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs` or create a separate test file:\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::fs;\n    use tempfile::TempDir;\n\n    #[tokio::test]\n    async fn test_read_file_success() {\n        let temp_dir = TempDir::new().unwrap();\n        let file_path = temp_dir.path().join(\"test.md\");\n        fs::write(\u0026file_path, \"# Test\\n\\nContent here\").unwrap();\n\n        let service = TaskSearchService::new(temp_dir.path().to_path_buf());\n        let request = ReadFileRequest {\n            path: \"test.md\".to_string(),\n        };\n\n        let result = service.read_file(Parameters(request)).await;\n        assert!(result.is_ok());\n\n        let response = result.unwrap().0;\n        assert_eq!(response.content, \"# Test\\n\\nContent here\");\n        assert_eq!(response.file_name, \"test.md\");\n    }\n\n    #[tokio::test]\n    async fn test_read_file_not_found() {\n        let temp_dir = TempDir::new().unwrap();\n        let service = TaskSearchService::new(temp_dir.path().to_path_buf());\n        let request = ReadFileRequest {\n            path: \"nonexistent.md\".to_string(),\n        };\n\n        let result = service.read_file(Parameters(request)).await;\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_read_file_path_traversal_blocked() {\n        let temp_dir = TempDir::new().unwrap();\n        let service = TaskSearchService::new(temp_dir.path().to_path_buf());\n        let request = ReadFileRequest {\n            path: \"../../../etc/passwd\".to_string(),\n        };\n\n        let result = service.read_file(Parameters(request)).await;\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_read_file_non_markdown_rejected() {\n        let temp_dir = TempDir::new().unwrap();\n        let file_path = temp_dir.path().join(\"test.txt\");\n        fs::write(\u0026file_path, \"Not markdown\").unwrap();\n\n        let service = TaskSearchService::new(temp_dir.path().to_path_buf());\n        let request = ReadFileRequest {\n            path: \"test.txt\".to_string(),\n        };\n\n        let result = service.read_file(Parameters(request)).await;\n        assert!(result.is_err());\n    }\n}\n```\n\nAdd `tempfile` as a dev dependency in `/home/jeffutter/src/markdown-todo-extractor/Cargo.toml`:\n\n```toml\n[dev-dependencies]\ntempfile = \"3\"\n```\n\n### Files to Modify\n\n| File | Changes |\n|------|---------|\n| `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs` | Add `ReadFileRequest`, `ReadFileResponse` structs and `read_file` tool method |\n| `/home/jeffutter/src/markdown-todo-extractor/src/main.rs` | Add HTTP handlers and route for `/api/file` endpoint |\n| `/home/jeffutter/src/markdown-todo-extractor/Cargo.toml` | Add `tempfile` dev dependency for tests |\n\n### Testing Strategy\n\n1. **Unit Tests**: Test the core logic with various scenarios (success, not found, path traversal, non-markdown)\n2. **Manual Testing**: Test with actual MCP client (Claude Desktop or similar)\n   ```bash\n   # Start the server\n   cargo run -- --mcp-stdio /path/to/vault\n   \n   # Or HTTP mode\n   cargo run -- --mcp-http /path/to/vault\n   ```\n\n### Future Enhancements (Out of Scope)\n\n- **Chunking**: For very large files, add optional `chunk_index` parameter similar to Obsidian Copilot\n- **Link Extraction**: Parse and return wiki-style links from the content\n- **Metadata Extraction**: Return YAML frontmatter separately parsed\n- **Line Range**: Allow reading specific line ranges from a file","status":"closed","priority":2,"issue_type":"feature","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-19T18:49:51.850252653-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-19T22:27:03.960128433-06:00","closed_at":"2026-01-19T22:27:03.960128433-06:00","close_reason":"Closed","labels":["planned"],"dependencies":[{"issue_id":"markdown-todo-extractor-tx9.2","depends_on_id":"markdown-todo-extractor-tx9","type":"parent-child","created_at":"2026-01-19T18:49:51.855943654-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-tx9.3","title":"Create file list tool","description":"# Create file list tool\n\nCreate a tool to list the directory tree of the obsidian vault.\n\n- The tree should include all files and folders.\n- Reference this for ideas: https://github.com/logancyang/obsidian-copilot/blob/master/src/tools/FileTreeTools.ts\n\nThis tool will provide the LLM with a complete view of the vault's file structure, enabling it to:\n- Understand the organization of notes and folders\n- Navigate to specific files or folders\n- Discover what documents exist in a particular area\n\n## Implementation Plan\n\n### 1. Add Request/Response Types in `src/mcp.rs`\n\n```rust\n/// Parameters for the list_files tool\n#[derive(Debug, Deserialize, JsonSchema)]\npub struct ListFilesRequest {\n    #[schemars(description = \"Subpath within the vault to list (optional, defaults to vault root)\")]\n    pub path: Option\u003cString\u003e,\n    \n    #[schemars(description = \"Maximum depth to traverse (optional, defaults to unlimited)\")]\n    pub max_depth: Option\u003cusize\u003e,\n    \n    #[schemars(description = \"Include file sizes in output (optional, defaults to false)\")]\n    pub include_sizes: Option\u003cbool\u003e,\n}\n\n/// A node in the file tree\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct FileTreeNode {\n    pub name: String,\n    pub path: String,\n    pub is_directory: bool,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub size_bytes: Option\u003cu64\u003e,\n    #[serde(skip_serializing_if = \"Vec::is_empty\", default)]\n    pub children: Vec\u003cFileTreeNode\u003e,\n}\n\n/// Response for the list_files tool\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct ListFilesResponse {\n    pub root: FileTreeNode,\n    pub total_files: usize,\n    pub total_directories: usize,\n}\n```\n\n### 2. Implement MCP Tool Method in `TaskSearchService`\n\n```rust\n#[tool(description = \"List the directory tree of the vault. Returns a hierarchical view of all files and folders. Useful for understanding vault structure and finding files.\")]\nasync fn list_files(\n    \u0026self,\n    Parameters(request): Parameters\u003cListFilesRequest\u003e,\n) -\u003e Result\u003cJson\u003cListFilesResponse\u003e, ErrorData\u003e {\n    // Resolve the search path\n    let search_path = if let Some(ref subpath) = request.path {\n        self.base_path.join(subpath)\n    } else {\n        self.base_path.clone()\n    };\n    \n    // Validate path is within vault\n    // Build the file tree recursively\n    // Respect Config path exclusions\n    // Apply max_depth if specified\n}\n```\n\n### 3. Helper Function for Tree Building\n\nAdd a helper function to recursively build the tree:\n\n```rust\nfn build_file_tree(\n    path: \u0026Path,\n    base_path: \u0026Path,\n    config: \u0026Config,\n    current_depth: usize,\n    max_depth: Option\u003cusize\u003e,\n    include_sizes: bool,\n) -\u003e Result\u003c(FileTreeNode, usize, usize), Box\u003cdyn std::error::Error\u003e\u003e {\n    // Check depth limit\n    // Check if path should be excluded via config\n    // Read directory entries\n    // Recursively process subdirectories\n    // Collect file entries\n    // Return node with accumulated counts\n}\n```\n\n### 4. Output Format Considerations\n\nTwo output format options:\n\n**Option A: Hierarchical (Recommended)**\n```json\n{\n  \"root\": {\n    \"name\": \"vault\",\n    \"path\": \"\",\n    \"is_directory\": true,\n    \"children\": [\n      {\"name\": \"Projects\", \"path\": \"Projects\", \"is_directory\": true, \"children\": [...]},\n      {\"name\": \"note.md\", \"path\": \"note.md\", \"is_directory\": false}\n    ]\n  },\n  \"total_files\": 150,\n  \"total_directories\": 25\n}\n```\n\n**Option B: Flat List (Alternative)**\n```json\n{\n  \"files\": [\"Projects/plan.md\", \"Notes/idea.md\"],\n  \"directories\": [\"Projects\", \"Notes\"],\n  \"total_files\": 150,\n  \"total_directories\": 25\n}\n```\n\nUse hierarchical format as it better represents the tree structure and matches the reference implementation.\n\n### 5. Size Management\n\nLike the obsidian-copilot reference, implement size limits:\n- If the JSON response exceeds a threshold (e.g., 500KB), simplify by:\n  - Omitting file lists and showing only directory structure\n  - Or truncating at a certain depth\n- Add a `truncated` field to indicate if output was limited\n\n### 6. Add HTTP Endpoint in `main.rs`\n\n```rust\n/// HTTP handler for listing files (GET)\nasync fn list_files_handler_get(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    query: axum::extract::Query\u003cmcp::ListFilesRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ListFilesResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    list_files_impl(state, query.0).await\n}\n\n/// HTTP handler for listing files (POST)\nasync fn list_files_handler_post(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    axum::Json(request): axum::Json\u003cmcp::ListFilesRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ListFilesResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    list_files_impl(state, request).await\n}\n```\n\nAdd route:\n```rust\n.route(\n    \"/api/files\",\n    axum::routing::get(list_files_handler_get).post(list_files_handler_post),\n)\n```\n\n### 7. Update Tools Handler\n\nAdd to `/tools` endpoint:\n```rust\nlet list_files_schema = schema_for!(ListFilesRequest);\n// Add to tools array\n```\n\n### 8. Configuration Integration\n\nThe tool should respect the existing `Config.exclude_paths` patterns, filtering out excluded directories and files from the tree output.\n\n### 9. Testing\n\n- Test tree building for nested directories\n- Test max_depth limiting\n- Test path exclusion via Config\n- Test empty directory handling\n- Test size limiting for large vaults\n\n### Files to Modify\n\n1. `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs` - Add tool and types\n2. `/home/jeffutter/src/markdown-todo-extractor/src/main.rs` - Add HTTP handlers and routes","status":"closed","priority":2,"issue_type":"feature","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-19T18:50:16.158158253-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-19T22:29:03.667089571-06:00","closed_at":"2026-01-19T22:29:03.667089571-06:00","close_reason":"Closed","labels":["planned"],"dependencies":[{"issue_id":"markdown-todo-extractor-tx9.3","depends_on_id":"markdown-todo-extractor-tx9","type":"parent-child","created_at":"2026-01-19T18:50:16.15909786-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-tx9.4","title":"Tag List Tool","description":"# Tag List Tool\n\nCreate a tool to list all tags with their document counts.\n\n## Requirements\n\n- In the list, include the tag name and number of documents that reference it.\n- Tags should be pulled from the vault\n- Tags are in the YAML frontmatter of the files in the `tags` key\n- Reference this for ideas: https://github.com/logancyang/obsidian-copilot/blob/master/src/tools/TagTools.ts\n\nThis tool will provide the LLM with a complete inventory of all tags in the vault, along with statistics on how many documents reference each tag. This enables:\n- Understanding the tag taxonomy used in the vault\n- Finding commonly used vs. rarely used tags\n- Discovering topics with the most content\n- Supporting tag-based search and filtering decisions\n\n## Implementation Plan\n\n### Overview\n\nAdd a new `list_tags` MCP tool that returns all tags from YAML frontmatter with document counts. The existing `extract_tags` tool returns only unique tag names; this new tool adds statistics. This follows the pattern established by the Obsidian Copilot TagTools.ts reference, but is focused on frontmatter tags only (as specified in the requirements).\n\n### 1. Add `TagCount` Struct and Counting Method to `src/tag_extractor.rs`\n\nAdd a new struct to hold tag statistics and a method to extract tags with counts.\n\n**Add imports at the top of the file:**\n```rust\nuse schemars::JsonSchema;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n```\n\n**Add the `TagCount` struct after the existing `TagExtractor` struct definition:**\n```rust\n/// Tag with occurrence statistics\n#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]\npub struct TagCount {\n    /// The tag name (without # prefix)\n    pub tag: String,\n    /// Number of documents containing this tag\n    pub document_count: usize,\n}\n```\n\n**Add a new method to `TagExtractor` implementation:**\n```rust\n/// Extract all tags with document counts from markdown files in the given path\n/// Returns tags sorted by document_count descending, then alphabetically\npub fn extract_tags_with_counts(\n    \u0026self,\n    path: \u0026Path,\n) -\u003e Result\u003cVec\u003cTagCount\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n    let files = if path.is_file() {\n        vec![path.to_path_buf()]\n    } else {\n        collect_markdown_files(path)?\n    };\n\n    // Track which documents contain each tag\n    // Key: tag name, Value: set of file paths that contain this tag\n    let tag_documents: HashMap\u003cString, std::collections::HashSet\u003cPathBuf\u003e\u003e = files\n        .par_iter()\n        .filter_map(|file_path| {\n            self.extract_tags_from_file(file_path)\n                .ok()\n                .map(|tags| (file_path.clone(), tags))\n        })\n        .fold(\n            || HashMap::new(),\n            |mut acc: HashMap\u003cString, std::collections::HashSet\u003cPathBuf\u003e\u003e, (file_path, tags)| {\n                // Deduplicate tags within the same file (a file counts once per tag)\n                let unique_tags: std::collections::HashSet\u003cString\u003e = tags.into_iter().collect();\n                for tag in unique_tags {\n                    acc.entry(tag)\n                        .or_insert_with(std::collections::HashSet::new)\n                        .insert(file_path.clone());\n                }\n                acc\n            },\n        )\n        .reduce(\n            || HashMap::new(),\n            |mut a, b| {\n                for (tag, files) in b {\n                    a.entry(tag)\n                        .or_insert_with(std::collections::HashSet::new)\n                        .extend(files);\n                }\n                a\n            },\n        );\n\n    // Convert to Vec\u003cTagCount\u003e sorted by document_count desc, then tag name asc\n    let mut result: Vec\u003cTagCount\u003e = tag_documents\n        .into_iter()\n        .map(|(tag, files)| TagCount {\n            tag,\n            document_count: files.len(),\n        })\n        .collect();\n\n    result.sort_by(|a, b| {\n        b.document_count\n            .cmp(\u0026a.document_count)\n            .then_with(|| a.tag.cmp(\u0026b.tag))\n    });\n\n    Ok(result)\n}\n```\n\n**Key design decisions:**\n- Use `HashSet\u003cPathBuf\u003e` to track unique documents per tag (a document counts once even if it has the same tag multiple times in frontmatter)\n- Sort by document_count descending (most popular tags first), then alphabetically for stable ordering\n- Reuse existing `extract_tags_from_file` method to maintain consistency\n\n### 2. Add Request/Response Types to `src/mcp.rs`\n\n**Add import for `TagCount` at the top:**\n```rust\nuse crate::tag_extractor::{TagCount, TagExtractor};\n```\n\n**Add new request/response structs:**\n```rust\n/// Parameters for the list_tags tool\n#[derive(Debug, Deserialize, JsonSchema)]\npub struct ListTagsRequest {\n    #[schemars(description = \"Subpath within the vault to search (optional, defaults to entire vault)\")]\n    pub path: Option\u003cString\u003e,\n\n    #[schemars(description = \"Minimum document count to include a tag (optional, defaults to 1)\")]\n    pub min_count: Option\u003cusize\u003e,\n\n    #[schemars(description = \"Maximum number of tags to return (optional, defaults to all)\")]\n    pub limit: Option\u003cusize\u003e,\n}\n\n/// Response for the list_tags tool\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct ListTagsResponse {\n    /// List of tags with their document counts\n    pub tags: Vec\u003cTagCount\u003e,\n    /// Total number of unique tags found (before filtering/limiting)\n    pub total_unique_tags: usize,\n    /// Whether the results were truncated due to limit parameter\n    pub truncated: bool,\n}\n```\n\n### 3. Add MCP Tool Method to `TaskSearchService` in `src/mcp.rs`\n\nAdd the new tool method inside the `#[tool_router] impl TaskSearchService` block:\n\n```rust\n#[tool(description = \"List all tags in the vault with document counts. Returns tags sorted by frequency (most common first). Useful for understanding the tag taxonomy, finding popular topics, and discovering content organization patterns.\")]\nasync fn list_tags(\n    \u0026self,\n    Parameters(request): Parameters\u003cListTagsRequest\u003e,\n) -\u003e Result\u003cJson\u003cListTagsResponse\u003e, ErrorData\u003e {\n    // Resolve search path\n    let search_path = if let Some(ref subpath) = request.path {\n        self.base_path.join(subpath)\n    } else {\n        self.base_path.clone()\n    };\n\n    // Extract tags with counts\n    let mut tags = self\n        .tag_extractor\n        .extract_tags_with_counts(\u0026search_path)\n        .map_err(|e| ErrorData {\n            code: ErrorCode(-32603),\n            message: Cow::from(format!(\"Failed to extract tags: {}\", e)),\n            data: None,\n        })?;\n\n    // Track total before filtering\n    let total_unique_tags = tags.len();\n\n    // Filter by min_count if specified\n    if let Some(min_count) = request.min_count {\n        tags.retain(|t| t.document_count \u003e= min_count);\n    }\n\n    // Apply limit if specified\n    let truncated = if let Some(limit) = request.limit {\n        if tags.len() \u003e limit {\n            tags.truncate(limit);\n            true\n        } else {\n            false\n        }\n    } else {\n        false\n    };\n\n    Ok(Json(ListTagsResponse {\n        tags,\n        total_unique_tags,\n        truncated,\n    }))\n}\n```\n\n### 4. Add HTTP Endpoint Handlers to `src/main.rs`\n\n**Add HTTP handler functions:**\n```rust\n/// HTTP handler for listing tags with counts (GET)\nasync fn list_tags_handler_get(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    query: axum::extract::Query\u003cmcp::ListTagsRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ListTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    list_tags_impl(state, query.0).await\n}\n\n/// HTTP handler for listing tags with counts (POST)\nasync fn list_tags_handler_post(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    axum::Json(request): axum::Json\u003cmcp::ListTagsRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ListTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    list_tags_impl(state, request).await\n}\n\n/// Shared implementation for listing tags with counts\nasync fn list_tags_impl(\n    state: AppState,\n    request: mcp::ListTagsRequest,\n) -\u003e Result\u003caxum::Json\u003cmcp::ListTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    // Resolve search path\n    let search_path = if let Some(ref subpath) = request.path {\n        state.base_path.join(subpath)\n    } else {\n        state.base_path.clone()\n    };\n\n    // Extract tags with counts\n    let mut tags = state\n        .tag_extractor\n        .extract_tags_with_counts(\u0026search_path)\n        .map_err(|e| {\n            (\n                axum::http::StatusCode::INTERNAL_SERVER_ERROR,\n                format!(\"Failed to extract tags: {}\", e),\n            )\n        })?;\n\n    // Track total before filtering\n    let total_unique_tags = tags.len();\n\n    // Filter by min_count if specified\n    if let Some(min_count) = request.min_count {\n        tags.retain(|t| t.document_count \u003e= min_count);\n    }\n\n    // Apply limit if specified\n    let truncated = if let Some(limit) = request.limit {\n        if tags.len() \u003e limit {\n            tags.truncate(limit);\n            true\n        } else {\n            false\n        }\n    } else {\n        false\n    };\n\n    Ok(axum::Json(mcp::ListTagsResponse {\n        tags,\n        total_unique_tags,\n        truncated,\n    }))\n}\n```\n\n**Add the route in the router configuration (in the `if args.mcp_http` block):**\n```rust\n.route(\n    \"/api/tags/list\",\n    axum::routing::get(list_tags_handler_get).post(list_tags_handler_post),\n)\n```\n\n**Add the new tool to the `tools_handler` function:**\n```rust\nasync fn tools_handler() -\u003e impl axum::response::IntoResponse {\n    use axum::Json;\n    use mcp::{ExtractTagsRequest, ListTagsRequest, SearchTasksRequest};\n    use schemars::schema_for;\n    use serde_json::json;\n\n    let search_tasks_schema = schema_for!(SearchTasksRequest);\n    let extract_tags_schema = schema_for!(ExtractTagsRequest);\n    let list_tags_schema = schema_for!(ListTagsRequest);\n\n    let tools = json!({\n        \"tools\": [\n            {\n                \"name\": \"search_tasks\",\n                \"description\": \"Search for tasks in Markdown files with optional filtering by status, dates, and tags\",\n                \"input_schema\": search_tasks_schema\n            },\n            {\n                \"name\": \"extract_tags\",\n                \"description\": \"Extract all unique tags from YAML frontmatter in Markdown files\",\n                \"input_schema\": extract_tags_schema\n            },\n            {\n                \"name\": \"list_tags\",\n                \"description\": \"List all tags in the vault with document counts. Returns tags sorted by frequency.\",\n                \"input_schema\": list_tags_schema\n            }\n        ]\n    });\n\n    Json(tools)\n}\n```\n\n**Update the startup message to include the new endpoint:**\n```rust\neprintln!(\"  - GET/POST http://{}/api/tags/list\", addr);\n```\n\n### 5. Add Tests to `src/tag_extractor.rs`\n\nAdd tests at the end of the `#[cfg(test)] mod tests` block:\n\n```rust\n#[test]\nfn test_extract_tags_with_counts_single_file() {\n    let extractor = TagExtractor::new();\n    let temp_dir = std::env::temp_dir().join(\"test_tags_counts\");\n    std::fs::create_dir_all(\u0026temp_dir).unwrap();\n    \n    let content = r#\"---\ntags:\n  - rust\n  - programming\n---\n# Content\n\"#;\n    let file_path = temp_dir.join(\"test1.md\");\n    std::fs::write(\u0026file_path, content).unwrap();\n    \n    let counts = extractor.extract_tags_with_counts(\u0026temp_dir).unwrap();\n    \n    assert_eq!(counts.len(), 2);\n    assert!(counts.iter().any(|t| t.tag == \"rust\" \u0026\u0026 t.document_count == 1));\n    assert!(counts.iter().any(|t| t.tag == \"programming\" \u0026\u0026 t.document_count == 1));\n    \n    std::fs::remove_dir_all(\u0026temp_dir).ok();\n}\n\n#[test]\nfn test_extract_tags_with_counts_multiple_files() {\n    let extractor = TagExtractor::new();\n    let temp_dir = std::env::temp_dir().join(\"test_tags_counts_multi\");\n    std::fs::create_dir_all(\u0026temp_dir).unwrap();\n    \n    // File 1: has rust and programming tags\n    let content1 = r#\"---\ntags:\n  - rust\n  - programming\n---\n\"#;\n    std::fs::write(temp_dir.join(\"file1.md\"), content1).unwrap();\n    \n    // File 2: has rust and cli tags\n    let content2 = r#\"---\ntags:\n  - rust\n  - cli\n---\n\"#;\n    std::fs::write(temp_dir.join(\"file2.md\"), content2).unwrap();\n    \n    let counts = extractor.extract_tags_with_counts(\u0026temp_dir).unwrap();\n    \n    // rust appears in 2 documents, programming and cli in 1 each\n    let rust = counts.iter().find(|t| t.tag == \"rust\").unwrap();\n    assert_eq!(rust.document_count, 2);\n    \n    let programming = counts.iter().find(|t| t.tag == \"programming\").unwrap();\n    assert_eq!(programming.document_count, 1);\n    \n    let cli = counts.iter().find(|t| t.tag == \"cli\").unwrap();\n    assert_eq!(cli.document_count, 1);\n    \n    // Should be sorted by count desc\n    assert_eq!(counts[0].tag, \"rust\");\n    \n    std::fs::remove_dir_all(\u0026temp_dir).ok();\n}\n\n#[test]\nfn test_extract_tags_with_counts_duplicate_in_same_file() {\n    let extractor = TagExtractor::new();\n    let temp_dir = std::env::temp_dir().join(\"test_tags_counts_dup\");\n    std::fs::create_dir_all(\u0026temp_dir).unwrap();\n    \n    // File with duplicate tag (should only count once per document)\n    let content = r#\"---\ntags:\n  - rust\n  - rust\n  - programming\n---\n\"#;\n    std::fs::write(temp_dir.join(\"file.md\"), content).unwrap();\n    \n    let counts = extractor.extract_tags_with_counts(\u0026temp_dir).unwrap();\n    \n    let rust = counts.iter().find(|t| t.tag == \"rust\").unwrap();\n    assert_eq!(rust.document_count, 1); // Should be 1, not 2\n    \n    std::fs::remove_dir_all(\u0026temp_dir).ok();\n}\n\n#[test]\nfn test_extract_tags_with_counts_empty_vault() {\n    let extractor = TagExtractor::new();\n    let temp_dir = std::env::temp_dir().join(\"test_tags_counts_empty\");\n    std::fs::create_dir_all(\u0026temp_dir).unwrap();\n    \n    let counts = extractor.extract_tags_with_counts(\u0026temp_dir).unwrap();\n    assert!(counts.is_empty());\n    \n    std::fs::remove_dir_all(\u0026temp_dir).ok();\n}\n```\n\n### Files to Modify\n\n1. `/home/jeffutter/src/markdown-todo-extractor/src/tag_extractor.rs`\n   - Add imports for `schemars`, `serde`, `HashMap`\n   - Add `TagCount` struct\n   - Add `extract_tags_with_counts` method\n   - Add unit tests\n\n2. `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs`\n   - Update import to include `TagCount`\n   - Add `ListTagsRequest` struct\n   - Add `ListTagsResponse` struct\n   - Add `list_tags` tool method\n\n3. `/home/jeffutter/src/markdown-todo-extractor/src/main.rs`\n   - Add `list_tags_handler_get` function\n   - Add `list_tags_handler_post` function\n   - Add `list_tags_impl` function\n   - Add route for `/api/tags/list`\n   - Update `tools_handler` to include new tool schema\n   - Update startup message to show new endpoint\n\n### Tool Naming Rationale\n\nThe existing `extract_tags` tool returns just unique tag names (a simple list). The new `list_tags` tool returns tags with statistics. Both tools serve different purposes:\n\n- `extract_tags`: Quick list of all unique tags (lightweight, existing functionality)\n- `list_tags`: Detailed tag statistics with document counts (new functionality)\n\nThis follows the pattern seen in the Obsidian Copilot reference where tag listing is a distinct operation from tag extraction.\n\n### API Examples\n\n**MCP Tool Call:**\n```json\n{\n  \"tool\": \"list_tags\",\n  \"arguments\": {\n    \"path\": \"Projects\",\n    \"min_count\": 2,\n    \"limit\": 50\n  }\n}\n```\n\n**HTTP GET:**\n```\nGET /api/tags/list?path=Projects\u0026min_count=2\u0026limit=50\n```\n\n**HTTP POST:**\n```json\nPOST /api/tags/list\n{\n  \"path\": \"Projects\",\n  \"min_count\": 2,\n  \"limit\": 50\n}\n```\n\n**Response:**\n```json\n{\n  \"tags\": [\n    {\"tag\": \"work\", \"document_count\": 15},\n    {\"tag\": \"project\", \"document_count\": 10},\n    {\"tag\": \"meeting\", \"document_count\": 7}\n  ],\n  \"total_unique_tags\": 45,\n  \"truncated\": false\n}\n```","status":"closed","priority":2,"issue_type":"feature","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-19T18:50:38.764378029-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-19T22:31:35.344875943-06:00","closed_at":"2026-01-19T22:31:35.344875943-06:00","close_reason":"Closed","labels":["planned"],"dependencies":[{"issue_id":"markdown-todo-extractor-tx9.4","depends_on_id":"markdown-todo-extractor-tx9","type":"parent-child","created_at":"2026-01-19T18:50:38.764956166-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-tx9.5","title":"Tag Search Tool","description":"# Tag Search Tool\n\nCreate a tool to list files based on tags.\n\n## Requirements\n\n- Tags should be searched for in the vault.\n- Tags are in the YAML frontmatter of files under the 'tags' key\n\n\nThis tool enables the LLM to find documents that match specific tags from their YAML frontmatter. Unlike the semantic search in RAG, this provides exact tag-based filtering, useful for:\n- Finding all documents with a particular tag\n- Finding documents that match multiple tags (AND/OR logic)\n- Browsing content by topic/category\n\n---\n\n## Implementation Plan\n\n### Overview\n\nCreate a new `search_by_tags` MCP tool and REST API endpoint that finds markdown files matching specified frontmatter tags. The tool will extend the existing `TagExtractor` module and follow the established patterns in the codebase.\n\n### Architecture Decision\n\nExtend the existing `TagExtractor` rather than creating a new extractor module. The `TagExtractor` already has:\n- YAML frontmatter parsing logic (`extract_frontmatter`, `parse_tags_from_frontmatter`)\n- File collection mechanism (`collect_markdown_files`)\n- Parallel processing with rayon\n\nAdding tag search functionality to this module maintains cohesion and reduces code duplication.\n\n---\n\n### Step 1: Add Config Integration to TagExtractor\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/tag_extractor.rs`\n\nCurrently `TagExtractor` is a simple unit struct without configuration. Update to match `TaskExtractor` pattern:\n\n```rust\nuse crate::config::Config;\nuse std::sync::Arc;\n\n/// Extractor for YAML frontmatter tags\npub struct TagExtractor {\n    config: Arc\u003cConfig\u003e,\n}\n\nimpl TagExtractor {\n    pub fn new(config: Arc\u003cConfig\u003e) -\u003e Self {\n        Self { config }\n    }\n}\n```\n\nUpdate `collect_markdown_files` to accept `\u0026Config` and apply path exclusions:\n\n```rust\nfn collect_markdown_files(dir: \u0026Path, config: \u0026Config) -\u003e Result\u003cVec\u003cPathBuf\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n    let mut files = Vec::new();\n\n    if dir.is_dir() {\n        for entry in fs::read_dir(dir)? {\n            let entry = entry?;\n            let path = entry.path();\n\n            // Skip excluded paths\n            if config.should_exclude(\u0026path) {\n                continue;\n            }\n\n            if path.is_dir() {\n                files.extend(collect_markdown_files(\u0026path, config)?);\n            } else if path.extension().and_then(|s| s.to_str()) == Some(\"md\") {\n                files.push(path);\n            }\n        }\n    }\n\n    Ok(files)\n}\n```\n\nUpdate `extract_tags` to pass config:\n\n```rust\npub fn extract_tags(\u0026self, path: \u0026Path) -\u003e Result\u003cVec\u003cString\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n    let files = if path.is_file() {\n        vec![path.to_path_buf()]\n    } else {\n        collect_markdown_files(path, \u0026self.config)?\n    };\n    // ... rest unchanged\n}\n```\n\n---\n\n### Step 2: Add TaggedFile Struct and Search Method\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/tag_extractor.rs`\n\nAdd new struct and search method:\n\n```rust\nuse schemars::JsonSchema;\n\n/// Represents a file that matches tag search criteria\n#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]\npub struct TaggedFile {\n    /// Absolute path to the file\n    pub file_path: String,\n    /// File name without path\n    pub file_name: String,\n    /// Tags that matched the search criteria\n    pub matched_tags: Vec\u003cString\u003e,\n    /// All tags found in the file's frontmatter\n    pub all_tags: Vec\u003cString\u003e,\n}\n\nimpl TagExtractor {\n    /// Make the internal method public for single file tag extraction\n    pub fn get_file_tags(\u0026self, file_path: \u0026Path) -\u003e Result\u003cVec\u003cString\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n        self.extract_tags_from_file(file_path)\n    }\n\n    /// Search for files by tags with AND/OR logic\n    /// \n    /// # Arguments\n    /// * `path` - Directory to search\n    /// * `tags` - Tags to search for\n    /// * `match_all` - If true, file must have ALL tags (AND logic). If false, file must have ANY tag (OR logic)\n    pub fn search_by_tags(\n        \u0026self,\n        path: \u0026Path,\n        tags: \u0026[String],\n        match_all: bool,\n    ) -\u003e Result\u003cVec\u003cTaggedFile\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n        let files = if path.is_file() {\n            vec![path.to_path_buf()]\n        } else {\n            collect_markdown_files(path, \u0026self.config)?\n        };\n\n        // Normalize search tags to lowercase for case-insensitive comparison\n        let search_tags: Vec\u003cString\u003e = tags.iter().map(|t| t.to_lowercase()).collect();\n\n        let results: Vec\u003cTaggedFile\u003e = files\n            .par_iter()\n            .filter_map(|file_path| {\n                // Extract tags from file\n                let all_tags = self.extract_tags_from_file(file_path).ok()?;\n                \n                if all_tags.is_empty() {\n                    return None;\n                }\n\n                // Normalize file tags for comparison\n                let normalized_tags: Vec\u003cString\u003e = all_tags.iter().map(|t| t.to_lowercase()).collect();\n\n                // Find which search tags match this file\n                let matched_tags: Vec\u003cString\u003e = search_tags\n                    .iter()\n                    .filter(|search_tag| normalized_tags.contains(search_tag))\n                    .cloned()\n                    .collect();\n\n                // Apply match logic\n                let matches = if match_all {\n                    // AND logic: all search tags must be present\n                    matched_tags.len() == search_tags.len()\n                } else {\n                    // OR logic: at least one search tag must be present\n                    !matched_tags.is_empty()\n                };\n\n                if matches {\n                    Some(TaggedFile {\n                        file_path: file_path.to_string_lossy().to_string(),\n                        file_name: file_path.file_name()?.to_string_lossy().to_string(),\n                        matched_tags,\n                        all_tags,\n                    })\n                } else {\n                    None\n                }\n            })\n            .collect();\n\n        Ok(results)\n    }\n}\n```\n\n---\n\n### Step 3: Add MCP Tool in mcp.rs\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs`\n\nAdd request/response types:\n\n```rust\nuse crate::tag_extractor::TaggedFile;\n\n/// Parameters for the search_by_tags tool\n#[derive(Debug, Deserialize, JsonSchema)]\npub struct SearchByTagsRequest {\n    #[schemars(description = \"Tags to search for\")]\n    pub tags: Vec\u003cString\u003e,\n\n    #[schemars(description = \"If true, file must have ALL tags (AND logic). If false, file must have ANY tag (OR logic). Default: false\")]\n    pub match_all: Option\u003cbool\u003e,\n\n    #[schemars(description = \"Subpath within the base directory to search (optional)\")]\n    pub subpath: Option\u003cString\u003e,\n\n    #[schemars(description = \"Limit the number of files returned\")]\n    pub limit: Option\u003cusize\u003e,\n}\n\n/// Response for the search_by_tags tool\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct SearchByTagsResponse {\n    pub files: Vec\u003cTaggedFile\u003e,\n    pub total_count: usize,\n}\n```\n\nAdd tool method to `TaskSearchService`:\n\n```rust\n#[tool(description = \"Search for files by YAML frontmatter tags with AND/OR matching\")]\nasync fn search_by_tags(\n    \u0026self,\n    Parameters(request): Parameters\u003cSearchByTagsRequest\u003e,\n) -\u003e Result\u003cJson\u003cSearchByTagsResponse\u003e, ErrorData\u003e {\n    // Determine the search path (base path + optional subpath)\n    let search_path = if let Some(subpath) = request.subpath {\n        self.base_path.join(subpath)\n    } else {\n        self.base_path.clone()\n    };\n\n    let match_all = request.match_all.unwrap_or(false);\n\n    // Search for files by tags\n    let mut files = self\n        .tag_extractor\n        .search_by_tags(\u0026search_path, \u0026request.tags, match_all)\n        .map_err(|e| ErrorData {\n            code: ErrorCode(-32603),\n            message: Cow::from(format!(\"Failed to search by tags: {}\", e)),\n            data: None,\n        })?;\n\n    let total_count = files.len();\n\n    // Apply limit if specified\n    if let Some(limit) = request.limit {\n        files.truncate(limit);\n    }\n\n    Ok(Json(SearchByTagsResponse { files, total_count }))\n}\n```\n\n---\n\n### Step 4: Add REST API Endpoints in main.rs\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/main.rs`\n\nAdd HTTP handlers following the existing pattern:\n\n```rust\n/// HTTP handler for searching by tags (GET with query params)\nasync fn search_by_tags_handler_get(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    query: axum::extract::Query\u003cmcp::SearchByTagsRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::SearchByTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    search_by_tags_impl(state, query.0).await\n}\n\n/// HTTP handler for searching by tags (POST with JSON body)\nasync fn search_by_tags_handler_post(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    axum::Json(request): axum::Json\u003cmcp::SearchByTagsRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::SearchByTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    search_by_tags_impl(state, request).await\n}\n\n/// Shared implementation for tag search\nasync fn search_by_tags_impl(\n    state: AppState,\n    request: mcp::SearchByTagsRequest,\n) -\u003e Result\u003caxum::Json\u003cmcp::SearchByTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    // Determine the search path (base path + optional subpath)\n    let search_path = if let Some(ref subpath) = request.subpath {\n        state.base_path.join(subpath)\n    } else {\n        state.base_path.clone()\n    };\n\n    let match_all = request.match_all.unwrap_or(false);\n\n    // Search for files by tags\n    let mut files = state\n        .tag_extractor\n        .search_by_tags(\u0026search_path, \u0026request.tags, match_all)\n        .map_err(|e| {\n            (\n                axum::http::StatusCode::INTERNAL_SERVER_ERROR,\n                format!(\"Failed to search by tags: {}\", e),\n            )\n        })?;\n\n    let total_count = files.len();\n\n    // Apply limit if specified\n    if let Some(limit) = request.limit {\n        files.truncate(limit);\n    }\n\n    Ok(axum::Json(mcp::SearchByTagsResponse { files, total_count }))\n}\n```\n\nRegister routes in the router:\n\n```rust\n.route(\n    \"/api/search_by_tags\",\n    axum::routing::get(search_by_tags_handler_get).post(search_by_tags_handler_post),\n)\n```\n\nUpdate `tools_handler()` to include the new tool schema:\n\n```rust\nasync fn tools_handler() -\u003e impl axum::response::IntoResponse {\n    use mcp::{ExtractTagsRequest, SearchByTagsRequest, SearchTasksRequest};\n    use schemars::schema_for;\n    // ...\n    let search_by_tags_schema = schema_for!(SearchByTagsRequest);\n\n    let tools = json!({\n        \"tools\": [\n            // ... existing tools ...\n            {\n                \"name\": \"search_by_tags\",\n                \"description\": \"Search for files by YAML frontmatter tags with AND/OR matching\",\n                \"input_schema\": search_by_tags_schema\n            }\n        ]\n    });\n    // ...\n}\n```\n\nUpdate startup messages:\n\n```rust\neprintln!(\"  - GET/POST http://{}/api/search_by_tags\", addr);\n```\n\n---\n\n### Step 5: Add CLI Subcommand in cli.rs\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/cli.rs`\n\nAdd new subcommand to `Commands` enum:\n\n```rust\n#[derive(Subcommand, Debug)]\npub enum Commands {\n    /// Extract and filter tasks from markdown files\n    Tasks(Box\u003cTasksCommand\u003e),\n    /// Extract all unique tags from markdown files\n    Tags {\n        /// Path to file or folder to scan\n        #[arg(required = true)]\n        path: PathBuf,\n    },\n    /// Search for files by tags\n    SearchByTags {\n        /// Path to file or folder to scan\n        #[arg(required = true)]\n        path: PathBuf,\n\n        /// Tags to search for (comma-separated)\n        #[arg(long, value_delimiter = ',', required = true)]\n        tags: Vec\u003cString\u003e,\n\n        /// Require all tags to match (AND logic) instead of any (OR logic)\n        #[arg(long)]\n        match_all: bool,\n\n        /// Limit number of results\n        #[arg(long)]\n        limit: Option\u003cusize\u003e,\n    },\n}\n```\n\nAdd handling in `run_cli()`:\n\n```rust\nSome(Commands::SearchByTags { path, tags, match_all, limit }) =\u003e {\n    // Load configuration from the path\n    let config = Arc::new(Config::load_from_base_path(\u0026path));\n\n    // Create tag extractor\n    let extractor = TagExtractor::new(config);\n\n    // Search for files by tags\n    let mut files = extractor.search_by_tags(path, \u0026tags, match_all)?;\n\n    // Apply limit if specified\n    if let Some(limit) = limit {\n        files.truncate(limit);\n    }\n\n    // Output as JSON\n    let json = serde_json::to_string_pretty(\u0026files)?;\n    println!(\"{}\", json);\n\n    Ok(())\n}\n```\n\n---\n\n### Step 6: Update Existing Code for Config Integration\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs`\n\nUpdate `TaskSearchService::new()` to pass config to TagExtractor:\n\n```rust\npub fn new(base_path: PathBuf) -\u003e Self {\n    let config = Arc::new(Config::load_from_base_path(\u0026base_path));\n\n    Self {\n        tool_router: Self::tool_router(),\n        base_path,\n        task_extractor: Arc::new(TaskExtractor::new(config.clone())),\n        tag_extractor: Arc::new(TagExtractor::new(config)),  // Updated\n    }\n}\n```\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/main.rs`\n\nUpdate `AppState` initialization:\n\n```rust\nlet app_state = AppState {\n    base_path: base_path.clone(),\n    task_extractor: Arc::new(extractor::TaskExtractor::new(config.clone())),\n    tag_extractor: Arc::new(tag_extractor::TagExtractor::new(config.clone())),  // Updated\n    config,\n};\n```\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/cli.rs`\n\nUpdate Tags command to pass config:\n\n```rust\nSome(Commands::Tags { path }) =\u003e {\n    // Load configuration from the path\n    let config = Arc::new(Config::load_from_base_path(\u0026path));\n\n    // Create tag extractor\n    let extractor = TagExtractor::new(config);\n    // ... rest unchanged\n}\n```\n\n---\n\n### Step 7: Add Unit Tests\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/tag_extractor.rs`\n\nAdd tests for the new functionality:\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::io::Write;\n    use tempfile::TempDir;\n\n    fn create_test_config() -\u003e Arc\u003cConfig\u003e {\n        Arc::new(Config::default())\n    }\n\n    fn create_test_file(dir: \u0026Path, name: \u0026str, content: \u0026str) -\u003e PathBuf {\n        let path = dir.join(name);\n        let mut file = std::fs::File::create(\u0026path).unwrap();\n        file.write_all(content.as_bytes()).unwrap();\n        path\n    }\n\n    #[test]\n    fn test_search_by_tags_or_logic() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = create_test_config();\n        let extractor = TagExtractor::new(config);\n\n        // Create test files\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - rust\\n  - cli\\n---\\n# File 1\");\n        create_test_file(temp_dir.path(), \"file2.md\", \"---\\ntags:\\n  - python\\n  - cli\\n---\\n# File 2\");\n        create_test_file(temp_dir.path(), \"file3.md\", \"---\\ntags:\\n  - java\\n---\\n# File 3\");\n\n        // Search with OR logic (default)\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"rust\".to_string(), \"python\".to_string()], false).unwrap();\n\n        assert_eq!(results.len(), 2);\n        assert!(results.iter().any(|f| f.file_name == \"file1.md\"));\n        assert!(results.iter().any(|f| f.file_name == \"file2.md\"));\n    }\n\n    #[test]\n    fn test_search_by_tags_and_logic() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = create_test_config();\n        let extractor = TagExtractor::new(config);\n\n        // Create test files\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - rust\\n  - cli\\n---\\n# File 1\");\n        create_test_file(temp_dir.path(), \"file2.md\", \"---\\ntags:\\n  - rust\\n---\\n# File 2\");\n\n        // Search with AND logic\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"rust\".to_string(), \"cli\".to_string()], true).unwrap();\n\n        assert_eq!(results.len(), 1);\n        assert_eq!(results[0].file_name, \"file1.md\");\n    }\n\n    #[test]\n    fn test_search_by_tags_case_insensitive() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = create_test_config();\n        let extractor = TagExtractor::new(config);\n\n        // Create test file with mixed case tags\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - Rust\\n  - CLI\\n---\\n# File 1\");\n\n        // Search with lowercase\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"rust\".to_string()], false).unwrap();\n        assert_eq!(results.len(), 1);\n\n        // Search with uppercase\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"RUST\".to_string()], false).unwrap();\n        assert_eq!(results.len(), 1);\n    }\n\n    #[test]\n    fn test_search_by_tags_empty_result() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = create_test_config();\n        let extractor = TagExtractor::new(config);\n\n        // Create test file\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - rust\\n---\\n# File 1\");\n\n        // Search for non-existent tag\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"nonexistent\".to_string()], false).unwrap();\n        assert!(results.is_empty());\n    }\n\n    #[test]\n    fn test_search_by_tags_respects_exclusions() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = Arc::new(Config {\n            exclude_paths: vec![\"excluded\".to_string()],\n        });\n        let extractor = TagExtractor::new(config);\n\n        // Create test files\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - rust\\n---\\n# File 1\");\n        \n        // Create excluded directory\n        let excluded_dir = temp_dir.path().join(\"excluded\");\n        std::fs::create_dir(\u0026excluded_dir).unwrap();\n        create_test_file(\u0026excluded_dir, \"file2.md\", \"---\\ntags:\\n  - rust\\n---\\n# File 2\");\n\n        // Search should not include excluded file\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"rust\".to_string()], false).unwrap();\n        assert_eq!(results.len(), 1);\n        assert_eq!(results[0].file_name, \"file1.md\");\n    }\n\n    #[test]\n    fn test_tagged_file_contains_all_tags() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = create_test_config();\n        let extractor = TagExtractor::new(config);\n\n        // Create test file with multiple tags\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - rust\\n  - cli\\n  - tool\\n---\\n# File 1\");\n\n        // Search for one tag\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"rust\".to_string()], false).unwrap();\n\n        assert_eq!(results.len(), 1);\n        assert_eq!(results[0].matched_tags, vec![\"rust\".to_string()]);\n        assert_eq!(results[0].all_tags, vec![\"rust\".to_string(), \"cli\".to_string(), \"tool\".to_string()]);\n    }\n}\n```\n\nNote: Add `tempfile = \"3\"` to dev-dependencies in Cargo.toml for tests.\n\n---\n\n### File Changes Summary\n\n| File | Changes |\n|------|---------|\n| `Cargo.toml` | Add `tempfile = \"3\"` to dev-dependencies |\n| `src/tag_extractor.rs` | Add Config integration, `TaggedFile` struct, `search_by_tags()` method, `get_file_tags()` method, unit tests |\n| `src/mcp.rs` | Add `SearchByTagsRequest`, `SearchByTagsResponse`, `search_by_tags` tool, update constructor |\n| `src/main.rs` | Add REST API handlers and routes for `/api/search_by_tags`, update AppState, update tools_handler |\n| `src/cli.rs` | Add `SearchByTags` subcommand, update Tags command for Config |\n\n---\n\n### API Examples\n\n**MCP Tool Call:**\n```json\n{\n  \"tool\": \"search_by_tags\",\n  \"arguments\": {\n    \"tags\": [\"project\", \"active\"],\n    \"match_all\": true,\n    \"limit\": 20\n  }\n}\n```\n\n**REST API Call:**\n```bash\n# GET with query params\ncurl \"http://localhost:8000/api/search_by_tags?tags=project,active\u0026match_all=true\u0026limit=20\"\n\n# POST with JSON body\ncurl -X POST http://localhost:8000/api/search_by_tags \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"tags\": [\"project\", \"active\"], \"match_all\": true, \"limit\": 20}'\n```\n\n**CLI:**\n```bash\n# Search for files with ANY of the tags (OR logic - default)\nmarkdown-todo-extractor search-by-tags /path/to/vault --tags project,active\n\n# Search for files with ALL tags (AND logic)\nmarkdown-todo-extractor search-by-tags /path/to/vault --tags project,active --match-all\n\n# With limit\nmarkdown-todo-extractor search-by-tags /path/to/vault --tags project --limit 10\n```\n\n**Response Format:**\n```json\n{\n  \"files\": [\n    {\n      \"file_path\": \"/vault/projects/ProjectA.md\",\n      \"file_name\": \"ProjectA.md\",\n      \"matched_tags\": [\"project\", \"active\"],\n      \"all_tags\": [\"project\", \"active\", \"2024\"]\n    }\n  ],\n  \"total_count\": 1\n}\n```\n\n---\n\n### Testing Checklist\n\n1. [ ] Unit tests pass for tag search logic (AND/OR)\n2. [ ] Unit tests pass for case-insensitive matching\n3. [ ] Unit tests pass for path exclusions\n4. [ ] MCP tool works via stdio\n5. [ ] REST API endpoints work (GET and POST)\n6. [ ] CLI subcommand works\n7. [ ] AND logic correctly filters files (must have ALL tags)\n8. [ ] OR logic correctly includes files (must have ANY tag)\n9. [ ] Path exclusions are respected\n10. [ ] Limit parameter works\n11. [ ] Subpath parameter works\n12. [ ] Empty results handled gracefully\n13. [ ] Files without frontmatter are skipped gracefully\n14. [ ] `cargo build --release` succeeds\n15. [ ] `cargo clippy` passes\n16. [ ] `cargo fmt --check` passes","status":"closed","priority":2,"issue_type":"feature","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-19T18:51:01.199607478-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-19T22:35:17.861300724-06:00","closed_at":"2026-01-19T22:35:17.861300724-06:00","close_reason":"Closed","labels":["planned"],"dependencies":[{"issue_id":"markdown-todo-extractor-tx9.5","depends_on_id":"markdown-todo-extractor-tx9","type":"parent-child","created_at":"2026-01-19T18:51:01.200457025-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-v0k","title":"Refactor into capability-based modules with unified trait interface","description":"Refactor the project architecture to support multiple interfaces (MCP, HTTP, CLI) through a common trait system.\n\n## Architecture Overview\n\nSplit capabilities into focused modules:\n- Tag listing/retrieval\n- Task search  \n- File list/fetch\n\n## Module Design\n\nEach module should:\n1. Handle logic for its specific capability area\n2. Expose a type implementing a common trait\n3. Support all three interface types through the trait\n\n## Trait Requirements\n\nCreate a common trait that provides:\n- MCP server integration\n- HTTP REST-style endpoint support\n- CLI command interface\n\nWhen implemented, the trait should enable near-automatic exposure via all three interfaces.\n\n## Benefits\n\n- Consistent interface across all capability areas\n- Single implementation supports multiple access patterns\n- Easier to add new capabilities in the future\n- Cleaner separation of concerns","status":"closed","priority":2,"issue_type":"feature","owner":"jeff@jeffutter.com","created_at":"2026-01-19T23:21:12.250198134-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T11:13:13.51127458-06:00","closed_at":"2026-01-20T11:13:13.51127458-06:00","close_reason":"Closed"}
