{"id":"markdown-todo-extractor-29z","title":"Remove unused CapabilityRegistry accessor methods","description":"## Problem\n\nIn `src/capabilities/mod.rs`, the `CapabilityRegistry` has accessor methods marked with `#[allow(dead_code)]`:\n\n```rust\n#[allow(dead_code)]\npub fn base_path(\u0026self) -\u003e \u0026PathBuf {\n    \u0026self.base_path\n}\n\n/// Get the config\n#[allow(dead_code)]\npub fn config(\u0026self) -\u003e \u0026Arc\u003cConfig\u003e {\n    \u0026self.config\n}\n```\n\nThese methods:\n1. Are never called in the current codebase\n2. Required `#[allow(dead_code)]` to suppress compiler warnings\n3. Represent speculative design (added for potential future use)\n\n## Proposed Solution\n\nRemove these methods if they're not part of a planned public API:\n\n```rust\n// DELETE these methods from CapabilityRegistry impl\n```\n\nIf there's a planned use case, document it with a TODO comment instead.\n\n## Locations\n\n- `src/capabilities/mod.rs` lines ~91-100\n\n## Estimated Impact\n\n- ~10 lines of unused code removed\n- Eliminates `#[allow(dead_code)]` attributes (code smell)\n- Cleaner public API surface","status":"open","priority":4,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:48:19.246400463-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T22:48:19.246400463-06:00","labels":["simplification-opportunity"]}
{"id":"markdown-todo-extractor-2ko","title":"Consolidate CLI path handling across operations","description":"## Problem\n\n1. **Naming inconsistency**: CLI path fields have different names across request types:\n   - `SearchTasksRequest::path` (tasks.rs line 33)\n   - `ListTagsRequest::cli_path` (tags.rs line 59)\n   - `SearchByTagsRequest::cli_path` (tags.rs line 106)\n   - `ListFilesRequest::cli_path` (files.rs line 27)\n\n2. **Duplicated path handling logic**: Each operation's `execute_from_args` has nearly identical path resolution code (~20 lines each):\n\n```rust\nlet response = if let Some(ref path) = request.cli_path {\n    let config = Arc::new(Config::load_from_base_path(path.as_path()));\n    let capability = TagCapability::new(path.clone(), config);\n    let mut req_without_path = request;\n    req_without_path.cli_path = None;\n    capability.list_tags(req_without_path).await?\n} else {\n    self.capability.list_tags(request).await?\n};\n```\n\nThis pattern is repeated in all 6 operations with only the capability type changing.\n\n## Proposed Solution\n\n1. **Standardize naming**: Use `cli_path: Option\u003cPathBuf\u003e` consistently across all request types\n\n2. **Extract helper function**:\n```rust\n/// Resolves CLI path to create a temporary capability if provided,\n/// otherwise uses the registry's default capability.\npub fn with_cli_path\u003cC, F, R\u003e(\n    cli_path: Option\u003cPathBuf\u003e,\n    default_capability: Arc\u003cC\u003e,\n    capability_factory: impl FnOnce(PathBuf, Arc\u003cConfig\u003e) -\u003e C,\n    operation: F,\n) -\u003e CapabilityResult\u003cR\u003e\nwhere\n    F: FnOnce(\u0026C) -\u003e CapabilityResult\u003cR\u003e,\n{\n    match cli_path {\n        Some(path) =\u003e {\n            let config = Arc::new(Config::load_from_base_path(\u0026path));\n            let temp_capability = capability_factory(path, config);\n            operation(\u0026temp_capability)\n        }\n        None =\u003e operation(\u0026*default_capability),\n    }\n}\n```\n\n## Locations to Update\n\n- `src/capabilities/tasks.rs` - SearchTasksRequest, SearchTasksOperation\n- `src/capabilities/tags.rs` - ExtractTagsRequest, ListTagsRequest, SearchByTagsRequest, and their operations\n- `src/capabilities/files.rs` - ListFilesRequest, ReadFileRequest, and their operations\n\n## Estimated Impact\n\n- ~60 lines of duplicated path handling eliminated\n- Consistent naming across all request types\n- Single point of change for CLI path resolution logic","status":"closed","priority":2,"issue_type":"task","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:47:14.580831534-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T05:52:05.546891839-06:00","closed_at":"2026-01-21T05:52:05.546891839-06:00","close_reason":"Closed","labels":["simplification-opportunity"]}
{"id":"markdown-todo-extractor-2m1","title":"Add test coverage for filter.rs","description":"## Problem\n\nThe filtering module (`src/filter.rs` - 104 lines) has **no tests**. This module handles:\n\n- Status filtering (complete, incomplete)\n- Date range filtering (due dates)\n- Tag filtering\n- Path filtering\n\nComplex date comparison and tag matching logic is untested.\n\n## Key Functions That Need Tests\n\n1. **`filter_tasks()`** - Main filtering function\n   - Test status filter: only incomplete tasks\n   - Test status filter: only complete tasks\n   - Test no status filter: all tasks\n\n2. **Date filtering**\n   - Test due_after: tasks due after a specific date\n   - Test due_before: tasks due before a specific date\n   - Test date range: tasks between two dates\n   - Test tasks without due dates (should be excluded or included?)\n\n3. **Tag filtering**\n   - Test single tag filter\n   - Test multiple tag filter (AND vs OR logic?)\n   - Test tag not present in task\n\n4. **Path filtering**\n   - Test exact path match\n   - Test partial path match\n   - Test path prefix\n\n5. **Edge cases**\n   - Empty task list\n   - No filters applied (all tasks returned)\n   - Multiple filters combined\n\n## Locations\n\n- `src/filter.rs` - `FilterOptions` struct and `filter_tasks()` function\n\n## Estimated Impact\n\n- ~100-150 lines of test code to add\n- Documents filter behavior (especially edge cases)\n- Enables confident changes to filter logic","status":"closed","priority":3,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:47:47.423303908-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T11:02:16.393822736-06:00","closed_at":"2026-01-21T11:02:16.393822736-06:00","close_reason":"Closed","labels":["simplification-opportunity","testing"]}
{"id":"markdown-todo-extractor-40j","title":"Remove unused markdown dependency","description":"## Problem\n\nThe `markdown = \"1.0.0\"` dependency is listed in `Cargo.toml` but does not appear to be used anywhere in the source code.\n\n## Investigation Required\n\nSearch for:\n- `use markdown` - No results expected\n- `markdown::` - No results expected\n- Any actual markdown parsing that might justify the dependency\n\n## Possible Explanations\n\n1. **Vestigial**: Was used in earlier design and forgotten during cleanup\n2. **Indirect**: Used implicitly through another crate (unlikely)\n3. **Future feature**: Intended for planned functionality (but speculative design)\n\n## Proposed Solution\n\nIf confirmed unused:\n```toml\n# Remove from Cargo.toml\nmarkdown = \"1.0.0\"  # DELETE THIS LINE\n```\n\n## Estimated Impact\n\n- Reduces compile time\n- Reduces binary size\n- Cleaner dependency list","status":"closed","priority":4,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:48:18.57043359-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T06:12:39.566188467-06:00","closed_at":"2026-01-21T06:12:39.566188467-06:00","close_reason":"Removed unused markdown dependency from Cargo.toml. Verified with grep search that it was not used anywhere in the codebase. Build successful after removal.","labels":["dependencies","simplification-opportunity"]}
{"id":"markdown-todo-extractor-4ia","title":"Optimize tokio features to reduce binary size","description":"## Problem\n\nIn `Cargo.toml`, tokio is included with `features = [\"full\"]`:\n\n```toml\ntokio = { version = \"1.44.2\", features = [\"full\"] }\n```\n\nThis includes many features not needed by this CLI/server application:\n- `signal` - Unix signal handling\n- `process` - Child process spawning\n- `fs` - File system operations (we use std::fs)\n- `tracing` - Tracing integration\n- Various test utilities\n\n## Proposed Solution\n\nReplace `full` with only the features actually needed:\n\n```toml\ntokio = { version = \"1.44.2\", features = [\"rt-multi-thread\", \"macros\", \"net\", \"io-util\", \"sync\"] }\n```\n\nFeatures breakdown:\n- `rt-multi-thread` - Multi-threaded runtime for async execution\n- `macros` - `#[tokio::main]` attribute macro\n- `net` - TCP/UDP for HTTP server\n- `io-util` - Async I/O utilities\n- `sync` - Synchronization primitives (channels, mutexes)\n\n## Investigation Required\n\n1. Check which tokio features are actually used in the code\n2. May need `time` feature if any timeouts are used\n3. May need `signal` if graceful shutdown is implemented\n\n## Estimated Impact\n\n- Reduced binary size (~1-3 MB smaller)\n- Faster compile times\n- Clearer declaration of actual dependencies","status":"open","priority":4,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:48:18.912714052-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T22:48:18.912714052-06:00","labels":["dependencies","simplification-opportunity"]}
{"id":"markdown-todo-extractor-5i0","title":"Implement CLI automatic registration for files operations","description":"Apply CLI automatic registration pattern to 2 file operations (list_files, read_file). Follow the same pattern used for tasks: add Parser derives to request structs, implement CliOperation for operation structs, update create_cli_operations() in capabilities/mod.rs.","status":"closed","priority":2,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T19:35:13.508211743-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T21:11:31.744212654-06:00","closed_at":"2026-01-20T21:11:31.744212654-06:00","close_reason":"Closed","dependencies":[{"issue_id":"markdown-todo-extractor-5i0","depends_on_id":"markdown-todo-extractor-b68","type":"blocks","created_at":"2026-01-20T19:35:29.746867336-06:00","created_by":"Jeffery Utter"}],"comments":[{"id":2,"issue_id":"markdown-todo-extractor-5i0","author":"Jeffery Utter","text":"Implementation Guide - Files Operations\n\nFollow the CLI automatic registration pattern documented in CLAUDE.md section \"Adding CLI Automatic Registration for an Operation\".\n\nReference: src/capabilities/tasks.rs SearchTasksOperation for working example\n\nDepends on: markdown-todo-extractor-b68 (tags operations must be converted first)\n\nOperations to convert (2 total):\n1. ListFilesOperation (list_files) - CLI name: \"list-files\"\n2. ReadFileOperation (read_file) - CLI name: \"read-file\"\n\nFor EACH operation:\n1. Add Parser derive to request struct (ListFilesRequest, ReadFileRequest)\n   - Import: use clap::{CommandFactory, FromArgMatches, Parser};\n   - Add Parser to #[derive(...)]\n   - Add #[command(name = \"...\", about = \"...\")]\n   - For ReadFileRequest: may need special handling since it takes vault_path + file_path\n   - Add #[arg(...)] to all existing fields\n\n2. Implement CliOperation trait for operation struct\n   - command_name() - use existing CLI_NAME constant\n   - get_command() - call RequestStruct::command()\n   - execute_from_args() - parse args, handle path, call capability method, return JSON\n\n3. Register in create_cli_operations() in src/capabilities/mod.rs\n   - Uncomment the Arc::new(...) lines for file operations\n\n4. Update src/cli.rs - remove manual routing\n   - Remove ListFiles/ReadFile from Commands enum\n   - Remove command structs\n   - Remove match arms from run_cli()\n   - Update the check at top of run_cli() to include these commands\n\n5. Once all operations converted, simplify run_cli()\n   - Remove the conditional check for specific commands\n   - Make ALL commands use automatic routing\n   - Remove the old match statement entirely\n   - Keep only the automatic routing path\n\nTesting:\ncargo run -- list-files /tmp/test-vault --max-depth 2\ncargo run -- read-file /tmp/test-vault test.md","created_at":"2026-01-21T01:43:17Z"}]}
{"id":"markdown-todo-extractor-6oq","title":"Remove duplicate merge_from_env_var implementations in config.rs","description":"## Problem\n\nIn `src/config.rs`, the `merge_from_env_var` function is defined twice with `#[cfg(test)]` and `#[cfg(not(test))]` guards (lines 45-72), but **both implementations are identical**:\n\n```rust\n#[cfg(test)]\nfn merge_from_env_var(\u0026mut self, var_name: \u0026str) {\n    if let Ok(paths) = std::env::var(var_name) {\n        let env_paths: Vec\u003cString\u003e = paths\n            .split(',')\n            .map(|s| s.trim().to_string())\n            .filter(|s| !s.is_empty())\n            .collect();\n        self.exclude_paths.extend(env_paths);\n    }\n}\n\n#[cfg(not(test))]\nfn merge_from_env_var(\u0026mut self, var_name: \u0026str) {\n    if let Ok(paths) = std::env::var(var_name) {\n        let env_paths: Vec\u003cString\u003e = paths\n            .split(',')\n            .map(|s| s.trim().to_string())\n            .filter(|s| !s.is_empty())\n            .collect();\n        self.exclude_paths.extend(env_paths);\n    }\n}\n```\n\nThis is unnecessary duplication. The only difference should be at the call site (whether to call the function or not during tests).\n\n## Proposed Solution\n\nRemove the cfg guards from the function definition. If tests need to avoid merging env vars, handle it at the call site in `load_from_base_path()`:\n\n```rust\nfn merge_from_env_var(\u0026mut self, var_name: \u0026str) {\n    if let Ok(paths) = std::env::var(var_name) {\n        let env_paths: Vec\u003cString\u003e = paths\n            .split(',')\n            .map(|s| s.trim().to_string())\n            .filter(|s| !s.is_empty())\n            .collect();\n        self.exclude_paths.extend(env_paths);\n    }\n}\n```\n\n## Locations\n\n- `src/config.rs` lines 45-72\n\n## Estimated Impact\n\n- ~15 lines of duplicated code removed\n- Clearer code intent\n- Single function to maintain","status":"closed","priority":4,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:47:47.756692007-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T06:08:45.903267771-06:00","closed_at":"2026-01-21T06:08:45.903267771-06:00","close_reason":"Closed","labels":["simplification-opportunity"]}
{"id":"markdown-todo-extractor-6uj","title":"Implement CLI output trait for custom formatting","description":"Create trait/function to allow custom output formats (table, yaml, etc.) instead of JSON-only. This would allow a --format flag on commands. Should work with all CLI operations that use automatic registration.","status":"deferred","priority":3,"issue_type":"feature","owner":"jeff@jeffutter.com","created_at":"2026-01-20T19:35:18.972182877-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T10:36:52.735776821-06:00","dependencies":[{"issue_id":"markdown-todo-extractor-6uj","depends_on_id":"markdown-todo-extractor-b68","type":"blocks","created_at":"2026-01-20T19:35:34.167968323-06:00","created_by":"Jeffery Utter"},{"issue_id":"markdown-todo-extractor-6uj","depends_on_id":"markdown-todo-extractor-5i0","type":"blocks","created_at":"2026-01-20T19:35:34.195716504-06:00","created_by":"Jeffery Utter"}],"comments":[{"id":3,"issue_id":"markdown-todo-extractor-6uj","author":"Jeffery Utter","text":"Implementation Guide - CLI Output Trait\n\nCreate a trait/system to allow custom output formats beyond JSON.\n\nDepends on: All CLI operations must use automatic registration first (b68, 5i0)\n\nGoal: Add --format flag to allow output in different formats:\n- json (default, current behavior)\n- table (pretty-printed table for humans)\n- yaml (YAML format)\n- csv (for operations that return lists)\n\nSuggested Approach:\n\n1. Create OutputFormat trait in src/cli_router.rs or new file:\n   trait OutputFormat {\n       fn format(\u0026self, data: \u0026impl Serialize) -\u003e Result\u003cString, Box\u003cdyn Error\u003e\u003e;\n   }\n\n2. Implement for different formats:\n   - JsonFormat (current behavior)\n   - TableFormat (using prettytable-rs or comfy-table)\n   - YamlFormat (using serde_yaml)\n   - CsvFormat (using csv crate, for list-type responses)\n\n3. Add --format global flag to cli_router::build_cli()\n\n4. Update CliOperation trait:\n   - Add format parameter to execute_from_args()\n   - Or return raw data and let router handle formatting\n\n5. Update cli_router::execute_cli() to:\n   - Parse --format flag\n   - Create appropriate formatter\n   - Apply formatter to operation output\n\nDesign Decision: \nShould formatting happen IN each operation or AFTER in the router?\n- After in router = cleaner, operations return structured data\n- In operation = more flexible, operation-specific formatting\n\nRecommend: Router handles formatting (cleaner separation of concerns)\n\nTesting:\ncargo run -- tasks /tmp/test-vault --format json\ncargo run -- tasks /tmp/test-vault --format table\ncargo run -- list-tags /tmp/test-vault --format yaml","created_at":"2026-01-21T01:43:36Z"}]}
{"id":"markdown-todo-extractor-7i7","title":"Create generic JSON serialization wrapper for operations","description":"## Problem\n\nEvery HTTP operation has an identical `execute_json` implementation (~19 lines each) that:\n1. Deserializes request from `serde_json::Value`\n2. Calls the capability method\n3. Serializes response back to `serde_json::Value`\n\nThis pattern is **repeated identically in all 6 operations**:\n- `SearchTasksOperation` (tasks.rs)\n- `ExtractTagsOperation`, `ListTagsOperation`, `SearchByTagsOperation` (tags.rs)\n- `ListFilesOperation`, `ReadFileOperation` (files.rs)\n\n## Example of duplicated code\n\nFrom `tags.rs` (and nearly identical in 5 other places):\n```rust\nasync fn execute_json(\u0026self, json: serde_json::Value) -\u003e Result\u003cserde_json::Value, ErrorData\u003e {\n    let request: ListTagsRequest = serde_json::from_value(json).map_err(|e| ErrorData {\n        code: rmcp::model::ErrorCode(-32602),\n        message: Cow::from(format!(\"Invalid request parameters: {}\", e)),\n        data: None,\n    })?;\n\n    let response = self.capability.list_tags(request).await?;\n\n    serde_json::to_value(response).map_err(|e| ErrorData {\n        code: rmcp::model::ErrorCode(-32603),\n        message: Cow::from(format!(\"Failed to serialize response: {}\", e)),\n        data: None,\n    })\n}\n```\n\n## Proposed Solution\n\nCreate generic helpers in `http_router.rs` or a shared module:\n\n```rust\npub async fn execute_json_operation\u003cReq, Resp, F, Fut\u003e(\n    json: serde_json::Value,\n    operation: F,\n) -\u003e Result\u003cserde_json::Value, ErrorData\u003e\nwhere\n    Req: DeserializeOwned,\n    Resp: Serialize,\n    F: FnOnce(Req) -\u003e Fut,\n    Fut: Future\u003cOutput = CapabilityResult\u003cResp\u003e\u003e,\n{\n    let request: Req = deserialize_request(json)?;\n    let response = operation(request).await?;\n    serialize_response(response)\n}\n```\n\nThen each operation's `execute_json` becomes a one-liner:\n```rust\nasync fn execute_json(\u0026self, json: Value) -\u003e Result\u003cValue, ErrorData\u003e {\n    execute_json_operation(json, |req| self.capability.list_tags(req)).await\n}\n```\n\n## Estimated Impact\n\n- ~120 lines of duplicated serialization code eliminated\n- Single point of change for serialization logic\n- Consistent error handling across all operations\n\n## Dependencies\n\n- Should be done after \"Extract boilerplate error handling helpers\" for cleaner implementation","status":"closed","priority":2,"issue_type":"task","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:47:14.261216937-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T05:48:04.55133958-06:00","closed_at":"2026-01-21T05:48:04.55133958-06:00","close_reason":"Closed","labels":["simplification-opportunity"],"dependencies":[{"issue_id":"markdown-todo-extractor-7i7","depends_on_id":"markdown-todo-extractor-ie0","type":"blocks","created_at":"2026-01-20T22:47:46.37232196-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-84a","title":"Consider replacing config crate with simpler toml crate","description":"## Problem\n\nThe `config` crate (26KB) provides a comprehensive configuration framework with:\n- Multiple file format support (TOML, JSON, YAML, INI)\n- Environment variable binding\n- Layered configuration sources\n- Type coercion\n\nHowever, this project only uses:\n- TOML file loading from `.markdown-todo-extractor.toml`\n- Manual environment variable handling (done separately)\n\nThe `config` crate may be overkill for this use case.\n\n## Current Usage\n\nFrom `src/config.rs`:\n```rust\nuse config::{Config as ConfigBuilder, File};\n\nlet settings = ConfigBuilder::builder()\n    .add_source(File::with_name(\u0026config_path.to_string_lossy()).required(false))\n    .build()\n    .ok()?;\n\nsettings.try_deserialize::\u003cSelf\u003e().ok()\n```\n\n## Proposed Alternative\n\nUse the `toml` crate directly (~12KB):\n\n```rust\nuse std::fs;\nuse toml;\n\nlet content = fs::read_to_string(\u0026config_path).ok()?;\nlet config: Config = toml::from_str(\u0026content).ok()?;\n```\n\nOr with error handling:\n```rust\npub fn load_from_base_path(base_path: \u0026Path) -\u003e Self {\n    let config_path = base_path.join(\".markdown-todo-extractor.toml\");\n    \n    match fs::read_to_string(\u0026config_path) {\n        Ok(content) =\u003e toml::from_str(\u0026content).unwrap_or_default(),\n        Err(_) =\u003e Self::default(),\n    }\n}\n```\n\n## Trade-offs\n\n**Current (`config` crate):**\n- More flexible if we add JSON/YAML support later\n- Handles missing files gracefully\n- More complex API\n\n**Proposed (`toml` crate):**\n- Simpler, more direct\n- Smaller dependency\n- Explicit error handling\n- Limited to TOML format\n\n## Investigation Required\n\n1. Are there plans to support multiple config formats?\n2. Are other `config` crate features being used that I missed?\n\n## Estimated Impact\n\n- Smaller binary size\n- Simpler code\n- One less complex dependency","status":"open","priority":4,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:48:50.12793518-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T22:48:50.12793518-06:00","labels":["dependencies","simplification-opportunity"]}
{"id":"markdown-todo-extractor-92i","title":"Add test coverage for extractor.rs","description":"## Problem\n\nThe core task extraction module (`src/extractor.rs` - 421 lines) has **zero tests**. This is the most critical module containing:\n\n- Regex pattern matching for task detection (`- [ ]`, `- [x]`, `- [-]`, `- [?]`)\n- Metadata extraction (tags, dates, priorities)\n- Content cleaning (removing metadata from task text)\n- Sub-item detection and parsing\n\nWithout tests, regressions in regex patterns or extraction logic could go unnoticed.\n\n## Key Functions That Need Tests\n\n1. **`parse_task_line()`** - Core task detection\n   - Test various checkbox formats: `- [ ]`, `- [x]`, `- [-]`, `- [?]`\n   - Test edge cases: malformed checkboxes, nested lists, content after checkbox\n\n2. **`extract_tags()`** - Tag extraction\n   - Test single tag: `#work`\n   - Test multiple tags: `#work #urgent`\n   - Test tags with numbers: `#project1`\n   - Test edge case: `#` alone should not match\n\n3. **`extract_due_date()` / `extract_created_date()` / `extract_completed_date()`**\n   - Test emoji format: `üìÖ 2025-12-10`\n   - Test text format: `due: 2025-12-10`\n   - Test function format: `@due(2025-12-10)`\n   - Test invalid dates: `üìÖ not-a-date`\n\n4. **`extract_priority()`**\n   - Test emoji priorities: `‚è´` (urgent), `üîº` (high), `üîΩ` (low), `‚è¨` (lowest)\n   - Test text priority: `priority: high`\n\n5. **`clean_content()`**\n   - Verify metadata is removed from content\n   - Verify actual task text is preserved\n   - Test that cleaning doesn't remove too much\n\n6. **`is_sub_item()` / `parse_sub_item()`**\n   - Test indentation detection\n   - Test nested sub-items\n\n## Suggested Test Structure\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    mod parse_task_line {\n        use super::*;\n        \n        #[test]\n        fn test_unchecked_task() { ... }\n        \n        #[test]\n        fn test_completed_task() { ... }\n        \n        #[test]\n        fn test_cancelled_task() { ... }\n    }\n\n    mod metadata_extraction {\n        use super::*;\n        \n        #[test]\n        fn test_extract_single_tag() { ... }\n        \n        #[test]\n        fn test_extract_due_date_emoji() { ... }\n        // ... etc\n    }\n}\n```\n\n## Estimated Impact\n\n- ~200-300 lines of test code to add\n- Catches regressions in regex patterns\n- Documents expected behavior\n- Enables confident refactoring","status":"closed","priority":3,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:47:47.097212677-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T10:59:45.270192774-06:00","closed_at":"2026-01-21T10:59:45.270192774-06:00","close_reason":"Closed","labels":["simplification-opportunity","testing"]}
{"id":"markdown-todo-extractor-b68","title":"Implement CLI automatic registration for tags operations","description":"Apply CLI automatic registration pattern to 3 tag operations (extract_tags, list_tags, search_by_tags). Follow the same pattern used for tasks: add Parser derives to request structs, implement CliOperation for operation structs, update create_cli_operations() in capabilities/mod.rs.","status":"closed","priority":2,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T19:35:09.492684048-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T20:35:58.007247413-06:00","closed_at":"2026-01-20T20:35:58.007247413-06:00","close_reason":"Closed","comments":[{"id":1,"issue_id":"markdown-todo-extractor-b68","author":"Jeffery Utter","text":"Implementation Guide - Tags Operations\n\nFollow the CLI automatic registration pattern documented in CLAUDE.md section \"Adding CLI Automatic Registration for an Operation\".\n\nReference: src/capabilities/tasks.rs SearchTasksOperation for working example\n\nOperations to convert (3 total):\n1. ExtractTagsOperation (extract_tags) - CLI name: \"tags\"\n2. ListTagsOperation (list_tags) - CLI name: \"list-tags\"  \n3. SearchByTagsOperation (search_by_tags) - CLI name: \"search-by-tags\"\n\nFor EACH operation:\n1. Add Parser derive to request struct (ExtractTagsRequest, ListTagsRequest, SearchByTagsRequest)\n   - Import: use clap::{CommandFactory, FromArgMatches, Parser};\n   - Add Parser to #[derive(...)]\n   - Add #[command(name = \"...\", about = \"...\")] \n   - Add path field: path: Option\u003cPathBuf\u003e with proper annotations\n   - Add #[arg(...)] to all existing fields\n\n2. Implement CliOperation trait for operation struct\n   - command_name() - use existing CLI_NAME constant\n   - get_command() - call RequestStruct::command()\n   - execute_from_args() - parse args, handle path, call capability method, return JSON\n\n3. Register in create_cli_operations() in src/capabilities/mod.rs\n   - Uncomment the Arc::new(...) lines for tag operations\n\n4. Update src/cli.rs - remove manual routing\n   - Remove Tags/ListTags/SearchByTags from Commands enum\n   - Remove command structs (if any separate from request structs)\n   - Remove match arms from run_cli()\n   - Update the check at top of run_cli() to include these commands\n\nTesting:\ncargo run -- tags /tmp/test-vault\ncargo run -- list-tags /tmp/test-vault --min-count 2\ncargo run -- search-by-tags /tmp/test-vault --tags work","created_at":"2026-01-21T01:41:36Z"}]}
{"id":"markdown-todo-extractor-cpb","title":"Remove unused Capability trait or document its purpose","description":"## Problem\n\nThe `Capability` trait in `src/capabilities/mod.rs` defines methods that are **never called**:\n\n```rust\n#[allow(dead_code)]\npub trait Capability: Send + Sync + 'static {\n    fn id(\u0026self) -\u003e \u0026'static str;\n    fn description(\u0026self) -\u003e \u0026'static str;\n}\n```\n\nEach capability implements this trait:\n- `TaskCapability::id()` returns \"task\"\n- `TagCapability::id()` returns \"tags\"\n- `FileCapability::id()` returns \"files\"\n\nBut these methods are never used in the codebase:\n- Not used for routing\n- Not used for logging\n- Not used for capability discovery\n\nThe `#[allow(dead_code)]` attribute confirms this is known unused code.\n\n## Proposed Solutions\n\n**Option A: Remove the trait entirely**\n- If it serves no purpose, remove it\n- Capabilities don't need a shared trait if they're not used polymorphically\n\n**Option B: Use it for capability discovery**\n- Create a method to list all capabilities by id/description\n- Could be useful for MCP introspection or debugging\n\n**Option C: Document intended future use**\n- Add TODO comment explaining planned functionality\n- Or create a ticket for the feature that would use it\n\n## Locations\n\n- `src/capabilities/mod.rs` lines 19-25 (trait definition)\n- `src/capabilities/tasks.rs` (impl block)\n- `src/capabilities/tags.rs` (impl block)\n- `src/capabilities/files.rs` (impl block)\n\n## Estimated Impact\n\n- ~20 lines of unused trait code removed\n- Eliminates `#[allow(dead_code)]` on trait\n- Cleaner capability definitions","status":"open","priority":4,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:48:49.786524936-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T22:48:49.786524936-06:00","labels":["simplification-opportunity"]}
{"id":"markdown-todo-extractor-i6y","title":"Merge HttpOperation and CliOperation traits","description":"## Problem\n\nThere are two separate operation traits that each operation must implement:\n\n1. **HttpOperation** (http_router.rs lines 15-28):\n   - `path(\u0026self) -\u003e \u0026'static str`\n   - `description(\u0026self) -\u003e \u0026'static str`\n   - `execute_json(\u0026self, json: Value) -\u003e Result\u003cValue, ErrorData\u003e`\n\n2. **CliOperation** (cli_router.rs lines 5-25):\n   - `command_name(\u0026self) -\u003e \u0026'static str`\n   - `get_command(\u0026self) -\u003e clap::Command`\n   - `execute_from_args(\u0026self, matches: \u0026ArgMatches, registry: \u0026CapabilityRegistry) -\u003e Result\u003cString, ...\u003e`\n\nBoth traits require similar metadata (`path`/`command_name`, `description`) and result in:\n- Every operation implementing both traits (duplicate method signatures)\n- Two separate registration lists in `CapabilityRegistry`:\n  - `create_http_operations()` (mod.rs lines 106-118)\n  - `create_cli_operations()` (mod.rs lines 124-136)\n\n## Proposed Solution\n\nCreate a unified `Operation` trait:\n\n```rust\npub trait Operation: Send + Sync + 'static {\n    /// Unique identifier for the operation (used as HTTP path and CLI command name)\n    fn name(\u0026self) -\u003e \u0026'static str;\n    \n    /// Human-readable description\n    fn description(\u0026self) -\u003e \u0026'static str;\n    \n    /// Get the clap Command for CLI parsing\n    fn cli_command(\u0026self) -\u003e clap::Command;\n    \n    /// Execute with JSON input (for HTTP/MCP)\n    async fn execute_json(\u0026self, json: Value) -\u003e Result\u003cValue, ErrorData\u003e;\n    \n    /// Execute from CLI arguments\n    async fn execute_cli(\u0026self, matches: \u0026ArgMatches) -\u003e Result\u003cString, Box\u003cdyn Error\u003e\u003e;\n}\n```\n\nThen have a single registration:\n```rust\npub fn operations(\u0026self) -\u003e Vec\u003cArc\u003cdyn Operation\u003e\u003e {\n    vec![\n        Arc::new(SearchTasksOperation::new(self.tasks())),\n        Arc::new(ListTagsOperation::new(self.tags())),\n        // ... etc\n    ]\n}\n```\n\nAnd the routers can use the same list:\n```rust\n// HTTP router\nfor op in registry.operations() {\n    router = router.route(\u0026format!(\"/{}\", op.name()), post(/* ... */));\n}\n\n// CLI router\nfor op in registry.operations() {\n    let cmd = op.cli_command().name(op.name());\n    app = app.subcommand(cmd);\n}\n```\n\n## Estimated Impact\n\n- ~80 lines of duplicate trait implementations eliminated\n- Single source of truth for operation registration\n- Simplified architecture (one trait instead of two)\n\n## Considerations\n\n- Some operations might be HTTP-only or CLI-only in the future. Could use default implementations that return \"not supported\" error.\n- The current separation was intentional but the duplication cost outweighs the flexibility benefit at current scale.\n\n## Dependencies\n\n- Should be done after JSON serialization wrapper and CLI path handling consolidation","status":"closed","priority":2,"issue_type":"task","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:47:14.921767074-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T06:04:20.221270645-06:00","closed_at":"2026-01-21T06:04:20.221270645-06:00","close_reason":"Closed","labels":["simplification-opportunity"],"dependencies":[{"issue_id":"markdown-todo-extractor-i6y","depends_on_id":"markdown-todo-extractor-7i7","type":"blocks","created_at":"2026-01-20T22:47:46.718002424-06:00","created_by":"Jeffery Utter"},{"issue_id":"markdown-todo-extractor-i6y","depends_on_id":"markdown-todo-extractor-2ko","type":"blocks","created_at":"2026-01-20T22:47:46.747670222-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-ie0","title":"Extract boilerplate error handling helpers","description":"## Problem\n\nError handling is verbose and repetitive across the codebase. The same `ErrorData` construction pattern appears **20+ times**:\n\n```rust\n.map_err(|e| ErrorData {\n    code: ErrorCode(-32603),\n    message: Cow::from(format!(\"Failed to extract tasks: {}\", e)),\n    data: None,\n})?\n```\n\n## Locations\n\n- `src/capabilities/tasks.rs` (lines ~116-119, and similar patterns)\n- `src/capabilities/tags.rs` (multiple locations in operation impls)\n- `src/capabilities/files.rs` (multiple locations in operation impls)\n\n## Proposed Solution\n\nCreate helper functions in a shared module (e.g., `src/error.rs` or in `capabilities/mod.rs`):\n\n```rust\npub fn json_error(code: i32, msg: impl Into\u003cString\u003e) -\u003e ErrorData {\n    ErrorData {\n        code: ErrorCode(code),\n        message: Cow::from(msg.into()),\n        data: None,\n    }\n}\n\npub fn internal_error(msg: impl Into\u003cString\u003e) -\u003e ErrorData {\n    json_error(-32603, msg)\n}\n\npub fn invalid_params(msg: impl Into\u003cString\u003e) -\u003e ErrorData {\n    json_error(-32602, msg)\n}\n```\n\n## Estimated Impact\n\n- ~100 lines of duplicated error handling code eliminated\n- Single place to update error formatting\n- Better error categorization (use proper JSON-RPC error codes)\n\n## Research Notes\n\nAll internal errors currently use error code `-32603` (internal error). With proper helpers, we could distinguish:\n- `-32602` for invalid input\n- `-32603` for server errors\n- Custom codes for specific error types if needed","status":"closed","priority":2,"issue_type":"task","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:47:13.943209847-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T05:44:49.500541622-06:00","closed_at":"2026-01-21T05:44:49.500541622-06:00","close_reason":"Closed","labels":["simplification-opportunity"]}
{"id":"markdown-todo-extractor-j56","title":"Simplify CapabilityRegistry lazy initialization","description":"## Problem\n\nThe `CapabilityRegistry` uses `OnceLock` for lazy initialization of capabilities:\n\n```rust\npub struct CapabilityRegistry {\n    base_path: PathBuf,\n    config: Arc\u003cConfig\u003e,\n    task_capability: OnceLock\u003cArc\u003cTaskCapability\u003e\u003e,\n    tag_capability: OnceLock\u003cArc\u003cTagCapability\u003e\u003e,\n    file_capability: OnceLock\u003cArc\u003cFileCapability\u003e\u003e,\n}\n\npub fn tasks(\u0026self) -\u003e Arc\u003cTaskCapability\u003e {\n    self.task_capability\n        .get_or_init(|| {\n            Arc::new(TaskCapability::new(\n                self.base_path.clone(),\n                Arc::clone(\u0026self.config),\n            ))\n        })\n        .clone()\n}\n```\n\nThis pattern:\n1. Adds complexity with `OnceLock` + `Arc` + `.clone()` on every access\n2. Is designed for dynamic/lazy loading but capabilities are always loaded\n3. Requires initialization closure to clone `base_path` and `Arc` each time\n\n## Proposed Solution\n\nIf dynamic capability loading is not a requirement, simplify to eager initialization:\n\n```rust\npub struct CapabilityRegistry {\n    tasks: Arc\u003cTaskCapability\u003e,\n    tags: Arc\u003cTagCapability\u003e,\n    files: Arc\u003cFileCapability\u003e,\n}\n\nimpl CapabilityRegistry {\n    pub fn new(base_path: PathBuf) -\u003e Self {\n        let config = Arc::new(Config::load_from_base_path(\u0026base_path));\n        Self {\n            tasks: Arc::new(TaskCapability::new(base_path.clone(), Arc::clone(\u0026config))),\n            tags: Arc::new(TagCapability::new(base_path.clone(), Arc::clone(\u0026config))),\n            files: Arc::new(FileCapability::new(base_path, config)),\n        }\n    }\n\n    pub fn tasks(\u0026self) -\u003e Arc\u003cTaskCapability\u003e {\n        Arc::clone(\u0026self.tasks)\n    }\n    // ... similar for others\n}\n```\n\n## Trade-offs\n\n**Current approach benefits:**\n- Lazy initialization (only create capabilities when used)\n- Supports potential future dynamic capability loading\n\n**Proposed approach benefits:**\n- Simpler code (no OnceLock complexity)\n- No closure allocation per access\n- Fail-fast initialization (errors at startup vs first use)\n\n## Investigation Required\n\n1. Are there plans for dynamic capability loading?\n2. Is lazy initialization providing measurable benefit?\n3. Are any capabilities unused in typical workflows?\n\n## Estimated Impact\n\n- ~30 lines of simpler code\n- Removes `OnceLock` dependency\n- Clearer initialization flow","status":"open","priority":4,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:48:19.582640382-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T22:48:19.582640382-06:00","labels":["simplification-opportunity"]}
{"id":"markdown-todo-extractor-sdi","title":"Change file list output to visual rather than json","description":"Change the format of the file list output to an indented list.\n\nDirectory structure format:\n- One entry per line\n- Directories end with /\n- Children are indented 2 spaces relative to their parent\n- Files have no trailing slash\n\n## Example:\n\n```\nproject/\n  src/\n    main.py\n    utils.py\n  README.md\n```\n\n## Implementation Plan\n\n### 1. Create Visual Tree Formatter Function\n**Location**: `src/capabilities/files.rs`\n\nAdd a new function `format_tree_visual()` that converts a `FileTreeNode` to the indented visual format:\n\n```rust\nfn format_tree_visual(node: \u0026FileTreeNode, indent_level: usize) -\u003e String {\n    let mut output = String::new();\n    let indent = \"  \".repeat(indent_level);\n    \n    // Add current node\n    if node.is_directory {\n        output.push_str(\u0026format!(\"{}{}/\\n\", indent, node.name));\n    } else {\n        output.push_str(\u0026format!(\"{}{}\\n\", indent, node.name));\n    }\n    \n    // Recursively add children\n    for child in \u0026node.children {\n        output.push_str(\u0026format_tree_visual(child, indent_level + 1));\n    }\n    \n    output\n}\n```\n\n**Key implementation details**:\n- Use 2 spaces per indent level (not tabs)\n- Directories get a trailing `/`\n- Files have no trailing slash\n- Recursively process children with incremented indent level\n\n### 2. Add Visual Output Option to Response\n**Location**: `src/capabilities/files.rs`\n\nTwo approaches to consider:\n\n**Option A: Replace JSON response entirely**\n- Change `ListFilesResponse` to contain a `visual_tree: String` field\n- Remove or make optional the `root: FileTreeNode` field\n- CLI always outputs visual format\n- HTTP/MCP also outputs visual format\n\n**Option B: Add format parameter (more flexible)**\n- Add `format: Option\u003cString\u003e` to `ListFilesRequest` (values: \"json\", \"visual\")\n- Keep existing `FileTreeNode` structure\n- Add optional `visual_tree: Option\u003cString\u003e` to `ListFilesResponse`\n- Let caller choose format\n\n**Recommendation**: Start with Option A for simplicity. Can add Option B later if needed.\n\n### 3. Update CLI Output\n**Location**: `src/capabilities/files.rs:341-362` (ListFilesOperation CLI impl)\n\nChange the CLI operation to output the visual tree directly:\n\n```rust\nasync fn execute_from_args(...) -\u003e Result\u003cString, Box\u003cdyn std::error::Error\u003e\u003e {\n    // ... existing request parsing ...\n    \n    let response = /* ... get response ... */;\n    \n    // Output visual tree instead of JSON\n    Ok(response.visual_tree)\n}\n```\n\n**Current code**: Line 361 serializes to JSON\n**New code**: Return the visual tree string directly\n\n### 4. Update list_files Method\n**Location**: `src/capabilities/files.rs:121-177`\n\nAfter building the file tree with `build_file_tree()`:\n\n```rust\n// Build the file tree (existing code)\nlet (root, total_files, total_directories) = build_file_tree(...)?;\n\n// Generate visual representation\nlet visual_tree = format_tree_visual(\u0026root, 0);\n\nOk(ListFilesResponse {\n    visual_tree,\n    total_files,\n    total_directories,\n})\n```\n\n### 5. Update Response Struct\n**Location**: `src/capabilities/files.rs:56-62`\n\n```rust\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct ListFilesResponse {\n    pub visual_tree: String,\n    pub total_files: usize,\n    pub total_directories: usize,\n}\n```\n\n### 6. Handle Root Name Edge Case\nThe root directory name needs special handling:\n- If listing vault root, show the vault's directory name\n- If listing a subpath, show that subpath's name\n- Ensure consistent behavior with how `build_file_tree()` names the root\n\n### 7. Testing Strategy\n\nManual testing commands:\n```bash\n# Test basic listing\ncargo run -- list-files /path/to/vault\n\n# Test subpath\ncargo run -- list-files /path/to/vault --path \"subfolder\"\n\n# Test max depth\ncargo run -- list-files /path/to/vault --max-depth 2\n```\n\nExpected output format:\n```\nvault/\n  folder1/\n    file1.md\n    file2.md\n  folder2/\n    nested/\n      deep.md\n  root-file.md\n```\n\n### 8. Files to Modify\n\n1. `src/capabilities/files.rs`:\n   - Add `format_tree_visual()` function (~20 lines)\n   - Update `ListFilesResponse` struct (1 line change)\n   - Update `list_files()` method (add 2 lines)\n   - Update CLI `execute_from_args()` (change line 361)\n\n### 9. Backward Compatibility Considerations\n\n**Breaking changes**:\n- HTTP/MCP clients expecting JSON structure will break\n- This is acceptable if no external clients exist yet\n\n**If backward compatibility needed**:\n- Use Option B (format parameter) instead\n- Default to \"visual\" for CLI\n- Keep \"json\" as default for HTTP/MCP initially\n\n### 10. Edge Cases to Handle\n\n1. **Empty directories**: Still show with `/` suffix\n2. **Single file**: Should not have indent (at root level)\n3. **Deep nesting**: Verify indent math is correct\n4. **Special characters in names**: Ensure proper display\n5. **Excluded paths**: Already handled by `build_file_tree()`\n\n### Estimated Complexity\n- Low complexity change\n- ~30 lines of new code\n- ~5 lines of modifications to existing code\n- Main work is the formatter function and testing","status":"closed","priority":2,"issue_type":"feature","owner":"jeff@jeffutter.com","created_at":"2026-01-20T12:28:37.799886872-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T22:02:51.313707474-06:00","closed_at":"2026-01-20T22:02:51.313707474-06:00","close_reason":"Closed"}
{"id":"markdown-todo-extractor-tx9","title":"Create Additional Tools","description":"# Create Additional Tools\n\nThis epic encompasses the creation of additional LLM tools to enhance the knowledge base interaction capabilities. The tools extend the existing RAG search functionality to provide more granular access to vault contents.\n\n## Overview\n\nThis epic adds four complementary tools that give the LLM more ways to explore and access the Obsidian vault:\n\n| Tool | Purpose | Child Ticket |\n|------|---------|--------------|\n| **read_file** | Read the full content of a specific file | tx9.2 |\n| **list_files** | Browse the directory structure | tx9.3 |\n| **list_tags** | Discover available tags with statistics | tx9.4 |\n| **search_by_tags** | Find files matching specific tags | tx9.5 |\n\n## Implementation Plan\n\n### Architecture Approach\n\nAll four tools follow the existing patterns established in `src/mcp.rs`:\n\n1. **MCP Tool Registration**: Use the `#[tool]` macro from `rmcp` to register each tool with the `TaskSearchService`\n2. **REST API Endpoints**: Add corresponding HTTP handlers in `main.rs` for the HTTP MCP server mode\n3. **Shared Extractors**: Reuse and extend existing extractors (`TaskExtractor`, `TagExtractor`) where applicable\n4. **Configuration Integration**: All tools should respect the existing `Config` path exclusion patterns\n\n### File Organization\n\nThe implementation will modify existing modules following the current structure:\n\n| File | New Additions |\n|------|---------------|\n| `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs` | 4 new tool methods, 8 new request/response structs |\n| `/home/jeffutter/src/markdown-todo-extractor/src/main.rs` | 4 new HTTP handler pairs, 4 new routes, updated tools_handler |\n| `/home/jeffutter/src/markdown-todo-extractor/src/tag_extractor.rs` | `TagCount` struct, `TaggedFile` struct, `extract_tags_with_counts()`, `search_by_tags()`, Config integration |\n| `/home/jeffutter/src/markdown-todo-extractor/src/cli.rs` | `SearchByTags` subcommand |\n| `/home/jeffutter/src/markdown-todo-extractor/Cargo.toml` | `tempfile` dev dependency |\n\n### Common Patterns\n\nAll tools should:\n\n1. **Accept paths relative to the base path** (vault root) with optional subpath parameter\n2. **Return JSON responses** with consistent error handling via `ErrorData` for MCP and HTTP status codes for REST\n3. **Support both MCP stdio and HTTP modes** with identical functionality\n4. **Include comprehensive tool descriptions** for LLM consumption via `#[tool(description = \"...\")]`\n5. **Use `JsonSchema` derive** for automatic schema generation\n6. **Respect `Config.exclude_paths`** for path exclusions (critical for tx9.3, tx9.4, tx9.5)\n\n### Shared Infrastructure Changes\n\nBefore implementing individual tools, these cross-cutting changes are required:\n\n#### 1. TagExtractor Config Integration (Required by tx9.4, tx9.5)\n\nThe current `TagExtractor` is a simple unit struct without configuration. It needs to be updated to:\n- Accept `Arc\u003cConfig\u003e` in constructor (matching `TaskExtractor` pattern)\n- Pass config to `collect_markdown_files()` function\n- Apply path exclusions during file collection\n\nThis change affects:\n- `src/tag_extractor.rs`: Add config field and update methods\n- `src/mcp.rs`: Update `TaskSearchService::new()` to pass config\n- `src/main.rs`: Update `AppState` initialization\n- `src/cli.rs`: Update Tags command to create TagExtractor with config\n\n#### 2. Dev Dependency Addition\n\nAdd `tempfile = \"3\"` to `Cargo.toml` for unit tests across all tools.\n\n### Execution Order\n\nThe tools can be implemented in parallel, but for optimized development with code reuse:\n\n```\nPhase 1 (Independent - can run in parallel):\n  ‚îú‚îÄ‚îÄ tx9.2: read_file tool (standalone, no dependencies)\n  ‚îî‚îÄ‚îÄ tx9.3: list_files tool (standalone, no dependencies)\n\nPhase 2 (After TagExtractor Config Integration):\n  ‚îú‚îÄ‚îÄ tx9.4: list_tags tool (extends TagExtractor with counting)\n  ‚îî‚îÄ‚îÄ tx9.5: search_by_tags tool (extends TagExtractor with search)\n```\n\n**Recommended single-developer sequence:**\n1. **tx9.2 (read_file)** - Simplest tool, establishes the pattern\n2. **tx9.3 (list_files)** - Standalone, no extractor dependencies\n3. **TagExtractor Config Integration** - Shared infrastructure for tx9.4 and tx9.5\n4. **tx9.4 (list_tags)** - Adds counting to TagExtractor\n5. **tx9.5 (search_by_tags)** - Can reuse tag extraction logic from tx9.4\n\n### Security Considerations\n\nAll file-reading tools (tx9.2, tx9.3) must implement path traversal protection:\n\n```rust\n// Pattern for all file-accessing tools\nlet canonical_base = self.base_path.canonicalize()?;\nlet canonical_full = full_path.canonicalize()?;\n\nif !canonical_full.starts_with(\u0026canonical_base) {\n    return Err(ErrorData {\n        code: ErrorCode(-32602),\n        message: Cow::from(\"Invalid path: path must be within the vault\"),\n        data: None,\n    });\n}\n```\n\nFor tx9.2 (read_file), additionally restrict to `.md` files only.\n\n### Testing Strategy\n\nEach tool should include:\n\n1. **Unit tests** for core logic (in respective module)\n2. **Integration tests** for MCP tool invocation (if feasible)\n3. **Manual testing** with CLI and MCP stdio modes\n\nCommon test scenarios across all tools:\n- Happy path with valid inputs\n- Empty/missing inputs handled gracefully\n- Path exclusions respected (tx9.3, tx9.4, tx9.5)\n- Security validation (path traversal blocked in tx9.2, tx9.3)\n- Large vault handling (tx9.3 may need truncation)\n\n### API Endpoint Summary\n\nAfter all tools are implemented, the HTTP server will expose:\n\n| Endpoint | Tool | Method |\n|----------|------|--------|\n| `/api/tasks` | search_tasks | GET/POST |\n| `/api/tags` | extract_tags | GET/POST |\n| `/api/file` | read_file | GET/POST |\n| `/api/files` | list_files | GET/POST |\n| `/api/tags/list` | list_tags | GET/POST |\n| `/api/search_by_tags` | search_by_tags | GET/POST |\n\n### Success Criteria\n\n- [ ] All four tools implemented with MCP and HTTP interfaces\n- [ ] All tools respect path exclusion configuration\n- [ ] Security: Path traversal attacks blocked\n- [ ] Unit tests pass for all new functionality\n- [ ] `cargo build --release` succeeds\n- [ ] `cargo clippy` passes\n- [ ] `cargo fmt --check` passes\n- [ ] Manual testing confirms tools work in both stdio and HTTP modes\n\n### Child Ticket Summary\n\nEach child ticket has a detailed implementation plan:\n\n| Ticket | Status | Key Implementation Details |\n|--------|--------|---------------------------|\n| **tx9.2** | Planned | `ReadFileRequest`/`ReadFileResponse`, path validation, .md restriction |\n| **tx9.3** | Planned | `ListFilesRequest`/`ListFilesResponse`/`FileTreeNode`, hierarchical tree, size limiting |\n| **tx9.4** | Planned | `TagCount` struct, `extract_tags_with_counts()`, sort by frequency |\n| **tx9.5** | Planned | `TaggedFile` struct, `search_by_tags()`, AND/OR logic, CLI subcommand |\n\nSee individual child tickets for complete implementation details.","status":"closed","priority":2,"issue_type":"epic","owner":"jeff@jeffutter.com","created_at":"2026-01-19T18:43:26.983361979-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-19T22:35:17.868790617-06:00","closed_at":"2026-01-19T22:35:17.868790617-06:00","close_reason":"Closed","labels":["planned"]}
{"id":"markdown-todo-extractor-tx9.2","title":"Create File Read Tool","description":"# Create File Read Tool\n\nCreate a tool that returns the contents of a file at a given path.\n\n## Requirements\n\n- The tool should return the contents of the file at a given path\n- See this for ideas: https://github.com/logancyang/obsidian-copilot/blob/master/src/tools/NoteTools.ts\n\n---\n\n## Implementation Plan\n\n### Overview\n\nThis tool will allow MCP clients to read the full contents of a markdown file from the Obsidian vault. The implementation follows the existing patterns in the codebase for MCP tools (see `search_tasks` and `extract_tags` in `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs`).\n\n### Design Decisions\n\n1. **Path Handling**: Accept a relative path (relative to the vault base path). This follows the pattern used by Obsidian Copilot's NoteTools.ts which requires paths relative to vault root.\n\n2. **Security**: Validate that the resolved path is within the base path to prevent path traversal attacks (e.g., `../../../etc/passwd`).\n\n3. **File Extension**: Only allow reading `.md` files to stay consistent with the tool's purpose as a markdown task extractor.\n\n4. **Response Format**: Return a structured response with:\n   - `content`: The file contents\n   - `file_path`: The resolved path (relative to vault)\n   - `file_name`: The file name only\n   - Optionally: metadata like modification time\n\n5. **Error Handling**: Follow existing patterns using `ErrorData` with appropriate error codes:\n   - Invalid path (path traversal attempt)\n   - File not found\n   - File not a markdown file\n   - Read error\n\n6. **No Chunking Initially**: Unlike Obsidian Copilot which chunks large files into 200-line segments, start with a simpler implementation that returns the full file. Chunking can be added later if needed for very large files.\n\n### Implementation Steps\n\n#### Step 1: Add Request/Response Types in `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs`\n\nAdd new structs after the existing `ExtractTagsResponse`:\n\n```rust\n/// Parameters for the read_file tool\n#[derive(Debug, Deserialize, JsonSchema)]\npub struct ReadFileRequest {\n    #[schemars(description = \"Path to the file relative to the vault root (e.g., 'Notes/my-note.md')\")]\n    pub path: String,\n}\n\n/// Response for the read_file tool\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct ReadFileResponse {\n    /// The full content of the file\n    pub content: String,\n    /// The file path relative to the vault root\n    pub file_path: String,\n    /// Just the file name\n    pub file_name: String,\n}\n```\n\n#### Step 2: Add the MCP Tool Method in `TaskSearchService`\n\nAdd a new method with the `#[tool]` attribute inside the `#[tool_router] impl TaskSearchService` block:\n\n```rust\n#[tool(\n    description = \"Read the full contents of a markdown file from the vault\"\n)]\nasync fn read_file(\n    \u0026self,\n    Parameters(request): Parameters\u003cReadFileRequest\u003e,\n) -\u003e Result\u003cJson\u003cReadFileResponse\u003e, ErrorData\u003e {\n    // 1. Construct the full path\n    let requested_path = PathBuf::from(\u0026request.path);\n    let full_path = self.base_path.join(\u0026requested_path);\n\n    // 2. Canonicalize paths for security check\n    let canonical_base = self.base_path.canonicalize().map_err(|e| ErrorData {\n        code: ErrorCode(-32603),\n        message: Cow::from(format!(\"Failed to resolve base path: {}\", e)),\n        data: None,\n    })?;\n\n    let canonical_full = full_path.canonicalize().map_err(|e| ErrorData {\n        code: ErrorCode(-32602), // Invalid params\n        message: Cow::from(format!(\"File not found: {}\", request.path)),\n        data: None,\n    })?;\n\n    // 3. Security: Ensure path is within base directory\n    if !canonical_full.starts_with(\u0026canonical_base) {\n        return Err(ErrorData {\n            code: ErrorCode(-32602),\n            message: Cow::from(\"Invalid path: path must be within the vault\"),\n            data: None,\n        });\n    }\n\n    // 4. Validate it's a markdown file\n    if canonical_full.extension().and_then(|s| s.to_str()) != Some(\"md\") {\n        return Err(ErrorData {\n            code: ErrorCode(-32602),\n            message: Cow::from(\"Invalid file type: only .md files can be read\"),\n            data: None,\n        });\n    }\n\n    // 5. Read the file content\n    let content = std::fs::read_to_string(\u0026canonical_full).map_err(|e| ErrorData {\n        code: ErrorCode(-32603),\n        message: Cow::from(format!(\"Failed to read file: {}\", e)),\n        data: None,\n    })?;\n\n    // 6. Get relative path for response\n    let relative_path = canonical_full\n        .strip_prefix(\u0026canonical_base)\n        .unwrap_or(\u0026canonical_full)\n        .to_string_lossy()\n        .to_string();\n\n    let file_name = canonical_full\n        .file_name()\n        .unwrap_or_default()\n        .to_string_lossy()\n        .to_string();\n\n    Ok(Json(ReadFileResponse {\n        content,\n        file_path: relative_path,\n        file_name,\n    }))\n}\n```\n\n#### Step 3: Add HTTP REST Endpoint (Optional but Recommended)\n\nFor consistency with existing endpoints, add REST handlers in `/home/jeffutter/src/markdown-todo-extractor/src/main.rs`:\n\n1. Add handler functions:\n\n```rust\n/// HTTP handler for reading a file (GET with query params)\nasync fn file_handler_get(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    query: axum::extract::Query\u003cmcp::ReadFileRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ReadFileResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    read_file_impl(state, query.0).await\n}\n\n/// HTTP handler for reading a file (POST with JSON body)\nasync fn file_handler_post(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    axum::Json(request): axum::Json\u003cmcp::ReadFileRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ReadFileResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    read_file_impl(state, request).await\n}\n\n/// Shared implementation for file reading\nasync fn read_file_impl(\n    state: AppState,\n    request: mcp::ReadFileRequest,\n) -\u003e Result\u003caxum::Json\u003cmcp::ReadFileResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    // Similar validation logic as the MCP handler\n    let requested_path = std::path::PathBuf::from(\u0026request.path);\n    let full_path = state.base_path.join(\u0026requested_path);\n\n    // Security: canonicalize and check path\n    let canonical_base = state.base_path.canonicalize().map_err(|e| {\n        (axum::http::StatusCode::INTERNAL_SERVER_ERROR, format!(\"Failed to resolve base path: {}\", e))\n    })?;\n\n    let canonical_full = full_path.canonicalize().map_err(|_| {\n        (axum::http::StatusCode::NOT_FOUND, format!(\"File not found: {}\", request.path))\n    })?;\n\n    if !canonical_full.starts_with(\u0026canonical_base) {\n        return Err((axum::http::StatusCode::BAD_REQUEST, \"Invalid path: path must be within the vault\".to_string()));\n    }\n\n    if canonical_full.extension().and_then(|s| s.to_str()) != Some(\"md\") {\n        return Err((axum::http::StatusCode::BAD_REQUEST, \"Invalid file type: only .md files can be read\".to_string()));\n    }\n\n    let content = std::fs::read_to_string(\u0026canonical_full).map_err(|e| {\n        (axum::http::StatusCode::INTERNAL_SERVER_ERROR, format!(\"Failed to read file: {}\", e))\n    })?;\n\n    let relative_path = canonical_full\n        .strip_prefix(\u0026canonical_base)\n        .unwrap_or(\u0026canonical_full)\n        .to_string_lossy()\n        .to_string();\n\n    let file_name = canonical_full\n        .file_name()\n        .unwrap_or_default()\n        .to_string_lossy()\n        .to_string();\n\n    Ok(axum::Json(mcp::ReadFileResponse {\n        content,\n        file_path: relative_path,\n        file_name,\n    }))\n}\n```\n\n2. Add route in router:\n\n```rust\n.route(\n    \"/api/file\",\n    axum::routing::get(file_handler_get).post(file_handler_post),\n)\n```\n\n3. Update `tools_handler` to include the new tool schema.\n\n4. Update console output messages.\n\n#### Step 4: Add Unit Tests\n\nAdd tests in a new `#[cfg(test)]` module in `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs` or create a separate test file:\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::fs;\n    use tempfile::TempDir;\n\n    #[tokio::test]\n    async fn test_read_file_success() {\n        let temp_dir = TempDir::new().unwrap();\n        let file_path = temp_dir.path().join(\"test.md\");\n        fs::write(\u0026file_path, \"# Test\\n\\nContent here\").unwrap();\n\n        let service = TaskSearchService::new(temp_dir.path().to_path_buf());\n        let request = ReadFileRequest {\n            path: \"test.md\".to_string(),\n        };\n\n        let result = service.read_file(Parameters(request)).await;\n        assert!(result.is_ok());\n\n        let response = result.unwrap().0;\n        assert_eq!(response.content, \"# Test\\n\\nContent here\");\n        assert_eq!(response.file_name, \"test.md\");\n    }\n\n    #[tokio::test]\n    async fn test_read_file_not_found() {\n        let temp_dir = TempDir::new().unwrap();\n        let service = TaskSearchService::new(temp_dir.path().to_path_buf());\n        let request = ReadFileRequest {\n            path: \"nonexistent.md\".to_string(),\n        };\n\n        let result = service.read_file(Parameters(request)).await;\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_read_file_path_traversal_blocked() {\n        let temp_dir = TempDir::new().unwrap();\n        let service = TaskSearchService::new(temp_dir.path().to_path_buf());\n        let request = ReadFileRequest {\n            path: \"../../../etc/passwd\".to_string(),\n        };\n\n        let result = service.read_file(Parameters(request)).await;\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_read_file_non_markdown_rejected() {\n        let temp_dir = TempDir::new().unwrap();\n        let file_path = temp_dir.path().join(\"test.txt\");\n        fs::write(\u0026file_path, \"Not markdown\").unwrap();\n\n        let service = TaskSearchService::new(temp_dir.path().to_path_buf());\n        let request = ReadFileRequest {\n            path: \"test.txt\".to_string(),\n        };\n\n        let result = service.read_file(Parameters(request)).await;\n        assert!(result.is_err());\n    }\n}\n```\n\nAdd `tempfile` as a dev dependency in `/home/jeffutter/src/markdown-todo-extractor/Cargo.toml`:\n\n```toml\n[dev-dependencies]\ntempfile = \"3\"\n```\n\n### Files to Modify\n\n| File | Changes |\n|------|---------|\n| `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs` | Add `ReadFileRequest`, `ReadFileResponse` structs and `read_file` tool method |\n| `/home/jeffutter/src/markdown-todo-extractor/src/main.rs` | Add HTTP handlers and route for `/api/file` endpoint |\n| `/home/jeffutter/src/markdown-todo-extractor/Cargo.toml` | Add `tempfile` dev dependency for tests |\n\n### Testing Strategy\n\n1. **Unit Tests**: Test the core logic with various scenarios (success, not found, path traversal, non-markdown)\n2. **Manual Testing**: Test with actual MCP client (Claude Desktop or similar)\n   ```bash\n   # Start the server\n   cargo run -- --mcp-stdio /path/to/vault\n   \n   # Or HTTP mode\n   cargo run -- --mcp-http /path/to/vault\n   ```\n\n### Future Enhancements (Out of Scope)\n\n- **Chunking**: For very large files, add optional `chunk_index` parameter similar to Obsidian Copilot\n- **Link Extraction**: Parse and return wiki-style links from the content\n- **Metadata Extraction**: Return YAML frontmatter separately parsed\n- **Line Range**: Allow reading specific line ranges from a file","status":"closed","priority":2,"issue_type":"feature","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-19T18:49:51.850252653-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-19T22:27:03.960128433-06:00","closed_at":"2026-01-19T22:27:03.960128433-06:00","close_reason":"Closed","labels":["planned"],"dependencies":[{"issue_id":"markdown-todo-extractor-tx9.2","depends_on_id":"markdown-todo-extractor-tx9","type":"parent-child","created_at":"2026-01-19T18:49:51.855943654-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-tx9.3","title":"Create file list tool","description":"# Create file list tool\n\nCreate a tool to list the directory tree of the obsidian vault.\n\n- The tree should include all files and folders.\n- Reference this for ideas: https://github.com/logancyang/obsidian-copilot/blob/master/src/tools/FileTreeTools.ts\n\nThis tool will provide the LLM with a complete view of the vault's file structure, enabling it to:\n- Understand the organization of notes and folders\n- Navigate to specific files or folders\n- Discover what documents exist in a particular area\n\n## Implementation Plan\n\n### 1. Add Request/Response Types in `src/mcp.rs`\n\n```rust\n/// Parameters for the list_files tool\n#[derive(Debug, Deserialize, JsonSchema)]\npub struct ListFilesRequest {\n    #[schemars(description = \"Subpath within the vault to list (optional, defaults to vault root)\")]\n    pub path: Option\u003cString\u003e,\n    \n    #[schemars(description = \"Maximum depth to traverse (optional, defaults to unlimited)\")]\n    pub max_depth: Option\u003cusize\u003e,\n    \n    #[schemars(description = \"Include file sizes in output (optional, defaults to false)\")]\n    pub include_sizes: Option\u003cbool\u003e,\n}\n\n/// A node in the file tree\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct FileTreeNode {\n    pub name: String,\n    pub path: String,\n    pub is_directory: bool,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub size_bytes: Option\u003cu64\u003e,\n    #[serde(skip_serializing_if = \"Vec::is_empty\", default)]\n    pub children: Vec\u003cFileTreeNode\u003e,\n}\n\n/// Response for the list_files tool\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct ListFilesResponse {\n    pub root: FileTreeNode,\n    pub total_files: usize,\n    pub total_directories: usize,\n}\n```\n\n### 2. Implement MCP Tool Method in `TaskSearchService`\n\n```rust\n#[tool(description = \"List the directory tree of the vault. Returns a hierarchical view of all files and folders. Useful for understanding vault structure and finding files.\")]\nasync fn list_files(\n    \u0026self,\n    Parameters(request): Parameters\u003cListFilesRequest\u003e,\n) -\u003e Result\u003cJson\u003cListFilesResponse\u003e, ErrorData\u003e {\n    // Resolve the search path\n    let search_path = if let Some(ref subpath) = request.path {\n        self.base_path.join(subpath)\n    } else {\n        self.base_path.clone()\n    };\n    \n    // Validate path is within vault\n    // Build the file tree recursively\n    // Respect Config path exclusions\n    // Apply max_depth if specified\n}\n```\n\n### 3. Helper Function for Tree Building\n\nAdd a helper function to recursively build the tree:\n\n```rust\nfn build_file_tree(\n    path: \u0026Path,\n    base_path: \u0026Path,\n    config: \u0026Config,\n    current_depth: usize,\n    max_depth: Option\u003cusize\u003e,\n    include_sizes: bool,\n) -\u003e Result\u003c(FileTreeNode, usize, usize), Box\u003cdyn std::error::Error\u003e\u003e {\n    // Check depth limit\n    // Check if path should be excluded via config\n    // Read directory entries\n    // Recursively process subdirectories\n    // Collect file entries\n    // Return node with accumulated counts\n}\n```\n\n### 4. Output Format Considerations\n\nTwo output format options:\n\n**Option A: Hierarchical (Recommended)**\n```json\n{\n  \"root\": {\n    \"name\": \"vault\",\n    \"path\": \"\",\n    \"is_directory\": true,\n    \"children\": [\n      {\"name\": \"Projects\", \"path\": \"Projects\", \"is_directory\": true, \"children\": [...]},\n      {\"name\": \"note.md\", \"path\": \"note.md\", \"is_directory\": false}\n    ]\n  },\n  \"total_files\": 150,\n  \"total_directories\": 25\n}\n```\n\n**Option B: Flat List (Alternative)**\n```json\n{\n  \"files\": [\"Projects/plan.md\", \"Notes/idea.md\"],\n  \"directories\": [\"Projects\", \"Notes\"],\n  \"total_files\": 150,\n  \"total_directories\": 25\n}\n```\n\nUse hierarchical format as it better represents the tree structure and matches the reference implementation.\n\n### 5. Size Management\n\nLike the obsidian-copilot reference, implement size limits:\n- If the JSON response exceeds a threshold (e.g., 500KB), simplify by:\n  - Omitting file lists and showing only directory structure\n  - Or truncating at a certain depth\n- Add a `truncated` field to indicate if output was limited\n\n### 6. Add HTTP Endpoint in `main.rs`\n\n```rust\n/// HTTP handler for listing files (GET)\nasync fn list_files_handler_get(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    query: axum::extract::Query\u003cmcp::ListFilesRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ListFilesResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    list_files_impl(state, query.0).await\n}\n\n/// HTTP handler for listing files (POST)\nasync fn list_files_handler_post(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    axum::Json(request): axum::Json\u003cmcp::ListFilesRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ListFilesResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    list_files_impl(state, request).await\n}\n```\n\nAdd route:\n```rust\n.route(\n    \"/api/files\",\n    axum::routing::get(list_files_handler_get).post(list_files_handler_post),\n)\n```\n\n### 7. Update Tools Handler\n\nAdd to `/tools` endpoint:\n```rust\nlet list_files_schema = schema_for!(ListFilesRequest);\n// Add to tools array\n```\n\n### 8. Configuration Integration\n\nThe tool should respect the existing `Config.exclude_paths` patterns, filtering out excluded directories and files from the tree output.\n\n### 9. Testing\n\n- Test tree building for nested directories\n- Test max_depth limiting\n- Test path exclusion via Config\n- Test empty directory handling\n- Test size limiting for large vaults\n\n### Files to Modify\n\n1. `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs` - Add tool and types\n2. `/home/jeffutter/src/markdown-todo-extractor/src/main.rs` - Add HTTP handlers and routes","status":"closed","priority":2,"issue_type":"feature","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-19T18:50:16.158158253-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-19T22:29:03.667089571-06:00","closed_at":"2026-01-19T22:29:03.667089571-06:00","close_reason":"Closed","labels":["planned"],"dependencies":[{"issue_id":"markdown-todo-extractor-tx9.3","depends_on_id":"markdown-todo-extractor-tx9","type":"parent-child","created_at":"2026-01-19T18:50:16.15909786-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-tx9.4","title":"Tag List Tool","description":"# Tag List Tool\n\nCreate a tool to list all tags with their document counts.\n\n## Requirements\n\n- In the list, include the tag name and number of documents that reference it.\n- Tags should be pulled from the vault\n- Tags are in the YAML frontmatter of the files in the `tags` key\n- Reference this for ideas: https://github.com/logancyang/obsidian-copilot/blob/master/src/tools/TagTools.ts\n\nThis tool will provide the LLM with a complete inventory of all tags in the vault, along with statistics on how many documents reference each tag. This enables:\n- Understanding the tag taxonomy used in the vault\n- Finding commonly used vs. rarely used tags\n- Discovering topics with the most content\n- Supporting tag-based search and filtering decisions\n\n## Implementation Plan\n\n### Overview\n\nAdd a new `list_tags` MCP tool that returns all tags from YAML frontmatter with document counts. The existing `extract_tags` tool returns only unique tag names; this new tool adds statistics. This follows the pattern established by the Obsidian Copilot TagTools.ts reference, but is focused on frontmatter tags only (as specified in the requirements).\n\n### 1. Add `TagCount` Struct and Counting Method to `src/tag_extractor.rs`\n\nAdd a new struct to hold tag statistics and a method to extract tags with counts.\n\n**Add imports at the top of the file:**\n```rust\nuse schemars::JsonSchema;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n```\n\n**Add the `TagCount` struct after the existing `TagExtractor` struct definition:**\n```rust\n/// Tag with occurrence statistics\n#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]\npub struct TagCount {\n    /// The tag name (without # prefix)\n    pub tag: String,\n    /// Number of documents containing this tag\n    pub document_count: usize,\n}\n```\n\n**Add a new method to `TagExtractor` implementation:**\n```rust\n/// Extract all tags with document counts from markdown files in the given path\n/// Returns tags sorted by document_count descending, then alphabetically\npub fn extract_tags_with_counts(\n    \u0026self,\n    path: \u0026Path,\n) -\u003e Result\u003cVec\u003cTagCount\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n    let files = if path.is_file() {\n        vec![path.to_path_buf()]\n    } else {\n        collect_markdown_files(path)?\n    };\n\n    // Track which documents contain each tag\n    // Key: tag name, Value: set of file paths that contain this tag\n    let tag_documents: HashMap\u003cString, std::collections::HashSet\u003cPathBuf\u003e\u003e = files\n        .par_iter()\n        .filter_map(|file_path| {\n            self.extract_tags_from_file(file_path)\n                .ok()\n                .map(|tags| (file_path.clone(), tags))\n        })\n        .fold(\n            || HashMap::new(),\n            |mut acc: HashMap\u003cString, std::collections::HashSet\u003cPathBuf\u003e\u003e, (file_path, tags)| {\n                // Deduplicate tags within the same file (a file counts once per tag)\n                let unique_tags: std::collections::HashSet\u003cString\u003e = tags.into_iter().collect();\n                for tag in unique_tags {\n                    acc.entry(tag)\n                        .or_insert_with(std::collections::HashSet::new)\n                        .insert(file_path.clone());\n                }\n                acc\n            },\n        )\n        .reduce(\n            || HashMap::new(),\n            |mut a, b| {\n                for (tag, files) in b {\n                    a.entry(tag)\n                        .or_insert_with(std::collections::HashSet::new)\n                        .extend(files);\n                }\n                a\n            },\n        );\n\n    // Convert to Vec\u003cTagCount\u003e sorted by document_count desc, then tag name asc\n    let mut result: Vec\u003cTagCount\u003e = tag_documents\n        .into_iter()\n        .map(|(tag, files)| TagCount {\n            tag,\n            document_count: files.len(),\n        })\n        .collect();\n\n    result.sort_by(|a, b| {\n        b.document_count\n            .cmp(\u0026a.document_count)\n            .then_with(|| a.tag.cmp(\u0026b.tag))\n    });\n\n    Ok(result)\n}\n```\n\n**Key design decisions:**\n- Use `HashSet\u003cPathBuf\u003e` to track unique documents per tag (a document counts once even if it has the same tag multiple times in frontmatter)\n- Sort by document_count descending (most popular tags first), then alphabetically for stable ordering\n- Reuse existing `extract_tags_from_file` method to maintain consistency\n\n### 2. Add Request/Response Types to `src/mcp.rs`\n\n**Add import for `TagCount` at the top:**\n```rust\nuse crate::tag_extractor::{TagCount, TagExtractor};\n```\n\n**Add new request/response structs:**\n```rust\n/// Parameters for the list_tags tool\n#[derive(Debug, Deserialize, JsonSchema)]\npub struct ListTagsRequest {\n    #[schemars(description = \"Subpath within the vault to search (optional, defaults to entire vault)\")]\n    pub path: Option\u003cString\u003e,\n\n    #[schemars(description = \"Minimum document count to include a tag (optional, defaults to 1)\")]\n    pub min_count: Option\u003cusize\u003e,\n\n    #[schemars(description = \"Maximum number of tags to return (optional, defaults to all)\")]\n    pub limit: Option\u003cusize\u003e,\n}\n\n/// Response for the list_tags tool\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct ListTagsResponse {\n    /// List of tags with their document counts\n    pub tags: Vec\u003cTagCount\u003e,\n    /// Total number of unique tags found (before filtering/limiting)\n    pub total_unique_tags: usize,\n    /// Whether the results were truncated due to limit parameter\n    pub truncated: bool,\n}\n```\n\n### 3. Add MCP Tool Method to `TaskSearchService` in `src/mcp.rs`\n\nAdd the new tool method inside the `#[tool_router] impl TaskSearchService` block:\n\n```rust\n#[tool(description = \"List all tags in the vault with document counts. Returns tags sorted by frequency (most common first). Useful for understanding the tag taxonomy, finding popular topics, and discovering content organization patterns.\")]\nasync fn list_tags(\n    \u0026self,\n    Parameters(request): Parameters\u003cListTagsRequest\u003e,\n) -\u003e Result\u003cJson\u003cListTagsResponse\u003e, ErrorData\u003e {\n    // Resolve search path\n    let search_path = if let Some(ref subpath) = request.path {\n        self.base_path.join(subpath)\n    } else {\n        self.base_path.clone()\n    };\n\n    // Extract tags with counts\n    let mut tags = self\n        .tag_extractor\n        .extract_tags_with_counts(\u0026search_path)\n        .map_err(|e| ErrorData {\n            code: ErrorCode(-32603),\n            message: Cow::from(format!(\"Failed to extract tags: {}\", e)),\n            data: None,\n        })?;\n\n    // Track total before filtering\n    let total_unique_tags = tags.len();\n\n    // Filter by min_count if specified\n    if let Some(min_count) = request.min_count {\n        tags.retain(|t| t.document_count \u003e= min_count);\n    }\n\n    // Apply limit if specified\n    let truncated = if let Some(limit) = request.limit {\n        if tags.len() \u003e limit {\n            tags.truncate(limit);\n            true\n        } else {\n            false\n        }\n    } else {\n        false\n    };\n\n    Ok(Json(ListTagsResponse {\n        tags,\n        total_unique_tags,\n        truncated,\n    }))\n}\n```\n\n### 4. Add HTTP Endpoint Handlers to `src/main.rs`\n\n**Add HTTP handler functions:**\n```rust\n/// HTTP handler for listing tags with counts (GET)\nasync fn list_tags_handler_get(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    query: axum::extract::Query\u003cmcp::ListTagsRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ListTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    list_tags_impl(state, query.0).await\n}\n\n/// HTTP handler for listing tags with counts (POST)\nasync fn list_tags_handler_post(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    axum::Json(request): axum::Json\u003cmcp::ListTagsRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::ListTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    list_tags_impl(state, request).await\n}\n\n/// Shared implementation for listing tags with counts\nasync fn list_tags_impl(\n    state: AppState,\n    request: mcp::ListTagsRequest,\n) -\u003e Result\u003caxum::Json\u003cmcp::ListTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    // Resolve search path\n    let search_path = if let Some(ref subpath) = request.path {\n        state.base_path.join(subpath)\n    } else {\n        state.base_path.clone()\n    };\n\n    // Extract tags with counts\n    let mut tags = state\n        .tag_extractor\n        .extract_tags_with_counts(\u0026search_path)\n        .map_err(|e| {\n            (\n                axum::http::StatusCode::INTERNAL_SERVER_ERROR,\n                format!(\"Failed to extract tags: {}\", e),\n            )\n        })?;\n\n    // Track total before filtering\n    let total_unique_tags = tags.len();\n\n    // Filter by min_count if specified\n    if let Some(min_count) = request.min_count {\n        tags.retain(|t| t.document_count \u003e= min_count);\n    }\n\n    // Apply limit if specified\n    let truncated = if let Some(limit) = request.limit {\n        if tags.len() \u003e limit {\n            tags.truncate(limit);\n            true\n        } else {\n            false\n        }\n    } else {\n        false\n    };\n\n    Ok(axum::Json(mcp::ListTagsResponse {\n        tags,\n        total_unique_tags,\n        truncated,\n    }))\n}\n```\n\n**Add the route in the router configuration (in the `if args.mcp_http` block):**\n```rust\n.route(\n    \"/api/tags/list\",\n    axum::routing::get(list_tags_handler_get).post(list_tags_handler_post),\n)\n```\n\n**Add the new tool to the `tools_handler` function:**\n```rust\nasync fn tools_handler() -\u003e impl axum::response::IntoResponse {\n    use axum::Json;\n    use mcp::{ExtractTagsRequest, ListTagsRequest, SearchTasksRequest};\n    use schemars::schema_for;\n    use serde_json::json;\n\n    let search_tasks_schema = schema_for!(SearchTasksRequest);\n    let extract_tags_schema = schema_for!(ExtractTagsRequest);\n    let list_tags_schema = schema_for!(ListTagsRequest);\n\n    let tools = json!({\n        \"tools\": [\n            {\n                \"name\": \"search_tasks\",\n                \"description\": \"Search for tasks in Markdown files with optional filtering by status, dates, and tags\",\n                \"input_schema\": search_tasks_schema\n            },\n            {\n                \"name\": \"extract_tags\",\n                \"description\": \"Extract all unique tags from YAML frontmatter in Markdown files\",\n                \"input_schema\": extract_tags_schema\n            },\n            {\n                \"name\": \"list_tags\",\n                \"description\": \"List all tags in the vault with document counts. Returns tags sorted by frequency.\",\n                \"input_schema\": list_tags_schema\n            }\n        ]\n    });\n\n    Json(tools)\n}\n```\n\n**Update the startup message to include the new endpoint:**\n```rust\neprintln!(\"  - GET/POST http://{}/api/tags/list\", addr);\n```\n\n### 5. Add Tests to `src/tag_extractor.rs`\n\nAdd tests at the end of the `#[cfg(test)] mod tests` block:\n\n```rust\n#[test]\nfn test_extract_tags_with_counts_single_file() {\n    let extractor = TagExtractor::new();\n    let temp_dir = std::env::temp_dir().join(\"test_tags_counts\");\n    std::fs::create_dir_all(\u0026temp_dir).unwrap();\n    \n    let content = r#\"---\ntags:\n  - rust\n  - programming\n---\n# Content\n\"#;\n    let file_path = temp_dir.join(\"test1.md\");\n    std::fs::write(\u0026file_path, content).unwrap();\n    \n    let counts = extractor.extract_tags_with_counts(\u0026temp_dir).unwrap();\n    \n    assert_eq!(counts.len(), 2);\n    assert!(counts.iter().any(|t| t.tag == \"rust\" \u0026\u0026 t.document_count == 1));\n    assert!(counts.iter().any(|t| t.tag == \"programming\" \u0026\u0026 t.document_count == 1));\n    \n    std::fs::remove_dir_all(\u0026temp_dir).ok();\n}\n\n#[test]\nfn test_extract_tags_with_counts_multiple_files() {\n    let extractor = TagExtractor::new();\n    let temp_dir = std::env::temp_dir().join(\"test_tags_counts_multi\");\n    std::fs::create_dir_all(\u0026temp_dir).unwrap();\n    \n    // File 1: has rust and programming tags\n    let content1 = r#\"---\ntags:\n  - rust\n  - programming\n---\n\"#;\n    std::fs::write(temp_dir.join(\"file1.md\"), content1).unwrap();\n    \n    // File 2: has rust and cli tags\n    let content2 = r#\"---\ntags:\n  - rust\n  - cli\n---\n\"#;\n    std::fs::write(temp_dir.join(\"file2.md\"), content2).unwrap();\n    \n    let counts = extractor.extract_tags_with_counts(\u0026temp_dir).unwrap();\n    \n    // rust appears in 2 documents, programming and cli in 1 each\n    let rust = counts.iter().find(|t| t.tag == \"rust\").unwrap();\n    assert_eq!(rust.document_count, 2);\n    \n    let programming = counts.iter().find(|t| t.tag == \"programming\").unwrap();\n    assert_eq!(programming.document_count, 1);\n    \n    let cli = counts.iter().find(|t| t.tag == \"cli\").unwrap();\n    assert_eq!(cli.document_count, 1);\n    \n    // Should be sorted by count desc\n    assert_eq!(counts[0].tag, \"rust\");\n    \n    std::fs::remove_dir_all(\u0026temp_dir).ok();\n}\n\n#[test]\nfn test_extract_tags_with_counts_duplicate_in_same_file() {\n    let extractor = TagExtractor::new();\n    let temp_dir = std::env::temp_dir().join(\"test_tags_counts_dup\");\n    std::fs::create_dir_all(\u0026temp_dir).unwrap();\n    \n    // File with duplicate tag (should only count once per document)\n    let content = r#\"---\ntags:\n  - rust\n  - rust\n  - programming\n---\n\"#;\n    std::fs::write(temp_dir.join(\"file.md\"), content).unwrap();\n    \n    let counts = extractor.extract_tags_with_counts(\u0026temp_dir).unwrap();\n    \n    let rust = counts.iter().find(|t| t.tag == \"rust\").unwrap();\n    assert_eq!(rust.document_count, 1); // Should be 1, not 2\n    \n    std::fs::remove_dir_all(\u0026temp_dir).ok();\n}\n\n#[test]\nfn test_extract_tags_with_counts_empty_vault() {\n    let extractor = TagExtractor::new();\n    let temp_dir = std::env::temp_dir().join(\"test_tags_counts_empty\");\n    std::fs::create_dir_all(\u0026temp_dir).unwrap();\n    \n    let counts = extractor.extract_tags_with_counts(\u0026temp_dir).unwrap();\n    assert!(counts.is_empty());\n    \n    std::fs::remove_dir_all(\u0026temp_dir).ok();\n}\n```\n\n### Files to Modify\n\n1. `/home/jeffutter/src/markdown-todo-extractor/src/tag_extractor.rs`\n   - Add imports for `schemars`, `serde`, `HashMap`\n   - Add `TagCount` struct\n   - Add `extract_tags_with_counts` method\n   - Add unit tests\n\n2. `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs`\n   - Update import to include `TagCount`\n   - Add `ListTagsRequest` struct\n   - Add `ListTagsResponse` struct\n   - Add `list_tags` tool method\n\n3. `/home/jeffutter/src/markdown-todo-extractor/src/main.rs`\n   - Add `list_tags_handler_get` function\n   - Add `list_tags_handler_post` function\n   - Add `list_tags_impl` function\n   - Add route for `/api/tags/list`\n   - Update `tools_handler` to include new tool schema\n   - Update startup message to show new endpoint\n\n### Tool Naming Rationale\n\nThe existing `extract_tags` tool returns just unique tag names (a simple list). The new `list_tags` tool returns tags with statistics. Both tools serve different purposes:\n\n- `extract_tags`: Quick list of all unique tags (lightweight, existing functionality)\n- `list_tags`: Detailed tag statistics with document counts (new functionality)\n\nThis follows the pattern seen in the Obsidian Copilot reference where tag listing is a distinct operation from tag extraction.\n\n### API Examples\n\n**MCP Tool Call:**\n```json\n{\n  \"tool\": \"list_tags\",\n  \"arguments\": {\n    \"path\": \"Projects\",\n    \"min_count\": 2,\n    \"limit\": 50\n  }\n}\n```\n\n**HTTP GET:**\n```\nGET /api/tags/list?path=Projects\u0026min_count=2\u0026limit=50\n```\n\n**HTTP POST:**\n```json\nPOST /api/tags/list\n{\n  \"path\": \"Projects\",\n  \"min_count\": 2,\n  \"limit\": 50\n}\n```\n\n**Response:**\n```json\n{\n  \"tags\": [\n    {\"tag\": \"work\", \"document_count\": 15},\n    {\"tag\": \"project\", \"document_count\": 10},\n    {\"tag\": \"meeting\", \"document_count\": 7}\n  ],\n  \"total_unique_tags\": 45,\n  \"truncated\": false\n}\n```","status":"closed","priority":2,"issue_type":"feature","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-19T18:50:38.764378029-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-19T22:31:35.344875943-06:00","closed_at":"2026-01-19T22:31:35.344875943-06:00","close_reason":"Closed","labels":["planned"],"dependencies":[{"issue_id":"markdown-todo-extractor-tx9.4","depends_on_id":"markdown-todo-extractor-tx9","type":"parent-child","created_at":"2026-01-19T18:50:38.764956166-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-tx9.5","title":"Tag Search Tool","description":"# Tag Search Tool\n\nCreate a tool to list files based on tags.\n\n## Requirements\n\n- Tags should be searched for in the vault.\n- Tags are in the YAML frontmatter of files under the 'tags' key\n\n\nThis tool enables the LLM to find documents that match specific tags from their YAML frontmatter. Unlike the semantic search in RAG, this provides exact tag-based filtering, useful for:\n- Finding all documents with a particular tag\n- Finding documents that match multiple tags (AND/OR logic)\n- Browsing content by topic/category\n\n---\n\n## Implementation Plan\n\n### Overview\n\nCreate a new `search_by_tags` MCP tool and REST API endpoint that finds markdown files matching specified frontmatter tags. The tool will extend the existing `TagExtractor` module and follow the established patterns in the codebase.\n\n### Architecture Decision\n\nExtend the existing `TagExtractor` rather than creating a new extractor module. The `TagExtractor` already has:\n- YAML frontmatter parsing logic (`extract_frontmatter`, `parse_tags_from_frontmatter`)\n- File collection mechanism (`collect_markdown_files`)\n- Parallel processing with rayon\n\nAdding tag search functionality to this module maintains cohesion and reduces code duplication.\n\n---\n\n### Step 1: Add Config Integration to TagExtractor\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/tag_extractor.rs`\n\nCurrently `TagExtractor` is a simple unit struct without configuration. Update to match `TaskExtractor` pattern:\n\n```rust\nuse crate::config::Config;\nuse std::sync::Arc;\n\n/// Extractor for YAML frontmatter tags\npub struct TagExtractor {\n    config: Arc\u003cConfig\u003e,\n}\n\nimpl TagExtractor {\n    pub fn new(config: Arc\u003cConfig\u003e) -\u003e Self {\n        Self { config }\n    }\n}\n```\n\nUpdate `collect_markdown_files` to accept `\u0026Config` and apply path exclusions:\n\n```rust\nfn collect_markdown_files(dir: \u0026Path, config: \u0026Config) -\u003e Result\u003cVec\u003cPathBuf\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n    let mut files = Vec::new();\n\n    if dir.is_dir() {\n        for entry in fs::read_dir(dir)? {\n            let entry = entry?;\n            let path = entry.path();\n\n            // Skip excluded paths\n            if config.should_exclude(\u0026path) {\n                continue;\n            }\n\n            if path.is_dir() {\n                files.extend(collect_markdown_files(\u0026path, config)?);\n            } else if path.extension().and_then(|s| s.to_str()) == Some(\"md\") {\n                files.push(path);\n            }\n        }\n    }\n\n    Ok(files)\n}\n```\n\nUpdate `extract_tags` to pass config:\n\n```rust\npub fn extract_tags(\u0026self, path: \u0026Path) -\u003e Result\u003cVec\u003cString\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n    let files = if path.is_file() {\n        vec![path.to_path_buf()]\n    } else {\n        collect_markdown_files(path, \u0026self.config)?\n    };\n    // ... rest unchanged\n}\n```\n\n---\n\n### Step 2: Add TaggedFile Struct and Search Method\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/tag_extractor.rs`\n\nAdd new struct and search method:\n\n```rust\nuse schemars::JsonSchema;\n\n/// Represents a file that matches tag search criteria\n#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]\npub struct TaggedFile {\n    /// Absolute path to the file\n    pub file_path: String,\n    /// File name without path\n    pub file_name: String,\n    /// Tags that matched the search criteria\n    pub matched_tags: Vec\u003cString\u003e,\n    /// All tags found in the file's frontmatter\n    pub all_tags: Vec\u003cString\u003e,\n}\n\nimpl TagExtractor {\n    /// Make the internal method public for single file tag extraction\n    pub fn get_file_tags(\u0026self, file_path: \u0026Path) -\u003e Result\u003cVec\u003cString\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n        self.extract_tags_from_file(file_path)\n    }\n\n    /// Search for files by tags with AND/OR logic\n    /// \n    /// # Arguments\n    /// * `path` - Directory to search\n    /// * `tags` - Tags to search for\n    /// * `match_all` - If true, file must have ALL tags (AND logic). If false, file must have ANY tag (OR logic)\n    pub fn search_by_tags(\n        \u0026self,\n        path: \u0026Path,\n        tags: \u0026[String],\n        match_all: bool,\n    ) -\u003e Result\u003cVec\u003cTaggedFile\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n        let files = if path.is_file() {\n            vec![path.to_path_buf()]\n        } else {\n            collect_markdown_files(path, \u0026self.config)?\n        };\n\n        // Normalize search tags to lowercase for case-insensitive comparison\n        let search_tags: Vec\u003cString\u003e = tags.iter().map(|t| t.to_lowercase()).collect();\n\n        let results: Vec\u003cTaggedFile\u003e = files\n            .par_iter()\n            .filter_map(|file_path| {\n                // Extract tags from file\n                let all_tags = self.extract_tags_from_file(file_path).ok()?;\n                \n                if all_tags.is_empty() {\n                    return None;\n                }\n\n                // Normalize file tags for comparison\n                let normalized_tags: Vec\u003cString\u003e = all_tags.iter().map(|t| t.to_lowercase()).collect();\n\n                // Find which search tags match this file\n                let matched_tags: Vec\u003cString\u003e = search_tags\n                    .iter()\n                    .filter(|search_tag| normalized_tags.contains(search_tag))\n                    .cloned()\n                    .collect();\n\n                // Apply match logic\n                let matches = if match_all {\n                    // AND logic: all search tags must be present\n                    matched_tags.len() == search_tags.len()\n                } else {\n                    // OR logic: at least one search tag must be present\n                    !matched_tags.is_empty()\n                };\n\n                if matches {\n                    Some(TaggedFile {\n                        file_path: file_path.to_string_lossy().to_string(),\n                        file_name: file_path.file_name()?.to_string_lossy().to_string(),\n                        matched_tags,\n                        all_tags,\n                    })\n                } else {\n                    None\n                }\n            })\n            .collect();\n\n        Ok(results)\n    }\n}\n```\n\n---\n\n### Step 3: Add MCP Tool in mcp.rs\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs`\n\nAdd request/response types:\n\n```rust\nuse crate::tag_extractor::TaggedFile;\n\n/// Parameters for the search_by_tags tool\n#[derive(Debug, Deserialize, JsonSchema)]\npub struct SearchByTagsRequest {\n    #[schemars(description = \"Tags to search for\")]\n    pub tags: Vec\u003cString\u003e,\n\n    #[schemars(description = \"If true, file must have ALL tags (AND logic). If false, file must have ANY tag (OR logic). Default: false\")]\n    pub match_all: Option\u003cbool\u003e,\n\n    #[schemars(description = \"Subpath within the base directory to search (optional)\")]\n    pub subpath: Option\u003cString\u003e,\n\n    #[schemars(description = \"Limit the number of files returned\")]\n    pub limit: Option\u003cusize\u003e,\n}\n\n/// Response for the search_by_tags tool\n#[derive(Debug, Serialize, Deserialize, JsonSchema)]\npub struct SearchByTagsResponse {\n    pub files: Vec\u003cTaggedFile\u003e,\n    pub total_count: usize,\n}\n```\n\nAdd tool method to `TaskSearchService`:\n\n```rust\n#[tool(description = \"Search for files by YAML frontmatter tags with AND/OR matching\")]\nasync fn search_by_tags(\n    \u0026self,\n    Parameters(request): Parameters\u003cSearchByTagsRequest\u003e,\n) -\u003e Result\u003cJson\u003cSearchByTagsResponse\u003e, ErrorData\u003e {\n    // Determine the search path (base path + optional subpath)\n    let search_path = if let Some(subpath) = request.subpath {\n        self.base_path.join(subpath)\n    } else {\n        self.base_path.clone()\n    };\n\n    let match_all = request.match_all.unwrap_or(false);\n\n    // Search for files by tags\n    let mut files = self\n        .tag_extractor\n        .search_by_tags(\u0026search_path, \u0026request.tags, match_all)\n        .map_err(|e| ErrorData {\n            code: ErrorCode(-32603),\n            message: Cow::from(format!(\"Failed to search by tags: {}\", e)),\n            data: None,\n        })?;\n\n    let total_count = files.len();\n\n    // Apply limit if specified\n    if let Some(limit) = request.limit {\n        files.truncate(limit);\n    }\n\n    Ok(Json(SearchByTagsResponse { files, total_count }))\n}\n```\n\n---\n\n### Step 4: Add REST API Endpoints in main.rs\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/main.rs`\n\nAdd HTTP handlers following the existing pattern:\n\n```rust\n/// HTTP handler for searching by tags (GET with query params)\nasync fn search_by_tags_handler_get(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    query: axum::extract::Query\u003cmcp::SearchByTagsRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::SearchByTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    search_by_tags_impl(state, query.0).await\n}\n\n/// HTTP handler for searching by tags (POST with JSON body)\nasync fn search_by_tags_handler_post(\n    axum::extract::State(state): axum::extract::State\u003cAppState\u003e,\n    axum::Json(request): axum::Json\u003cmcp::SearchByTagsRequest\u003e,\n) -\u003e Result\u003caxum::Json\u003cmcp::SearchByTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    search_by_tags_impl(state, request).await\n}\n\n/// Shared implementation for tag search\nasync fn search_by_tags_impl(\n    state: AppState,\n    request: mcp::SearchByTagsRequest,\n) -\u003e Result\u003caxum::Json\u003cmcp::SearchByTagsResponse\u003e, (axum::http::StatusCode, String)\u003e {\n    // Determine the search path (base path + optional subpath)\n    let search_path = if let Some(ref subpath) = request.subpath {\n        state.base_path.join(subpath)\n    } else {\n        state.base_path.clone()\n    };\n\n    let match_all = request.match_all.unwrap_or(false);\n\n    // Search for files by tags\n    let mut files = state\n        .tag_extractor\n        .search_by_tags(\u0026search_path, \u0026request.tags, match_all)\n        .map_err(|e| {\n            (\n                axum::http::StatusCode::INTERNAL_SERVER_ERROR,\n                format!(\"Failed to search by tags: {}\", e),\n            )\n        })?;\n\n    let total_count = files.len();\n\n    // Apply limit if specified\n    if let Some(limit) = request.limit {\n        files.truncate(limit);\n    }\n\n    Ok(axum::Json(mcp::SearchByTagsResponse { files, total_count }))\n}\n```\n\nRegister routes in the router:\n\n```rust\n.route(\n    \"/api/search_by_tags\",\n    axum::routing::get(search_by_tags_handler_get).post(search_by_tags_handler_post),\n)\n```\n\nUpdate `tools_handler()` to include the new tool schema:\n\n```rust\nasync fn tools_handler() -\u003e impl axum::response::IntoResponse {\n    use mcp::{ExtractTagsRequest, SearchByTagsRequest, SearchTasksRequest};\n    use schemars::schema_for;\n    // ...\n    let search_by_tags_schema = schema_for!(SearchByTagsRequest);\n\n    let tools = json!({\n        \"tools\": [\n            // ... existing tools ...\n            {\n                \"name\": \"search_by_tags\",\n                \"description\": \"Search for files by YAML frontmatter tags with AND/OR matching\",\n                \"input_schema\": search_by_tags_schema\n            }\n        ]\n    });\n    // ...\n}\n```\n\nUpdate startup messages:\n\n```rust\neprintln!(\"  - GET/POST http://{}/api/search_by_tags\", addr);\n```\n\n---\n\n### Step 5: Add CLI Subcommand in cli.rs\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/cli.rs`\n\nAdd new subcommand to `Commands` enum:\n\n```rust\n#[derive(Subcommand, Debug)]\npub enum Commands {\n    /// Extract and filter tasks from markdown files\n    Tasks(Box\u003cTasksCommand\u003e),\n    /// Extract all unique tags from markdown files\n    Tags {\n        /// Path to file or folder to scan\n        #[arg(required = true)]\n        path: PathBuf,\n    },\n    /// Search for files by tags\n    SearchByTags {\n        /// Path to file or folder to scan\n        #[arg(required = true)]\n        path: PathBuf,\n\n        /// Tags to search for (comma-separated)\n        #[arg(long, value_delimiter = ',', required = true)]\n        tags: Vec\u003cString\u003e,\n\n        /// Require all tags to match (AND logic) instead of any (OR logic)\n        #[arg(long)]\n        match_all: bool,\n\n        /// Limit number of results\n        #[arg(long)]\n        limit: Option\u003cusize\u003e,\n    },\n}\n```\n\nAdd handling in `run_cli()`:\n\n```rust\nSome(Commands::SearchByTags { path, tags, match_all, limit }) =\u003e {\n    // Load configuration from the path\n    let config = Arc::new(Config::load_from_base_path(\u0026path));\n\n    // Create tag extractor\n    let extractor = TagExtractor::new(config);\n\n    // Search for files by tags\n    let mut files = extractor.search_by_tags(path, \u0026tags, match_all)?;\n\n    // Apply limit if specified\n    if let Some(limit) = limit {\n        files.truncate(limit);\n    }\n\n    // Output as JSON\n    let json = serde_json::to_string_pretty(\u0026files)?;\n    println!(\"{}\", json);\n\n    Ok(())\n}\n```\n\n---\n\n### Step 6: Update Existing Code for Config Integration\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/mcp.rs`\n\nUpdate `TaskSearchService::new()` to pass config to TagExtractor:\n\n```rust\npub fn new(base_path: PathBuf) -\u003e Self {\n    let config = Arc::new(Config::load_from_base_path(\u0026base_path));\n\n    Self {\n        tool_router: Self::tool_router(),\n        base_path,\n        task_extractor: Arc::new(TaskExtractor::new(config.clone())),\n        tag_extractor: Arc::new(TagExtractor::new(config)),  // Updated\n    }\n}\n```\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/main.rs`\n\nUpdate `AppState` initialization:\n\n```rust\nlet app_state = AppState {\n    base_path: base_path.clone(),\n    task_extractor: Arc::new(extractor::TaskExtractor::new(config.clone())),\n    tag_extractor: Arc::new(tag_extractor::TagExtractor::new(config.clone())),  // Updated\n    config,\n};\n```\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/cli.rs`\n\nUpdate Tags command to pass config:\n\n```rust\nSome(Commands::Tags { path }) =\u003e {\n    // Load configuration from the path\n    let config = Arc::new(Config::load_from_base_path(\u0026path));\n\n    // Create tag extractor\n    let extractor = TagExtractor::new(config);\n    // ... rest unchanged\n}\n```\n\n---\n\n### Step 7: Add Unit Tests\n\n**File:** `/home/jeffutter/src/markdown-todo-extractor/src/tag_extractor.rs`\n\nAdd tests for the new functionality:\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::io::Write;\n    use tempfile::TempDir;\n\n    fn create_test_config() -\u003e Arc\u003cConfig\u003e {\n        Arc::new(Config::default())\n    }\n\n    fn create_test_file(dir: \u0026Path, name: \u0026str, content: \u0026str) -\u003e PathBuf {\n        let path = dir.join(name);\n        let mut file = std::fs::File::create(\u0026path).unwrap();\n        file.write_all(content.as_bytes()).unwrap();\n        path\n    }\n\n    #[test]\n    fn test_search_by_tags_or_logic() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = create_test_config();\n        let extractor = TagExtractor::new(config);\n\n        // Create test files\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - rust\\n  - cli\\n---\\n# File 1\");\n        create_test_file(temp_dir.path(), \"file2.md\", \"---\\ntags:\\n  - python\\n  - cli\\n---\\n# File 2\");\n        create_test_file(temp_dir.path(), \"file3.md\", \"---\\ntags:\\n  - java\\n---\\n# File 3\");\n\n        // Search with OR logic (default)\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"rust\".to_string(), \"python\".to_string()], false).unwrap();\n\n        assert_eq!(results.len(), 2);\n        assert!(results.iter().any(|f| f.file_name == \"file1.md\"));\n        assert!(results.iter().any(|f| f.file_name == \"file2.md\"));\n    }\n\n    #[test]\n    fn test_search_by_tags_and_logic() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = create_test_config();\n        let extractor = TagExtractor::new(config);\n\n        // Create test files\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - rust\\n  - cli\\n---\\n# File 1\");\n        create_test_file(temp_dir.path(), \"file2.md\", \"---\\ntags:\\n  - rust\\n---\\n# File 2\");\n\n        // Search with AND logic\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"rust\".to_string(), \"cli\".to_string()], true).unwrap();\n\n        assert_eq!(results.len(), 1);\n        assert_eq!(results[0].file_name, \"file1.md\");\n    }\n\n    #[test]\n    fn test_search_by_tags_case_insensitive() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = create_test_config();\n        let extractor = TagExtractor::new(config);\n\n        // Create test file with mixed case tags\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - Rust\\n  - CLI\\n---\\n# File 1\");\n\n        // Search with lowercase\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"rust\".to_string()], false).unwrap();\n        assert_eq!(results.len(), 1);\n\n        // Search with uppercase\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"RUST\".to_string()], false).unwrap();\n        assert_eq!(results.len(), 1);\n    }\n\n    #[test]\n    fn test_search_by_tags_empty_result() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = create_test_config();\n        let extractor = TagExtractor::new(config);\n\n        // Create test file\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - rust\\n---\\n# File 1\");\n\n        // Search for non-existent tag\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"nonexistent\".to_string()], false).unwrap();\n        assert!(results.is_empty());\n    }\n\n    #[test]\n    fn test_search_by_tags_respects_exclusions() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = Arc::new(Config {\n            exclude_paths: vec![\"excluded\".to_string()],\n        });\n        let extractor = TagExtractor::new(config);\n\n        // Create test files\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - rust\\n---\\n# File 1\");\n        \n        // Create excluded directory\n        let excluded_dir = temp_dir.path().join(\"excluded\");\n        std::fs::create_dir(\u0026excluded_dir).unwrap();\n        create_test_file(\u0026excluded_dir, \"file2.md\", \"---\\ntags:\\n  - rust\\n---\\n# File 2\");\n\n        // Search should not include excluded file\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"rust\".to_string()], false).unwrap();\n        assert_eq!(results.len(), 1);\n        assert_eq!(results[0].file_name, \"file1.md\");\n    }\n\n    #[test]\n    fn test_tagged_file_contains_all_tags() {\n        let temp_dir = TempDir::new().unwrap();\n        let config = create_test_config();\n        let extractor = TagExtractor::new(config);\n\n        // Create test file with multiple tags\n        create_test_file(temp_dir.path(), \"file1.md\", \"---\\ntags:\\n  - rust\\n  - cli\\n  - tool\\n---\\n# File 1\");\n\n        // Search for one tag\n        let results = extractor.search_by_tags(temp_dir.path(), \u0026[\"rust\".to_string()], false).unwrap();\n\n        assert_eq!(results.len(), 1);\n        assert_eq!(results[0].matched_tags, vec![\"rust\".to_string()]);\n        assert_eq!(results[0].all_tags, vec![\"rust\".to_string(), \"cli\".to_string(), \"tool\".to_string()]);\n    }\n}\n```\n\nNote: Add `tempfile = \"3\"` to dev-dependencies in Cargo.toml for tests.\n\n---\n\n### File Changes Summary\n\n| File | Changes |\n|------|---------|\n| `Cargo.toml` | Add `tempfile = \"3\"` to dev-dependencies |\n| `src/tag_extractor.rs` | Add Config integration, `TaggedFile` struct, `search_by_tags()` method, `get_file_tags()` method, unit tests |\n| `src/mcp.rs` | Add `SearchByTagsRequest`, `SearchByTagsResponse`, `search_by_tags` tool, update constructor |\n| `src/main.rs` | Add REST API handlers and routes for `/api/search_by_tags`, update AppState, update tools_handler |\n| `src/cli.rs` | Add `SearchByTags` subcommand, update Tags command for Config |\n\n---\n\n### API Examples\n\n**MCP Tool Call:**\n```json\n{\n  \"tool\": \"search_by_tags\",\n  \"arguments\": {\n    \"tags\": [\"project\", \"active\"],\n    \"match_all\": true,\n    \"limit\": 20\n  }\n}\n```\n\n**REST API Call:**\n```bash\n# GET with query params\ncurl \"http://localhost:8000/api/search_by_tags?tags=project,active\u0026match_all=true\u0026limit=20\"\n\n# POST with JSON body\ncurl -X POST http://localhost:8000/api/search_by_tags \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"tags\": [\"project\", \"active\"], \"match_all\": true, \"limit\": 20}'\n```\n\n**CLI:**\n```bash\n# Search for files with ANY of the tags (OR logic - default)\nmarkdown-todo-extractor search-by-tags /path/to/vault --tags project,active\n\n# Search for files with ALL tags (AND logic)\nmarkdown-todo-extractor search-by-tags /path/to/vault --tags project,active --match-all\n\n# With limit\nmarkdown-todo-extractor search-by-tags /path/to/vault --tags project --limit 10\n```\n\n**Response Format:**\n```json\n{\n  \"files\": [\n    {\n      \"file_path\": \"/vault/projects/ProjectA.md\",\n      \"file_name\": \"ProjectA.md\",\n      \"matched_tags\": [\"project\", \"active\"],\n      \"all_tags\": [\"project\", \"active\", \"2024\"]\n    }\n  ],\n  \"total_count\": 1\n}\n```\n\n---\n\n### Testing Checklist\n\n1. [ ] Unit tests pass for tag search logic (AND/OR)\n2. [ ] Unit tests pass for case-insensitive matching\n3. [ ] Unit tests pass for path exclusions\n4. [ ] MCP tool works via stdio\n5. [ ] REST API endpoints work (GET and POST)\n6. [ ] CLI subcommand works\n7. [ ] AND logic correctly filters files (must have ALL tags)\n8. [ ] OR logic correctly includes files (must have ANY tag)\n9. [ ] Path exclusions are respected\n10. [ ] Limit parameter works\n11. [ ] Subpath parameter works\n12. [ ] Empty results handled gracefully\n13. [ ] Files without frontmatter are skipped gracefully\n14. [ ] `cargo build --release` succeeds\n15. [ ] `cargo clippy` passes\n16. [ ] `cargo fmt --check` passes","status":"closed","priority":2,"issue_type":"feature","assignee":"jeffutter","owner":"jeff@jeffutter.com","created_at":"2026-01-19T18:51:01.199607478-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-19T22:35:17.861300724-06:00","closed_at":"2026-01-19T22:35:17.861300724-06:00","close_reason":"Closed","labels":["planned"],"dependencies":[{"issue_id":"markdown-todo-extractor-tx9.5","depends_on_id":"markdown-todo-extractor-tx9","type":"parent-child","created_at":"2026-01-19T18:51:01.200457025-06:00","created_by":"Jeffery Utter"}]}
{"id":"markdown-todo-extractor-v0k","title":"Refactor into capability-based modules with unified trait interface","description":"Refactor the project architecture to support multiple interfaces (MCP, HTTP, CLI) through a common trait system.\n\n## Architecture Overview\n\nSplit capabilities into focused modules:\n- Tag listing/retrieval\n- Task search  \n- File list/fetch\n\n## Module Design\n\nEach module should:\n1. Handle logic for its specific capability area\n2. Expose a type implementing a common trait\n3. Support all three interface types through the trait\n\n## Trait Requirements\n\nCreate a common trait that provides:\n- MCP server integration\n- HTTP REST-style endpoint support\n- CLI command interface\n\nWhen implemented, the trait should enable near-automatic exposure via all three interfaces.\n\n## Benefits\n\n- Consistent interface across all capability areas\n- Single implementation supports multiple access patterns\n- Easier to add new capabilities in the future\n- Cleaner separation of concerns","status":"closed","priority":2,"issue_type":"feature","owner":"jeff@jeffutter.com","created_at":"2026-01-19T23:21:12.250198134-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-20T11:13:13.51127458-06:00","closed_at":"2026-01-20T11:13:13.51127458-06:00","close_reason":"Closed"}
{"id":"markdown-todo-extractor-zxt","title":"Clarify ambiguous exclusion pattern matching semantics in config","description":"## Problem\n\nIn `src/config.rs`, the exclusion matching logic tries **both** glob patterns AND substring matching:\n\n```rust\npub fn should_exclude_path(\u0026self, path: \u0026Path) -\u003e bool {\n    let path_str = path.to_string_lossy();\n    for pattern_str in \u0026self.exclude_paths {\n        // Try to compile as glob pattern\n        if let Ok(pattern) = Pattern::new(pattern_str)\n            \u0026\u0026 pattern.matches(\u0026path_str)\n        {\n            return true;\n        }\n\n        // Also check if the path contains the pattern as a substring\n        if path_str.contains(pattern_str) {\n            return true;\n        }\n    }\n    false\n}\n```\n\nThis dual-matching causes confusion:\n\n1. **Pattern \"test\" matches as both**:\n   - Glob: matches file named \"test\"\n   - Substring: matches \"test\", \"testing\", \"my_test_file\", etc.\n\n2. **Users may expect only one behavior**:\n   - If they write `test`, do they expect exact match or substring?\n   - If they write `**/test/**`, they clearly want glob behavior\n\n3. **Order matters but shouldn't**:\n   - If glob pattern is invalid but matches as substring, it still works\n   - This makes debugging harder\n\n## Proposed Solutions\n\n**Option A: Explicit syntax differentiation**\n```toml\nexclude_paths = [\n    \"substring:Template\",     # Substring match\n    \"glob:**/Archive/**\"      # Glob pattern\n]\n```\n\n**Option B: Glob-only with documentation**\n- Only support glob patterns\n- Document that `*test*` should be used instead of `test`\n- Simpler, more predictable behavior\n\n**Option C: Substring-only for simple strings**\n- If pattern contains glob chars (`*`, `?`, `[`), treat as glob\n- Otherwise, treat as substring\n- Current behavior but documented\n\n## Locations\n\n- `src/config.rs` lines 74-94 (`should_exclude_path` method)\n\n## Estimated Impact\n\n- Clearer user experience\n- More predictable behavior\n- Reduced support confusion","status":"deferred","priority":3,"issue_type":"task","owner":"jeff@jeffutter.com","created_at":"2026-01-20T22:48:49.456287699-06:00","created_by":"Jeffery Utter","updated_at":"2026-01-21T10:36:35.664114774-06:00","labels":["simplification-opportunity"]}
